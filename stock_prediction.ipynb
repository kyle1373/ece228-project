{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece63324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eabcf2",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caffaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAW\n",
    "search_dirs = [\"Dataset/Equities/\", \"Dataset/BondsFundsIndexes/\"]\n",
    "stock_dict = {}\n",
    "\n",
    "for search_dir in search_dirs:\n",
    "        for filename in os.listdir(search_dir):\n",
    "            if filename.endswith('.csv'):\n",
    "                stock_name = filename[:-4] \n",
    "                stock_dict[stock_name] = pd.read_csv(os.path.join(search_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab76b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess\n",
    "preprocessed_stocks = {}\n",
    "stock_returns = {}\n",
    "minmax_scalers = {}\n",
    "\n",
    "for stock in stock_dict:\n",
    "    stock_data = stock_dict[stock]\n",
    "    stock_data = stock_data.drop(columns = ['Date'])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    preprocessed_stock = scaler.fit_transform(stock_data)\n",
    "    minmax_scalers[stock] = scaler\n",
    "    \n",
    "    preprocessed_stock = pd.DataFrame(preprocessed_stock, columns=stock_data.columns, index=stock_data.index)\n",
    "    preprocessed_stocks[stock]=preprocessed_stock\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f77fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Form time series\n",
    "def create_time_series_data(stock_data, window_size=60):\n",
    "    \"\"\"\n",
    "    Turn raw data into sliding window prediction.\n",
    "\n",
    "    - data: (num_days,)\n",
    "    - window_size: number of days to use for prediction\n",
    "\n",
    "    \n",
    "    Return\n",
    "    - X: nparray um_samples, window_size)\n",
    "    - y: nparray (num_samples,)\n",
    "    \"\"\"\n",
    "    num_days = data.shape[0]\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_days - window_size):\n",
    "        X.append(stock_data.iloc[i:i+window_size].values)\n",
    "        \n",
    "        ### Target Variable: Closing Price\n",
    "        close_day = data['Close'].iloc[i+window_size]\n",
    "        y.append(close_day)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "stock_60d = {}\n",
    "stock_1d = {}\n",
    "\n",
    "for stock, data in preprocessed_stocks.items():\n",
    "    stock_60d[stock], stock_1d[stock] = create_time_series_data(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1086fb",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eba77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53fcdbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=15, num_hidden_layers=6, output_dim=1):\n",
    "        super(DMLPModel, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        self.hidden_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 60 * 6)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=50, num_layers=4, output_dim=1, dropout_rate=0.4, recurrent_dropout_rate=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=recurrent_dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])  \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, output_dim=1, num_filters=2, kernel_size=2, hidden_dim=2):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters * (input_dim // 2 - 1), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    " \n",
    "def RandomForestModel(n_estimators=500, max_depth=20,min_samples_split=10,min_samples_leaf=10,max_features=40):\n",
    "    return RandomForestRegressor(\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    max_depth=max_depth,\n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    min_samples_leaf=min_samples_leaf,\n",
    "                                    max_features=max_features\n",
    "                                   )\n",
    "\n",
    "def SVRModel(C=2**2, gamma=2**-3):\n",
    "    return SVR(kernel='rbf', C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1c99d",
   "metadata": {},
   "source": [
    "## Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf697210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=100, patience=0, device='cpu', print_every=5):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    print_every = print_every\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device).view(-1,1)\n",
    "        X_val = X_val.to(device)\n",
    "        y_val = y_val.to(device).view(-1,1)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if epoch % print_every ==0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs - 1}, Training Loss: {loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val)\n",
    "                \n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss = loss\n",
    "\n",
    "                print(f\"Validation Loss: {loss:.6f}\")\n",
    "\n",
    "#             if val_loss < best_loss:\n",
    "#                 best_loss = val_loss\n",
    "#                 best_model_wts = model.state_dict()\n",
    "#                 patience_counter = 0\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "\n",
    "    #         if patience_counter > patience:\n",
    "    #             print(\"Early stopping\")\n",
    "    #             break\n",
    "    \n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f0292c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([492])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_val, y_test = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "# 6-1-3 split\n",
    "for stock in stock_60d:\n",
    "    x_train_temp, X_test[stock], y_train_temp, y_test[stock] = train_test_split(stock_60d[stock], stock_1d[stock], test_size=0.2, random_state=1)\n",
    "    X_train[stock], X_val[stock], y_train[stock], y_val[stock] = train_test_split(x_train_temp, y_train_temp, test_size=0.125, random_state=1)\n",
    "\n",
    "def to_tensor(data):\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "X_train_tensors = {stock: to_tensor(X_train[stock]) for stock in X_train}\n",
    "X_val_tensors = {stock: to_tensor(X_val[stock]) for stock in X_val}\n",
    "X_test_tensors = {stock: to_tensor(X_test[stock]) for stock in X_test}\n",
    "y_val_tensors = {stock: to_tensor(y_val[stock]) for stock in y_val}\n",
    "y_train_tensors = {stock: to_tensor(y_train[stock]) for stock in y_train}\n",
    "y_test_tensors = {stock: to_tensor(y_test[stock]) for stock in y_test}\n",
    "\n",
    "print(y_test_tensors['DEO'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "765de67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model DMLP: DEO\n",
      "Epoch 0/499, Training Loss: 0.415346\n",
      "Validation Loss: 0.356953\n",
      "Epoch 50/499, Training Loss: 0.003890\n",
      "Validation Loss: 0.004550\n",
      "Epoch 100/499, Training Loss: 0.002659\n",
      "Validation Loss: 0.002903\n",
      "Epoch 150/499, Training Loss: 0.001855\n",
      "Validation Loss: 0.001874\n",
      "Epoch 200/499, Training Loss: 0.001275\n",
      "Validation Loss: 0.001192\n",
      "Epoch 250/499, Training Loss: 0.000975\n",
      "Validation Loss: 0.000913\n",
      "Epoch 300/499, Training Loss: 0.000779\n",
      "Validation Loss: 0.000739\n",
      "Epoch 350/499, Training Loss: 0.000650\n",
      "Validation Loss: 0.000637\n",
      "Epoch 400/499, Training Loss: 0.000555\n",
      "Validation Loss: 0.000573\n",
      "Epoch 450/499, Training Loss: 0.000484\n",
      "Validation Loss: 0.000528\n",
      "Test Loss: 0.000523\n",
      "model DMLP: DLR\n",
      "Epoch 0/499, Training Loss: 0.215004\n",
      "Validation Loss: 0.186090\n",
      "Epoch 50/499, Training Loss: 0.004357\n",
      "Validation Loss: 0.004797\n",
      "Epoch 100/499, Training Loss: 0.002431\n",
      "Validation Loss: 0.002449\n",
      "Epoch 150/499, Training Loss: 0.001412\n",
      "Validation Loss: 0.001474\n",
      "Epoch 200/499, Training Loss: 0.000991\n",
      "Validation Loss: 0.001137\n",
      "Epoch 250/499, Training Loss: 0.000768\n",
      "Validation Loss: 0.000989\n",
      "Epoch 300/499, Training Loss: 0.000692\n",
      "Validation Loss: 0.000943\n",
      "Epoch 350/499, Training Loss: 0.000612\n",
      "Validation Loss: 0.000883\n",
      "Epoch 400/499, Training Loss: 0.000866\n",
      "Validation Loss: 0.001237\n",
      "Epoch 450/499, Training Loss: 0.000538\n",
      "Validation Loss: 0.000823\n",
      "Test Loss: 0.000665\n",
      "model DMLP: ULTA\n",
      "Epoch 0/499, Training Loss: 0.099144\n",
      "Validation Loss: 0.076769\n",
      "Epoch 50/499, Training Loss: 0.002691\n",
      "Validation Loss: 0.003055\n",
      "Epoch 100/499, Training Loss: 0.001296\n",
      "Validation Loss: 0.001502\n",
      "Epoch 150/499, Training Loss: 0.000833\n",
      "Validation Loss: 0.000893\n",
      "Epoch 200/499, Training Loss: 0.000612\n",
      "Validation Loss: 0.000660\n",
      "Epoch 250/499, Training Loss: 0.001922\n",
      "Validation Loss: 0.001539\n",
      "Epoch 300/499, Training Loss: 0.000476\n",
      "Validation Loss: 0.000527\n",
      "Epoch 350/499, Training Loss: 0.000426\n",
      "Validation Loss: 0.000487\n",
      "Epoch 400/499, Training Loss: 0.000394\n",
      "Validation Loss: 0.000461\n",
      "Epoch 450/499, Training Loss: 0.000370\n",
      "Validation Loss: 0.000442\n",
      "Test Loss: 0.000540\n",
      "model DMLP: V\n",
      "Epoch 0/499, Training Loss: 0.087855\n",
      "Validation Loss: 0.081783\n",
      "Epoch 50/499, Training Loss: 0.001181\n",
      "Validation Loss: 0.001262\n",
      "Epoch 100/499, Training Loss: 0.000825\n",
      "Validation Loss: 0.000772\n",
      "Epoch 150/499, Training Loss: 0.000594\n",
      "Validation Loss: 0.000528\n",
      "Epoch 200/499, Training Loss: 0.000501\n",
      "Validation Loss: 0.000461\n",
      "Epoch 250/499, Training Loss: 0.000423\n",
      "Validation Loss: 0.000388\n",
      "Epoch 300/499, Training Loss: 0.000494\n",
      "Validation Loss: 0.000349\n",
      "Epoch 350/499, Training Loss: 0.000329\n",
      "Validation Loss: 0.000322\n",
      "Epoch 400/499, Training Loss: 0.000356\n",
      "Validation Loss: 0.000336\n",
      "Epoch 450/499, Training Loss: 0.000623\n",
      "Validation Loss: 0.000840\n",
      "Test Loss: 0.000296\n",
      "model DMLP: DIS\n",
      "Epoch 0/499, Training Loss: 0.058331\n",
      "Validation Loss: 0.048521\n",
      "Epoch 50/499, Training Loss: 0.001728\n",
      "Validation Loss: 0.001636\n",
      "Epoch 100/499, Training Loss: 0.000879\n",
      "Validation Loss: 0.000898\n",
      "Epoch 150/499, Training Loss: 0.000670\n",
      "Validation Loss: 0.000756\n",
      "Epoch 200/499, Training Loss: 0.000551\n",
      "Validation Loss: 0.000674\n",
      "Epoch 250/499, Training Loss: 0.000533\n",
      "Validation Loss: 0.000658\n",
      "Epoch 300/499, Training Loss: 0.000464\n",
      "Validation Loss: 0.000616\n",
      "Epoch 350/499, Training Loss: 0.000430\n",
      "Validation Loss: 0.000584\n",
      "Epoch 400/499, Training Loss: 0.000448\n",
      "Validation Loss: 0.000600\n",
      "Epoch 450/499, Training Loss: 0.000405\n",
      "Validation Loss: 0.000540\n",
      "Test Loss: 0.000389\n",
      "model DMLP: CME\n",
      "Epoch 0/499, Training Loss: 0.514093\n",
      "Validation Loss: 0.468099\n",
      "Epoch 50/499, Training Loss: 0.008272\n",
      "Validation Loss: 0.007624\n",
      "Epoch 100/499, Training Loss: 0.002287\n",
      "Validation Loss: 0.002230\n",
      "Epoch 150/499, Training Loss: 0.002018\n",
      "Validation Loss: 0.001965\n",
      "Epoch 200/499, Training Loss: 0.001728\n",
      "Validation Loss: 0.001663\n",
      "Epoch 250/499, Training Loss: 0.001215\n",
      "Validation Loss: 0.001104\n",
      "Epoch 300/499, Training Loss: 0.000884\n",
      "Validation Loss: 0.000767\n",
      "Epoch 350/499, Training Loss: 0.000719\n",
      "Validation Loss: 0.000717\n",
      "Epoch 400/499, Training Loss: 0.000604\n",
      "Validation Loss: 0.000566\n",
      "Epoch 450/499, Training Loss: 0.000562\n",
      "Validation Loss: 0.000541\n",
      "Test Loss: 0.000730\n",
      "model DMLP: AAPL\n",
      "Epoch 0/499, Training Loss: 0.286028\n",
      "Validation Loss: 0.256383\n",
      "Epoch 50/499, Training Loss: 0.003159\n",
      "Validation Loss: 0.002674\n",
      "Epoch 100/499, Training Loss: 0.001552\n",
      "Validation Loss: 0.001678\n",
      "Epoch 150/499, Training Loss: 0.001271\n",
      "Validation Loss: 0.001329\n",
      "Epoch 200/499, Training Loss: 0.000961\n",
      "Validation Loss: 0.000948\n",
      "Epoch 250/499, Training Loss: 0.000690\n",
      "Validation Loss: 0.000625\n",
      "Epoch 300/499, Training Loss: 0.000543\n",
      "Validation Loss: 0.000474\n",
      "Epoch 350/499, Training Loss: 0.000458\n",
      "Validation Loss: 0.000414\n",
      "Epoch 400/499, Training Loss: 0.000390\n",
      "Validation Loss: 0.000371\n",
      "Epoch 450/499, Training Loss: 0.000339\n",
      "Validation Loss: 0.000336\n",
      "Test Loss: 0.000436\n",
      "model DMLP: GOOGL\n",
      "Epoch 0/499, Training Loss: 0.522097\n",
      "Validation Loss: 0.429631\n",
      "Epoch 50/499, Training Loss: 0.001704\n",
      "Validation Loss: 0.002399\n",
      "Epoch 100/499, Training Loss: 0.001210\n",
      "Validation Loss: 0.001509\n",
      "Epoch 150/499, Training Loss: 0.000900\n",
      "Validation Loss: 0.001083\n",
      "Epoch 200/499, Training Loss: 0.000960\n",
      "Validation Loss: 0.001180\n",
      "Epoch 250/499, Training Loss: 0.000813\n",
      "Validation Loss: 0.000987\n",
      "Epoch 300/499, Training Loss: 0.000665\n",
      "Validation Loss: 0.000779\n",
      "Epoch 350/499, Training Loss: 0.000541\n",
      "Validation Loss: 0.000600\n",
      "Epoch 400/499, Training Loss: 0.000460\n",
      "Validation Loss: 0.000477\n",
      "Epoch 450/499, Training Loss: 0.000412\n",
      "Validation Loss: 0.000408\n",
      "Test Loss: 0.000334\n",
      "model DMLP: UNH\n",
      "Epoch 0/499, Training Loss: 0.323477\n",
      "Validation Loss: 0.281395\n",
      "Epoch 50/499, Training Loss: 0.002205\n",
      "Validation Loss: 0.001904\n",
      "Epoch 100/499, Training Loss: 0.001155\n",
      "Validation Loss: 0.001118\n",
      "Epoch 150/499, Training Loss: 0.001032\n",
      "Validation Loss: 0.000977\n",
      "Epoch 200/499, Training Loss: 0.000953\n",
      "Validation Loss: 0.000892\n",
      "Epoch 250/499, Training Loss: 0.000869\n",
      "Validation Loss: 0.000804\n",
      "Epoch 300/499, Training Loss: 0.000783\n",
      "Validation Loss: 0.000719\n",
      "Epoch 350/499, Training Loss: 0.000700\n",
      "Validation Loss: 0.000633\n",
      "Epoch 400/499, Training Loss: 0.000618\n",
      "Validation Loss: 0.000552\n",
      "Epoch 450/499, Training Loss: 0.000540\n",
      "Validation Loss: 0.000481\n",
      "Test Loss: 0.000457\n",
      "model DMLP: GS\n",
      "Epoch 0/499, Training Loss: 0.180348\n",
      "Validation Loss: 0.149557\n",
      "Epoch 50/499, Training Loss: 0.003269\n",
      "Validation Loss: 0.003024\n",
      "Epoch 100/499, Training Loss: 0.001769\n",
      "Validation Loss: 0.001815\n",
      "Epoch 150/499, Training Loss: 0.001043\n",
      "Validation Loss: 0.001170\n",
      "Epoch 200/499, Training Loss: 0.000794\n",
      "Validation Loss: 0.000914\n",
      "Epoch 250/499, Training Loss: 0.000628\n",
      "Validation Loss: 0.000705\n",
      "Epoch 300/499, Training Loss: 0.000511\n",
      "Validation Loss: 0.000571\n",
      "Epoch 350/499, Training Loss: 0.000435\n",
      "Validation Loss: 0.000502\n",
      "Epoch 400/499, Training Loss: 0.000382\n",
      "Validation Loss: 0.000460\n",
      "Epoch 450/499, Training Loss: 0.001025\n",
      "Validation Loss: 0.001166\n",
      "Test Loss: 0.000372\n",
      "model DMLP: QCOM\n",
      "Epoch 0/499, Training Loss: 0.284476\n",
      "Validation Loss: 0.269046\n",
      "Epoch 50/499, Training Loss: 0.003525\n",
      "Validation Loss: 0.003861\n",
      "Epoch 100/499, Training Loss: 0.001949\n",
      "Validation Loss: 0.002059\n",
      "Epoch 150/499, Training Loss: 0.001112\n",
      "Validation Loss: 0.001201\n",
      "Epoch 200/499, Training Loss: 0.000839\n",
      "Validation Loss: 0.000953\n",
      "Epoch 250/499, Training Loss: 0.000656\n",
      "Validation Loss: 0.000767\n",
      "Epoch 300/499, Training Loss: 0.000531\n",
      "Validation Loss: 0.000637\n",
      "Epoch 350/499, Training Loss: 0.000454\n",
      "Validation Loss: 0.000547\n",
      "Epoch 400/499, Training Loss: 0.000400\n",
      "Validation Loss: 0.000480\n",
      "Epoch 450/499, Training Loss: 0.000359\n",
      "Validation Loss: 0.000430\n",
      "Test Loss: 0.000696\n",
      "model DMLP: BA\n",
      "Epoch 0/499, Training Loss: 0.422129\n",
      "Validation Loss: 0.346540\n",
      "Epoch 50/499, Training Loss: 0.005268\n",
      "Validation Loss: 0.005911\n",
      "Epoch 100/499, Training Loss: 0.002664\n",
      "Validation Loss: 0.003283\n",
      "Epoch 150/499, Training Loss: 0.001417\n",
      "Validation Loss: 0.001682\n",
      "Epoch 200/499, Training Loss: 0.001021\n",
      "Validation Loss: 0.001204\n",
      "Epoch 250/499, Training Loss: 0.000790\n",
      "Validation Loss: 0.000927\n",
      "Epoch 300/499, Training Loss: 0.000651\n",
      "Validation Loss: 0.000760\n",
      "Epoch 350/499, Training Loss: 0.000550\n",
      "Validation Loss: 0.000643\n",
      "Epoch 400/499, Training Loss: 0.000477\n",
      "Validation Loss: 0.000571\n",
      "Epoch 450/499, Training Loss: 0.000424\n",
      "Validation Loss: 0.000519\n",
      "Test Loss: 0.000565\n",
      "model DMLP: STZ\n",
      "Epoch 0/499, Training Loss: 0.291427\n",
      "Validation Loss: 0.246309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/499, Training Loss: 0.004797\n",
      "Validation Loss: 0.005448\n",
      "Epoch 100/499, Training Loss: 0.002785\n",
      "Validation Loss: 0.003138\n",
      "Epoch 150/499, Training Loss: 0.001579\n",
      "Validation Loss: 0.001795\n",
      "Epoch 200/499, Training Loss: 0.001805\n",
      "Validation Loss: 0.001487\n",
      "Epoch 250/499, Training Loss: 0.000831\n",
      "Validation Loss: 0.001017\n",
      "Epoch 300/499, Training Loss: 0.000881\n",
      "Validation Loss: 0.001012\n",
      "Epoch 350/499, Training Loss: 0.000665\n",
      "Validation Loss: 0.000818\n",
      "Epoch 400/499, Training Loss: 0.000741\n",
      "Validation Loss: 0.001101\n",
      "Epoch 450/499, Training Loss: 0.000591\n",
      "Validation Loss: 0.000741\n",
      "Test Loss: 0.000596\n",
      "model DMLP: NTDOY\n",
      "Epoch 0/499, Training Loss: 0.141819\n",
      "Validation Loss: 0.113538\n",
      "Epoch 50/499, Training Loss: 0.003151\n",
      "Validation Loss: 0.002655\n",
      "Epoch 100/499, Training Loss: 0.001684\n",
      "Validation Loss: 0.001431\n",
      "Epoch 150/499, Training Loss: 0.000889\n",
      "Validation Loss: 0.000869\n",
      "Epoch 200/499, Training Loss: 0.000675\n",
      "Validation Loss: 0.000685\n",
      "Epoch 250/499, Training Loss: 0.001098\n",
      "Validation Loss: 0.002238\n",
      "Epoch 300/499, Training Loss: 0.000472\n",
      "Validation Loss: 0.000505\n",
      "Epoch 350/499, Training Loss: 0.000418\n",
      "Validation Loss: 0.000451\n",
      "Epoch 400/499, Training Loss: 0.000956\n",
      "Validation Loss: 0.001744\n",
      "Epoch 450/499, Training Loss: 0.000369\n",
      "Validation Loss: 0.000395\n",
      "Test Loss: 0.000341\n",
      "model DMLP: MSFT\n",
      "Epoch 0/499, Training Loss: 0.263399\n",
      "Validation Loss: 0.228521\n",
      "Epoch 50/499, Training Loss: 0.001109\n",
      "Validation Loss: 0.001123\n",
      "Epoch 100/499, Training Loss: 0.000768\n",
      "Validation Loss: 0.000873\n",
      "Epoch 150/499, Training Loss: 0.000677\n",
      "Validation Loss: 0.000764\n",
      "Epoch 200/499, Training Loss: 0.000590\n",
      "Validation Loss: 0.000650\n",
      "Epoch 250/499, Training Loss: 0.000506\n",
      "Validation Loss: 0.000540\n",
      "Epoch 300/499, Training Loss: 0.000426\n",
      "Validation Loss: 0.000438\n",
      "Epoch 350/499, Training Loss: 0.000357\n",
      "Validation Loss: 0.000350\n",
      "Epoch 400/499, Training Loss: 0.000304\n",
      "Validation Loss: 0.000286\n",
      "Epoch 450/499, Training Loss: 0.000269\n",
      "Validation Loss: 0.000248\n",
      "Test Loss: 0.000287\n",
      "model DMLP: DE\n",
      "Epoch 0/499, Training Loss: 0.271137\n",
      "Validation Loss: 0.251958\n",
      "Epoch 50/499, Training Loss: 0.002551\n",
      "Validation Loss: 0.002921\n",
      "Epoch 100/499, Training Loss: 0.001726\n",
      "Validation Loss: 0.001843\n",
      "Epoch 150/499, Training Loss: 0.001234\n",
      "Validation Loss: 0.001336\n",
      "Epoch 200/499, Training Loss: 0.000830\n",
      "Validation Loss: 0.000927\n",
      "Epoch 250/499, Training Loss: 0.000654\n",
      "Validation Loss: 0.000744\n",
      "Epoch 300/499, Training Loss: 0.000552\n",
      "Validation Loss: 0.000633\n",
      "Epoch 350/499, Training Loss: 0.000474\n",
      "Validation Loss: 0.000546\n",
      "Epoch 400/499, Training Loss: 0.000417\n",
      "Validation Loss: 0.000474\n",
      "Epoch 450/499, Training Loss: 0.000371\n",
      "Validation Loss: 0.000423\n",
      "Test Loss: 0.000548\n",
      "model DMLP: BAC\n",
      "Epoch 0/499, Training Loss: 0.121703\n",
      "Validation Loss: 0.087896\n",
      "Epoch 50/499, Training Loss: 0.003452\n",
      "Validation Loss: 0.003632\n",
      "Epoch 100/499, Training Loss: 0.001785\n",
      "Validation Loss: 0.002038\n",
      "Epoch 150/499, Training Loss: 0.001075\n",
      "Validation Loss: 0.001244\n",
      "Epoch 200/499, Training Loss: 0.000734\n",
      "Validation Loss: 0.000879\n",
      "Epoch 250/499, Training Loss: 0.000580\n",
      "Validation Loss: 0.000681\n",
      "Epoch 300/499, Training Loss: 0.000634\n",
      "Validation Loss: 0.000599\n",
      "Epoch 350/499, Training Loss: 0.000468\n",
      "Validation Loss: 0.000617\n",
      "Epoch 400/499, Training Loss: 0.000403\n",
      "Validation Loss: 0.000525\n",
      "Epoch 450/499, Training Loss: 0.000437\n",
      "Validation Loss: 0.000567\n",
      "Test Loss: 0.000483\n",
      "model DMLP: IRM\n",
      "Epoch 0/499, Training Loss: 0.043226\n",
      "Validation Loss: 0.040894\n",
      "Epoch 50/499, Training Loss: 0.001459\n",
      "Validation Loss: 0.001555\n",
      "Epoch 100/499, Training Loss: 0.000760\n",
      "Validation Loss: 0.000767\n",
      "Epoch 150/499, Training Loss: 0.000502\n",
      "Validation Loss: 0.000645\n",
      "Epoch 200/499, Training Loss: 0.000428\n",
      "Validation Loss: 0.000556\n",
      "Epoch 250/499, Training Loss: 0.000348\n",
      "Validation Loss: 0.000295\n",
      "Epoch 300/499, Training Loss: 0.000364\n",
      "Validation Loss: 0.000308\n",
      "Epoch 350/499, Training Loss: 0.000311\n",
      "Validation Loss: 0.000244\n",
      "Epoch 400/499, Training Loss: 0.000290\n",
      "Validation Loss: 0.000242\n",
      "Epoch 450/499, Training Loss: 0.000273\n",
      "Validation Loss: 0.000216\n",
      "Test Loss: 0.000288\n",
      "model DMLP: ADM\n",
      "Epoch 0/499, Training Loss: 0.143588\n",
      "Validation Loss: 0.123034\n",
      "Epoch 50/499, Training Loss: 0.003061\n",
      "Validation Loss: 0.002333\n",
      "Epoch 100/499, Training Loss: 0.001412\n",
      "Validation Loss: 0.001257\n",
      "Epoch 150/499, Training Loss: 0.000905\n",
      "Validation Loss: 0.000872\n",
      "Epoch 200/499, Training Loss: 0.000671\n",
      "Validation Loss: 0.000608\n",
      "Epoch 250/499, Training Loss: 0.000542\n",
      "Validation Loss: 0.000470\n",
      "Epoch 300/499, Training Loss: 0.000454\n",
      "Validation Loss: 0.000385\n",
      "Epoch 350/499, Training Loss: 0.000394\n",
      "Validation Loss: 0.000336\n",
      "Epoch 400/499, Training Loss: 0.000419\n",
      "Validation Loss: 0.001422\n",
      "Epoch 450/499, Training Loss: 0.000360\n",
      "Validation Loss: 0.000316\n",
      "Test Loss: 0.000336\n",
      "model DMLP: GOOG\n",
      "Epoch 0/499, Training Loss: 0.146982\n",
      "Validation Loss: 0.112274\n",
      "Epoch 50/499, Training Loss: 0.001268\n",
      "Validation Loss: 0.001658\n",
      "Epoch 100/499, Training Loss: 0.000837\n",
      "Validation Loss: 0.001054\n",
      "Epoch 150/499, Training Loss: 0.002080\n",
      "Validation Loss: 0.004520\n",
      "Epoch 200/499, Training Loss: 0.000484\n",
      "Validation Loss: 0.000472\n",
      "Epoch 250/499, Training Loss: 0.000390\n",
      "Validation Loss: 0.000394\n",
      "Epoch 300/499, Training Loss: 0.000347\n",
      "Validation Loss: 0.000345\n",
      "Epoch 350/499, Training Loss: 0.000311\n",
      "Validation Loss: 0.000306\n",
      "Epoch 400/499, Training Loss: 0.000281\n",
      "Validation Loss: 0.000273\n",
      "Epoch 450/499, Training Loss: 0.000256\n",
      "Validation Loss: 0.000246\n",
      "Test Loss: 0.000280\n",
      "model DMLP: META\n",
      "Epoch 0/499, Training Loss: 0.284013\n",
      "Validation Loss: 0.206002\n",
      "Epoch 50/499, Training Loss: 0.002585\n",
      "Validation Loss: 0.002525\n",
      "Epoch 100/499, Training Loss: 0.001554\n",
      "Validation Loss: 0.001731\n",
      "Epoch 150/499, Training Loss: 0.001010\n",
      "Validation Loss: 0.001109\n",
      "Epoch 200/499, Training Loss: 0.000672\n",
      "Validation Loss: 0.000726\n",
      "Epoch 250/499, Training Loss: 0.000557\n",
      "Validation Loss: 0.000604\n",
      "Epoch 300/499, Training Loss: 0.000488\n",
      "Validation Loss: 0.000536\n",
      "Epoch 350/499, Training Loss: 0.000430\n",
      "Validation Loss: 0.000477\n",
      "Epoch 400/499, Training Loss: 0.000386\n",
      "Validation Loss: 0.000429\n",
      "Epoch 450/499, Training Loss: 0.000355\n",
      "Validation Loss: 0.000393\n",
      "Test Loss: 0.000288\n",
      "model DMLP: PG\n",
      "Epoch 0/499, Training Loss: 0.227136\n",
      "Validation Loss: 0.217927\n",
      "Epoch 50/499, Training Loss: 0.003186\n",
      "Validation Loss: 0.003023\n",
      "Epoch 100/499, Training Loss: 0.002003\n",
      "Validation Loss: 0.002112\n",
      "Epoch 150/499, Training Loss: 0.001558\n",
      "Validation Loss: 0.001665\n",
      "Epoch 200/499, Training Loss: 0.001198\n",
      "Validation Loss: 0.001219\n",
      "Epoch 250/499, Training Loss: 0.000818\n",
      "Validation Loss: 0.000869\n",
      "Epoch 300/499, Training Loss: 0.000650\n",
      "Validation Loss: 0.000697\n",
      "Epoch 350/499, Training Loss: 0.000559\n",
      "Validation Loss: 0.000599\n",
      "Epoch 400/499, Training Loss: 0.000497\n",
      "Validation Loss: 0.000518\n",
      "Epoch 450/499, Training Loss: 0.000453\n",
      "Validation Loss: 0.000465\n",
      "Test Loss: 0.000497\n",
      "model DMLP: ARE\n",
      "Epoch 0/499, Training Loss: 0.052424\n",
      "Validation Loss: 0.047054\n",
      "Epoch 50/499, Training Loss: 0.002376\n",
      "Validation Loss: 0.002122\n",
      "Epoch 100/499, Training Loss: 0.000963\n",
      "Validation Loss: 0.000929\n",
      "Epoch 150/499, Training Loss: 0.000670\n",
      "Validation Loss: 0.000865\n",
      "Epoch 200/499, Training Loss: 0.000533\n",
      "Validation Loss: 0.000566\n",
      "Epoch 250/499, Training Loss: 0.000481\n",
      "Validation Loss: 0.000542\n",
      "Epoch 300/499, Training Loss: 0.000731\n",
      "Validation Loss: 0.000464\n",
      "Epoch 350/499, Training Loss: 0.000418\n",
      "Validation Loss: 0.000443\n",
      "Epoch 400/499, Training Loss: 0.000442\n",
      "Validation Loss: 0.000606\n",
      "Epoch 450/499, Training Loss: 0.000525\n",
      "Validation Loss: 0.000567\n",
      "Test Loss: 0.000391\n",
      "model DMLP: LH\n",
      "Epoch 0/499, Training Loss: 0.107618\n",
      "Validation Loss: 0.082126\n",
      "Epoch 50/499, Training Loss: 0.002529\n",
      "Validation Loss: 0.002387\n",
      "Epoch 100/499, Training Loss: 0.001063\n",
      "Validation Loss: 0.001202\n",
      "Epoch 150/499, Training Loss: 0.000774\n",
      "Validation Loss: 0.001069\n",
      "Epoch 200/499, Training Loss: 0.000636\n",
      "Validation Loss: 0.000498\n",
      "Epoch 250/499, Training Loss: 0.000458\n",
      "Validation Loss: 0.000472\n",
      "Epoch 300/499, Training Loss: 0.000450\n",
      "Validation Loss: 0.000493\n",
      "Epoch 350/499, Training Loss: 0.000406\n",
      "Validation Loss: 0.000423\n",
      "Epoch 400/499, Training Loss: 0.000353\n",
      "Validation Loss: 0.000364\n",
      "Epoch 450/499, Training Loss: 0.000407\n",
      "Validation Loss: 0.000395\n",
      "Test Loss: 0.000531\n",
      "model DMLP: MRK\n",
      "Epoch 0/499, Training Loss: 0.081381\n",
      "Validation Loss: 0.072204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/499, Training Loss: 0.001606\n",
      "Validation Loss: 0.001589\n",
      "Epoch 100/499, Training Loss: 0.000901\n",
      "Validation Loss: 0.000942\n",
      "Epoch 150/499, Training Loss: 0.000601\n",
      "Validation Loss: 0.000648\n",
      "Epoch 200/499, Training Loss: 0.000476\n",
      "Validation Loss: 0.000554\n",
      "Epoch 250/499, Training Loss: 0.000399\n",
      "Validation Loss: 0.000488\n",
      "Epoch 300/499, Training Loss: 0.000489\n",
      "Validation Loss: 0.000599\n",
      "Epoch 350/499, Training Loss: 0.000568\n",
      "Validation Loss: 0.000410\n",
      "Epoch 400/499, Training Loss: 0.000287\n",
      "Validation Loss: 0.000370\n",
      "Epoch 450/499, Training Loss: 0.000411\n",
      "Validation Loss: 0.000592\n",
      "Test Loss: 0.000290\n",
      "model DMLP: LVMUY\n",
      "Epoch 0/499, Training Loss: 0.213551\n",
      "Validation Loss: 0.192947\n",
      "Epoch 50/499, Training Loss: 0.002406\n",
      "Validation Loss: 0.002537\n",
      "Epoch 100/499, Training Loss: 0.001687\n",
      "Validation Loss: 0.001593\n",
      "Epoch 150/499, Training Loss: 0.001296\n",
      "Validation Loss: 0.001184\n",
      "Epoch 200/499, Training Loss: 0.000916\n",
      "Validation Loss: 0.000800\n",
      "Epoch 250/499, Training Loss: 0.000672\n",
      "Validation Loss: 0.000580\n",
      "Epoch 300/499, Training Loss: 0.000567\n",
      "Validation Loss: 0.000490\n",
      "Epoch 350/499, Training Loss: 0.000489\n",
      "Validation Loss: 0.000418\n",
      "Epoch 400/499, Training Loss: 0.000427\n",
      "Validation Loss: 0.000364\n",
      "Epoch 450/499, Training Loss: 0.000377\n",
      "Validation Loss: 0.000326\n",
      "Test Loss: 0.000361\n",
      "model DMLP: LMT\n",
      "Epoch 0/499, Training Loss: 0.205483\n",
      "Validation Loss: 0.197124\n",
      "Epoch 50/499, Training Loss: 0.003930\n",
      "Validation Loss: 0.004257\n",
      "Epoch 100/499, Training Loss: 0.002205\n",
      "Validation Loss: 0.002358\n",
      "Epoch 150/499, Training Loss: 0.001422\n",
      "Validation Loss: 0.001503\n",
      "Epoch 200/499, Training Loss: 0.000933\n",
      "Validation Loss: 0.000998\n",
      "Epoch 250/499, Training Loss: 0.000718\n",
      "Validation Loss: 0.000730\n",
      "Epoch 300/499, Training Loss: 0.000723\n",
      "Validation Loss: 0.000579\n",
      "Epoch 350/499, Training Loss: 0.000539\n",
      "Validation Loss: 0.000513\n",
      "Epoch 400/499, Training Loss: 0.000497\n",
      "Validation Loss: 0.000501\n",
      "Epoch 450/499, Training Loss: 0.000527\n",
      "Validation Loss: 0.000618\n",
      "Test Loss: 0.000439\n",
      "model DMLP: MCD\n",
      "Epoch 0/499, Training Loss: 0.565199\n",
      "Validation Loss: 0.474092\n",
      "Epoch 50/499, Training Loss: 0.003026\n",
      "Validation Loss: 0.003475\n",
      "Epoch 100/499, Training Loss: 0.001846\n",
      "Validation Loss: 0.001900\n",
      "Epoch 150/499, Training Loss: 0.001619\n",
      "Validation Loss: 0.001655\n",
      "Epoch 200/499, Training Loss: 0.001418\n",
      "Validation Loss: 0.001439\n",
      "Epoch 250/499, Training Loss: 0.001231\n",
      "Validation Loss: 0.001236\n",
      "Epoch 300/499, Training Loss: 0.001052\n",
      "Validation Loss: 0.001045\n",
      "Epoch 350/499, Training Loss: 0.000886\n",
      "Validation Loss: 0.000872\n",
      "Epoch 400/499, Training Loss: 0.000746\n",
      "Validation Loss: 0.000732\n",
      "Epoch 450/499, Training Loss: 0.000641\n",
      "Validation Loss: 0.000635\n",
      "Test Loss: 0.000769\n",
      "model DMLP: MSBHF\n",
      "Epoch 0/499, Training Loss: 0.036938\n",
      "Validation Loss: 0.035204\n",
      "Epoch 50/499, Training Loss: 0.000772\n",
      "Validation Loss: 0.000743\n",
      "Epoch 100/499, Training Loss: 0.000360\n",
      "Validation Loss: 0.000289\n",
      "Epoch 150/499, Training Loss: 0.000258\n",
      "Validation Loss: 0.000226\n",
      "Epoch 200/499, Training Loss: 0.001898\n",
      "Validation Loss: 0.000471\n",
      "Epoch 250/499, Training Loss: 0.000209\n",
      "Validation Loss: 0.000192\n",
      "Epoch 300/499, Training Loss: 0.000173\n",
      "Validation Loss: 0.000183\n",
      "Epoch 350/499, Training Loss: 0.000158\n",
      "Validation Loss: 0.000174\n",
      "Epoch 400/499, Training Loss: 0.000145\n",
      "Validation Loss: 0.000165\n",
      "Epoch 450/499, Training Loss: 0.000231\n",
      "Validation Loss: 0.000221\n",
      "Test Loss: 0.000223\n",
      "model DMLP: JCI\n",
      "Epoch 0/499, Training Loss: 0.458355\n",
      "Validation Loss: 0.410254\n",
      "Epoch 50/499, Training Loss: 0.006712\n",
      "Validation Loss: 0.007871\n",
      "Epoch 100/499, Training Loss: 0.003134\n",
      "Validation Loss: 0.003505\n",
      "Epoch 150/499, Training Loss: 0.002268\n",
      "Validation Loss: 0.002614\n",
      "Epoch 200/499, Training Loss: 0.001408\n",
      "Validation Loss: 0.001692\n",
      "Epoch 250/499, Training Loss: 0.000952\n",
      "Validation Loss: 0.001163\n",
      "Epoch 300/499, Training Loss: 0.000870\n",
      "Validation Loss: 0.000836\n",
      "Epoch 350/499, Training Loss: 0.000528\n",
      "Validation Loss: 0.000630\n",
      "Epoch 400/499, Training Loss: 0.000529\n",
      "Validation Loss: 0.000632\n",
      "Epoch 450/499, Training Loss: 0.000409\n",
      "Validation Loss: 0.000492\n",
      "Test Loss: 0.000814\n",
      "model DMLP: MDLZ\n",
      "Epoch 0/499, Training Loss: 0.331148\n",
      "Validation Loss: 0.307420\n",
      "Epoch 50/499, Training Loss: 0.003140\n",
      "Validation Loss: 0.003051\n",
      "Epoch 100/499, Training Loss: 0.002167\n",
      "Validation Loss: 0.001913\n",
      "Epoch 150/499, Training Loss: 0.001524\n",
      "Validation Loss: 0.001291\n",
      "Epoch 200/499, Training Loss: 0.001006\n",
      "Validation Loss: 0.000882\n",
      "Epoch 250/499, Training Loss: 0.000821\n",
      "Validation Loss: 0.000757\n",
      "Epoch 300/499, Training Loss: 0.000663\n",
      "Validation Loss: 0.000566\n",
      "Epoch 350/499, Training Loss: 0.000572\n",
      "Validation Loss: 0.000474\n",
      "Epoch 400/499, Training Loss: 0.000500\n",
      "Validation Loss: 0.000389\n",
      "Epoch 450/499, Training Loss: 0.000720\n",
      "Validation Loss: 0.000466\n",
      "Test Loss: 0.000506\n",
      "model DMLP: ORCL\n",
      "Epoch 0/499, Training Loss: 0.113624\n",
      "Validation Loss: 0.092463\n",
      "Epoch 50/499, Training Loss: 0.001921\n",
      "Validation Loss: 0.001857\n",
      "Epoch 100/499, Training Loss: 0.001394\n",
      "Validation Loss: 0.001352\n",
      "Epoch 150/499, Training Loss: 0.001012\n",
      "Validation Loss: 0.000954\n",
      "Epoch 200/499, Training Loss: 0.000696\n",
      "Validation Loss: 0.000653\n",
      "Epoch 250/499, Training Loss: 0.000541\n",
      "Validation Loss: 0.000514\n",
      "Epoch 300/499, Training Loss: 0.000434\n",
      "Validation Loss: 0.000420\n",
      "Epoch 350/499, Training Loss: 0.000365\n",
      "Validation Loss: 0.000363\n",
      "Epoch 400/499, Training Loss: 0.000320\n",
      "Validation Loss: 0.000323\n",
      "Epoch 450/499, Training Loss: 0.000289\n",
      "Validation Loss: 0.000292\n",
      "Test Loss: 0.000346\n",
      "model DMLP: HCA\n",
      "Epoch 0/499, Training Loss: 0.111773\n",
      "Validation Loss: 0.094158\n",
      "Epoch 50/499, Training Loss: 0.001910\n",
      "Validation Loss: 0.001866\n",
      "Epoch 100/499, Training Loss: 0.001170\n",
      "Validation Loss: 0.001100\n",
      "Epoch 150/499, Training Loss: 0.000690\n",
      "Validation Loss: 0.000618\n",
      "Epoch 200/499, Training Loss: 0.000529\n",
      "Validation Loss: 0.000455\n",
      "Epoch 250/499, Training Loss: 0.000419\n",
      "Validation Loss: 0.000349\n",
      "Epoch 300/499, Training Loss: 0.000348\n",
      "Validation Loss: 0.000286\n",
      "Epoch 350/499, Training Loss: 0.000828\n",
      "Validation Loss: 0.000440\n",
      "Epoch 400/499, Training Loss: 0.000282\n",
      "Validation Loss: 0.000224\n",
      "Epoch 450/499, Training Loss: 0.000321\n",
      "Validation Loss: 0.000234\n",
      "Test Loss: 0.000376\n",
      "model DMLP: NVDA\n",
      "Epoch 0/499, Training Loss: 0.030342\n",
      "Validation Loss: 0.034115\n",
      "Epoch 50/499, Training Loss: 0.000537\n",
      "Validation Loss: 0.000752\n",
      "Epoch 100/499, Training Loss: 0.000258\n",
      "Validation Loss: 0.000362\n",
      "Epoch 150/499, Training Loss: 0.000172\n",
      "Validation Loss: 0.000243\n",
      "Epoch 200/499, Training Loss: 0.000128\n",
      "Validation Loss: 0.000200\n",
      "Epoch 250/499, Training Loss: 0.000104\n",
      "Validation Loss: 0.000173\n",
      "Epoch 300/499, Training Loss: 0.000090\n",
      "Validation Loss: 0.000156\n",
      "Epoch 350/499, Training Loss: 0.000081\n",
      "Validation Loss: 0.000149\n",
      "Epoch 400/499, Training Loss: 0.000074\n",
      "Validation Loss: 0.000129\n",
      "Epoch 450/499, Training Loss: 0.000075\n",
      "Validation Loss: 0.000121\n",
      "Test Loss: 0.000088\n",
      "model DMLP: WFC\n",
      "Epoch 0/499, Training Loss: 0.157711\n",
      "Validation Loss: 0.117413\n",
      "Epoch 50/499, Training Loss: 0.008411\n",
      "Validation Loss: 0.010154\n",
      "Epoch 100/499, Training Loss: 0.002526\n",
      "Validation Loss: 0.002954\n",
      "Epoch 150/499, Training Loss: 0.001354\n",
      "Validation Loss: 0.001526\n",
      "Epoch 200/499, Training Loss: 0.000998\n",
      "Validation Loss: 0.001076\n",
      "Epoch 250/499, Training Loss: 0.002155\n",
      "Validation Loss: 0.001289\n",
      "Epoch 300/499, Training Loss: 0.000724\n",
      "Validation Loss: 0.000751\n",
      "Epoch 350/499, Training Loss: 0.000733\n",
      "Validation Loss: 0.000722\n",
      "Epoch 400/499, Training Loss: 0.000615\n",
      "Validation Loss: 0.000650\n",
      "Epoch 450/499, Training Loss: 0.000637\n",
      "Validation Loss: 0.000699\n",
      "Test Loss: 0.000640\n",
      "model DMLP: PEP\n",
      "Epoch 0/499, Training Loss: 0.140384\n",
      "Validation Loss: 0.120351\n",
      "Epoch 50/499, Training Loss: 0.002569\n",
      "Validation Loss: 0.002096\n",
      "Epoch 100/499, Training Loss: 0.001752\n",
      "Validation Loss: 0.001559\n",
      "Epoch 150/499, Training Loss: 0.001364\n",
      "Validation Loss: 0.001205\n",
      "Epoch 200/499, Training Loss: 0.000981\n",
      "Validation Loss: 0.000867\n",
      "Epoch 250/499, Training Loss: 0.000732\n",
      "Validation Loss: 0.000673\n",
      "Epoch 300/499, Training Loss: 0.000606\n",
      "Validation Loss: 0.000558\n",
      "Epoch 350/499, Training Loss: 0.000520\n",
      "Validation Loss: 0.000473\n",
      "Epoch 400/499, Training Loss: 0.000456\n",
      "Validation Loss: 0.000409\n",
      "Epoch 450/499, Training Loss: 0.000597\n",
      "Validation Loss: 0.000558\n",
      "Test Loss: 0.000427\n",
      "model DMLP: IPGP\n",
      "Epoch 0/499, Training Loss: 0.075694\n",
      "Validation Loss: 0.066202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/499, Training Loss: 0.002151\n",
      "Validation Loss: 0.002169\n",
      "Epoch 100/499, Training Loss: 0.001231\n",
      "Validation Loss: 0.001268\n",
      "Epoch 150/499, Training Loss: 0.000871\n",
      "Validation Loss: 0.001235\n",
      "Epoch 200/499, Training Loss: 0.001026\n",
      "Validation Loss: 0.001132\n",
      "Epoch 250/499, Training Loss: 0.000660\n",
      "Validation Loss: 0.000966\n",
      "Epoch 300/499, Training Loss: 0.003557\n",
      "Validation Loss: 0.011329\n",
      "Epoch 350/499, Training Loss: 0.000639\n",
      "Validation Loss: 0.000948\n",
      "Epoch 400/499, Training Loss: 0.000576\n",
      "Validation Loss: 0.000914\n",
      "Epoch 450/499, Training Loss: 0.000550\n",
      "Validation Loss: 0.000901\n",
      "Test Loss: 0.000683\n",
      "model DMLP: BRK-B\n",
      "Epoch 0/499, Training Loss: 0.099988\n",
      "Validation Loss: 0.061927\n",
      "Epoch 50/499, Training Loss: 0.001869\n",
      "Validation Loss: 0.001578\n",
      "Epoch 100/499, Training Loss: 0.001195\n",
      "Validation Loss: 0.001127\n",
      "Epoch 150/499, Training Loss: 0.000960\n",
      "Validation Loss: 0.000883\n",
      "Epoch 200/499, Training Loss: 0.000733\n",
      "Validation Loss: 0.000663\n",
      "Epoch 250/499, Training Loss: 0.000535\n",
      "Validation Loss: 0.000493\n",
      "Epoch 300/499, Training Loss: 0.000415\n",
      "Validation Loss: 0.000407\n",
      "Epoch 350/499, Training Loss: 0.000353\n",
      "Validation Loss: 0.000357\n",
      "Epoch 400/499, Training Loss: 0.000308\n",
      "Validation Loss: 0.000313\n",
      "Epoch 450/499, Training Loss: 0.000272\n",
      "Validation Loss: 0.000277\n",
      "Test Loss: 0.000279\n",
      "model DMLP: SBUX\n",
      "Epoch 0/499, Training Loss: 0.403354\n",
      "Validation Loss: 0.371402\n",
      "Epoch 50/499, Training Loss: 0.005068\n",
      "Validation Loss: 0.005588\n",
      "Epoch 100/499, Training Loss: 0.002309\n",
      "Validation Loss: 0.002624\n",
      "Epoch 150/499, Training Loss: 0.001235\n",
      "Validation Loss: 0.001173\n",
      "Epoch 200/499, Training Loss: 0.000732\n",
      "Validation Loss: 0.000715\n",
      "Epoch 250/499, Training Loss: 0.000615\n",
      "Validation Loss: 0.000630\n",
      "Epoch 300/499, Training Loss: 0.000624\n",
      "Validation Loss: 0.000550\n",
      "Epoch 350/499, Training Loss: 0.000469\n",
      "Validation Loss: 0.000506\n",
      "Epoch 400/499, Training Loss: 0.000464\n",
      "Validation Loss: 0.000498\n",
      "Epoch 450/499, Training Loss: 0.000416\n",
      "Validation Loss: 0.000456\n",
      "Test Loss: 0.003057\n",
      "model DMLP: DJP\n",
      "Epoch 0/499, Training Loss: 0.511547\n",
      "Validation Loss: 0.447047\n",
      "Epoch 50/499, Training Loss: 0.004347\n",
      "Validation Loss: 0.003158\n",
      "Epoch 100/499, Training Loss: 0.002214\n",
      "Validation Loss: 0.001748\n",
      "Epoch 150/499, Training Loss: 0.001524\n",
      "Validation Loss: 0.001212\n",
      "Epoch 200/499, Training Loss: 0.000996\n",
      "Validation Loss: 0.000823\n",
      "Epoch 250/499, Training Loss: 0.000752\n",
      "Validation Loss: 0.000675\n",
      "Epoch 300/499, Training Loss: 0.000635\n",
      "Validation Loss: 0.000606\n",
      "Epoch 350/499, Training Loss: 0.000549\n",
      "Validation Loss: 0.000548\n",
      "Epoch 400/499, Training Loss: 0.000481\n",
      "Validation Loss: 0.000499\n",
      "Epoch 450/499, Training Loss: 0.000426\n",
      "Validation Loss: 0.000455\n",
      "Test Loss: 0.000427\n",
      "model DMLP: XLV\n",
      "Epoch 0/499, Training Loss: 0.374875\n",
      "Validation Loss: 0.335176\n",
      "Epoch 50/499, Training Loss: 0.002002\n",
      "Validation Loss: 0.002051\n",
      "Epoch 100/499, Training Loss: 0.001480\n",
      "Validation Loss: 0.001459\n",
      "Epoch 150/499, Training Loss: 0.001298\n",
      "Validation Loss: 0.001258\n",
      "Epoch 200/499, Training Loss: 0.001122\n",
      "Validation Loss: 0.001057\n",
      "Epoch 250/499, Training Loss: 0.000958\n",
      "Validation Loss: 0.000870\n",
      "Epoch 300/499, Training Loss: 0.000813\n",
      "Validation Loss: 0.000711\n",
      "Epoch 350/499, Training Loss: 0.000695\n",
      "Validation Loss: 0.000589\n",
      "Epoch 400/499, Training Loss: 0.000604\n",
      "Validation Loss: 0.000505\n",
      "Epoch 450/499, Training Loss: 0.000533\n",
      "Validation Loss: 0.000444\n",
      "Test Loss: 0.000433\n",
      "model DMLP: VFSTX\n",
      "Epoch 0/499, Training Loss: 0.493563\n",
      "Validation Loss: 0.345858\n",
      "Epoch 50/499, Training Loss: 0.006228\n",
      "Validation Loss: 0.007377\n",
      "Epoch 100/499, Training Loss: 0.002355\n",
      "Validation Loss: 0.002869\n",
      "Epoch 150/499, Training Loss: 0.001369\n",
      "Validation Loss: 0.001646\n",
      "Epoch 200/499, Training Loss: 0.000917\n",
      "Validation Loss: 0.000987\n",
      "Epoch 250/499, Training Loss: 0.004105\n",
      "Validation Loss: 0.002407\n",
      "Epoch 300/499, Training Loss: 0.000610\n",
      "Validation Loss: 0.000679\n",
      "Epoch 350/499, Training Loss: 0.000668\n",
      "Validation Loss: 0.000641\n",
      "Epoch 400/499, Training Loss: 0.000453\n",
      "Validation Loss: 0.000508\n",
      "Epoch 450/499, Training Loss: 0.000520\n",
      "Validation Loss: 0.000627\n",
      "Test Loss: 0.000522\n",
      "model DMLP: XLP\n",
      "Epoch 0/499, Training Loss: 0.350201\n",
      "Validation Loss: 0.324258\n",
      "Epoch 50/499, Training Loss: 0.002815\n",
      "Validation Loss: 0.003058\n",
      "Epoch 100/499, Training Loss: 0.001707\n",
      "Validation Loss: 0.001849\n",
      "Epoch 150/499, Training Loss: 0.001193\n",
      "Validation Loss: 0.001270\n",
      "Epoch 200/499, Training Loss: 0.000890\n",
      "Validation Loss: 0.000917\n",
      "Epoch 250/499, Training Loss: 0.000721\n",
      "Validation Loss: 0.000712\n",
      "Epoch 300/499, Training Loss: 0.000610\n",
      "Validation Loss: 0.000570\n",
      "Epoch 350/499, Training Loss: 0.000532\n",
      "Validation Loss: 0.000473\n",
      "Epoch 400/499, Training Loss: 0.000474\n",
      "Validation Loss: 0.000409\n",
      "Epoch 450/499, Training Loss: 0.000604\n",
      "Validation Loss: 0.000507\n",
      "Test Loss: 0.000514\n",
      "model DMLP: XLB\n",
      "Epoch 0/499, Training Loss: 0.309902\n",
      "Validation Loss: 0.246132\n",
      "Epoch 50/499, Training Loss: 0.003504\n",
      "Validation Loss: 0.003641\n",
      "Epoch 100/499, Training Loss: 0.002261\n",
      "Validation Loss: 0.002755\n",
      "Epoch 150/499, Training Loss: 0.001772\n",
      "Validation Loss: 0.002097\n",
      "Epoch 200/499, Training Loss: 0.001345\n",
      "Validation Loss: 0.001510\n",
      "Epoch 250/499, Training Loss: 0.001044\n",
      "Validation Loss: 0.001102\n",
      "Epoch 300/499, Training Loss: 0.000847\n",
      "Validation Loss: 0.000845\n",
      "Epoch 350/499, Training Loss: 0.000694\n",
      "Validation Loss: 0.000667\n",
      "Epoch 400/499, Training Loss: 0.000576\n",
      "Validation Loss: 0.000552\n",
      "Epoch 450/499, Training Loss: 0.000909\n",
      "Validation Loss: 0.000654\n",
      "Test Loss: 0.000601\n",
      "model DMLP: XLE\n",
      "Epoch 0/499, Training Loss: 0.310613\n",
      "Validation Loss: 0.260434\n",
      "Epoch 50/499, Training Loss: 0.008554\n",
      "Validation Loss: 0.010366\n",
      "Epoch 100/499, Training Loss: 0.003376\n",
      "Validation Loss: 0.003776\n",
      "Epoch 150/499, Training Loss: 0.002222\n",
      "Validation Loss: 0.002375\n",
      "Epoch 200/499, Training Loss: 0.001463\n",
      "Validation Loss: 0.001636\n",
      "Epoch 250/499, Training Loss: 0.001074\n",
      "Validation Loss: 0.001176\n",
      "Epoch 300/499, Training Loss: 0.000865\n",
      "Validation Loss: 0.000909\n",
      "Epoch 350/499, Training Loss: 0.000715\n",
      "Validation Loss: 0.000726\n",
      "Epoch 400/499, Training Loss: 0.000614\n",
      "Validation Loss: 0.000614\n",
      "Epoch 450/499, Training Loss: 0.000541\n",
      "Validation Loss: 0.000537\n",
      "Test Loss: 0.000591\n",
      "model DMLP: INDA\n",
      "Epoch 0/499, Training Loss: 0.065393\n",
      "Validation Loss: 0.048405\n",
      "Epoch 50/499, Training Loss: 0.001796\n",
      "Validation Loss: 0.001842\n",
      "Epoch 100/499, Training Loss: 0.000880\n",
      "Validation Loss: 0.000697\n",
      "Epoch 150/499, Training Loss: 0.000606\n",
      "Validation Loss: 0.000617\n",
      "Epoch 200/499, Training Loss: 0.000487\n",
      "Validation Loss: 0.000446\n",
      "Epoch 250/499, Training Loss: 0.000656\n",
      "Validation Loss: 0.001196\n",
      "Epoch 300/499, Training Loss: 0.000393\n",
      "Validation Loss: 0.000370\n",
      "Epoch 350/499, Training Loss: 0.001555\n",
      "Validation Loss: 0.000550\n",
      "Epoch 400/499, Training Loss: 0.000351\n",
      "Validation Loss: 0.000336\n",
      "Epoch 450/499, Training Loss: 0.000551\n",
      "Validation Loss: 0.000649\n",
      "Test Loss: 0.000398\n",
      "model DMLP: VCIT\n",
      "Epoch 0/499, Training Loss: 0.258772\n",
      "Validation Loss: 0.243941\n",
      "Epoch 50/499, Training Loss: 0.005798\n",
      "Validation Loss: 0.005578\n",
      "Epoch 100/499, Training Loss: 0.002711\n",
      "Validation Loss: 0.002757\n",
      "Epoch 150/499, Training Loss: 0.001892\n",
      "Validation Loss: 0.001804\n",
      "Epoch 200/499, Training Loss: 0.001338\n",
      "Validation Loss: 0.001166\n",
      "Epoch 250/499, Training Loss: 0.000987\n",
      "Validation Loss: 0.000870\n",
      "Epoch 300/499, Training Loss: 0.000745\n",
      "Validation Loss: 0.000714\n",
      "Epoch 350/499, Training Loss: 0.000582\n",
      "Validation Loss: 0.000599\n",
      "Epoch 400/499, Training Loss: 0.000732\n",
      "Validation Loss: 0.000739\n",
      "Epoch 450/499, Training Loss: 0.000486\n",
      "Validation Loss: 0.000558\n",
      "Test Loss: 0.000687\n",
      "model DMLP: BND\n",
      "Epoch 0/499, Training Loss: 0.321304\n",
      "Validation Loss: 0.288493\n",
      "Epoch 50/499, Training Loss: 0.008112\n",
      "Validation Loss: 0.008505\n",
      "Epoch 100/499, Training Loss: 0.002097\n",
      "Validation Loss: 0.002051\n",
      "Epoch 150/499, Training Loss: 0.001567\n",
      "Validation Loss: 0.001607\n",
      "Epoch 200/499, Training Loss: 0.001008\n",
      "Validation Loss: 0.001124\n",
      "Epoch 250/499, Training Loss: 0.000789\n",
      "Validation Loss: 0.000893\n",
      "Epoch 300/499, Training Loss: 0.000575\n",
      "Validation Loss: 0.000733\n",
      "Epoch 350/499, Training Loss: 0.000483\n",
      "Validation Loss: 0.000651\n",
      "Epoch 400/499, Training Loss: 0.000524\n",
      "Validation Loss: 0.000712\n",
      "Epoch 450/499, Training Loss: 0.000542\n",
      "Validation Loss: 0.000674\n",
      "Test Loss: 0.000619\n",
      "model DMLP: XLF\n",
      "Epoch 0/499, Training Loss: 0.201750\n",
      "Validation Loss: 0.158773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/499, Training Loss: 0.003337\n",
      "Validation Loss: 0.003698\n",
      "Epoch 100/499, Training Loss: 0.001821\n",
      "Validation Loss: 0.002090\n",
      "Epoch 150/499, Training Loss: 0.000919\n",
      "Validation Loss: 0.001015\n",
      "Epoch 200/499, Training Loss: 0.000941\n",
      "Validation Loss: 0.001094\n",
      "Epoch 250/499, Training Loss: 0.000560\n",
      "Validation Loss: 0.000623\n",
      "Epoch 300/499, Training Loss: 0.001618\n",
      "Validation Loss: 0.000615\n",
      "Epoch 350/499, Training Loss: 0.000478\n",
      "Validation Loss: 0.000543\n",
      "Epoch 400/499, Training Loss: 0.000421\n",
      "Validation Loss: 0.000492\n",
      "Epoch 450/499, Training Loss: 0.001084\n",
      "Validation Loss: 0.000573\n",
      "Test Loss: 0.000507\n",
      "model DMLP: IXP\n",
      "Epoch 0/499, Training Loss: 0.130774\n",
      "Validation Loss: 0.097601\n",
      "Epoch 50/499, Training Loss: 0.003040\n",
      "Validation Loss: 0.003373\n",
      "Epoch 100/499, Training Loss: 0.001326\n",
      "Validation Loss: 0.001299\n",
      "Epoch 150/499, Training Loss: 0.000940\n",
      "Validation Loss: 0.001031\n",
      "Epoch 200/499, Training Loss: 0.000721\n",
      "Validation Loss: 0.000863\n",
      "Epoch 250/499, Training Loss: 0.000650\n",
      "Validation Loss: 0.000760\n",
      "Epoch 300/499, Training Loss: 0.000532\n",
      "Validation Loss: 0.000670\n",
      "Epoch 350/499, Training Loss: 0.000490\n",
      "Validation Loss: 0.000652\n",
      "Epoch 400/499, Training Loss: 0.000541\n",
      "Validation Loss: 0.000662\n",
      "Epoch 450/499, Training Loss: 0.000440\n",
      "Validation Loss: 0.000573\n",
      "Test Loss: 0.000507\n",
      "model DMLP: XLI\n",
      "Epoch 0/499, Training Loss: 0.264385\n",
      "Validation Loss: 0.198966\n",
      "Epoch 50/499, Training Loss: 0.002795\n",
      "Validation Loss: 0.002915\n",
      "Epoch 100/499, Training Loss: 0.001623\n",
      "Validation Loss: 0.001896\n",
      "Epoch 150/499, Training Loss: 0.001221\n",
      "Validation Loss: 0.001392\n",
      "Epoch 200/499, Training Loss: 0.000913\n",
      "Validation Loss: 0.000994\n",
      "Epoch 250/499, Training Loss: 0.000722\n",
      "Validation Loss: 0.000765\n",
      "Epoch 300/499, Training Loss: 0.000600\n",
      "Validation Loss: 0.000625\n",
      "Epoch 350/499, Training Loss: 0.000515\n",
      "Validation Loss: 0.000532\n",
      "Epoch 400/499, Training Loss: 0.000451\n",
      "Validation Loss: 0.000466\n",
      "Epoch 450/499, Training Loss: 0.000402\n",
      "Validation Loss: 0.000420\n",
      "Test Loss: 0.000458\n",
      "model DMLP: EFA\n",
      "Epoch 0/499, Training Loss: 0.180890\n",
      "Validation Loss: 0.144924\n",
      "Epoch 50/499, Training Loss: 0.005419\n",
      "Validation Loss: 0.005530\n",
      "Epoch 100/499, Training Loss: 0.002398\n",
      "Validation Loss: 0.002518\n",
      "Epoch 150/499, Training Loss: 0.001463\n",
      "Validation Loss: 0.001506\n",
      "Epoch 200/499, Training Loss: 0.001086\n",
      "Validation Loss: 0.001085\n",
      "Epoch 250/499, Training Loss: 0.000920\n",
      "Validation Loss: 0.000881\n",
      "Epoch 300/499, Training Loss: 0.000807\n",
      "Validation Loss: 0.000773\n",
      "Epoch 350/499, Training Loss: 0.000736\n",
      "Validation Loss: 0.000719\n",
      "Epoch 400/499, Training Loss: 0.000771\n",
      "Validation Loss: 0.000744\n",
      "Epoch 450/499, Training Loss: 0.000655\n",
      "Validation Loss: 0.000705\n",
      "Test Loss: 0.000764\n",
      "model DMLP: XLU\n",
      "Epoch 0/499, Training Loss: 0.414832\n",
      "Validation Loss: 0.350019\n",
      "Epoch 50/499, Training Loss: 0.006299\n",
      "Validation Loss: 0.005642\n",
      "Epoch 100/499, Training Loss: 0.003366\n",
      "Validation Loss: 0.003518\n",
      "Epoch 150/499, Training Loss: 0.002613\n",
      "Validation Loss: 0.002570\n",
      "Epoch 200/499, Training Loss: 0.001880\n",
      "Validation Loss: 0.001679\n",
      "Epoch 250/499, Training Loss: 0.001422\n",
      "Validation Loss: 0.001203\n",
      "Epoch 300/499, Training Loss: 0.001149\n",
      "Validation Loss: 0.000950\n",
      "Epoch 350/499, Training Loss: 0.000948\n",
      "Validation Loss: 0.000782\n",
      "Epoch 400/499, Training Loss: 0.000804\n",
      "Validation Loss: 0.000662\n",
      "Epoch 450/499, Training Loss: 0.000716\n",
      "Validation Loss: 0.000604\n",
      "Test Loss: 0.000682\n",
      "model DMLP: EEM\n",
      "Epoch 0/499, Training Loss: 0.565543\n",
      "Validation Loss: 0.485126\n",
      "Epoch 50/499, Training Loss: 0.005515\n",
      "Validation Loss: 0.006393\n",
      "Epoch 100/499, Training Loss: 0.003606\n",
      "Validation Loss: 0.003853\n",
      "Epoch 150/499, Training Loss: 0.002535\n",
      "Validation Loss: 0.002620\n",
      "Epoch 200/499, Training Loss: 0.001635\n",
      "Validation Loss: 0.001608\n",
      "Epoch 250/499, Training Loss: 0.001266\n",
      "Validation Loss: 0.001236\n",
      "Epoch 300/499, Training Loss: 0.001077\n",
      "Validation Loss: 0.001063\n",
      "Epoch 350/499, Training Loss: 0.000934\n",
      "Validation Loss: 0.000929\n",
      "Epoch 400/499, Training Loss: 0.000826\n",
      "Validation Loss: 0.000826\n",
      "Epoch 450/499, Training Loss: 0.000744\n",
      "Validation Loss: 0.000744\n",
      "Test Loss: 0.000762\n",
      "avg val loss 0.0005117548613203258\n",
      "avg test loss 0.0005303752478977441\n"
     ]
    }
   ],
   "source": [
    "#DMLP\n",
    "dmlp_models = {}\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "input_size = 6\n",
    "\n",
    "test_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for stock in X_train_tensors:\n",
    "    print(\"model DMLP: \" + stock)\n",
    "#     model = LSTMModel(input_dim=input_size, hidden_dim=40)\n",
    "    model = DMLPModel(input_dim=60 * input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trained_model, val_loss = train_model(model, X_train_tensors[stock], y_train_tensors[stock], X_val_tensors[stock], y_val_tensors[stock], criterion, optimizer, num_epochs, patience=0, device=device, print_every=50)\n",
    "    val_losses.append(val_loss.item())\n",
    "    #Test\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(X_test_tensors[stock].to(device))\n",
    "        loss = criterion(outputs, y_test_tensors[stock].to(device).view(-1,1))\n",
    "\n",
    "        print(f\"Test Loss: {loss:.6f}\")\n",
    "        test_losses.append(loss.item())\n",
    "    dmlp_models[stock] = trained_model\n",
    "\n",
    "print(\"avg val loss\", np.mean(np.array(val_losses)))\n",
    "print(\"avg test loss\", np.mean(np.array(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68f2c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model LSTM: DEO\n",
      "Epoch 0/499, Training Loss: 0.185267\n",
      "Validation Loss: 0.090330\n",
      "Epoch 20/499, Training Loss: 0.010379\n",
      "Validation Loss: 0.006577\n",
      "Epoch 40/499, Training Loss: 0.004601\n",
      "Validation Loss: 0.001534\n",
      "Epoch 60/499, Training Loss: 0.003761\n",
      "Validation Loss: 0.000940\n",
      "Epoch 80/499, Training Loss: 0.003240\n",
      "Validation Loss: 0.000865\n",
      "Epoch 100/499, Training Loss: 0.003126\n",
      "Validation Loss: 0.000838\n",
      "Epoch 120/499, Training Loss: 0.002802\n",
      "Validation Loss: 0.000806\n",
      "Epoch 140/499, Training Loss: 0.002543\n",
      "Validation Loss: 0.000779\n",
      "Epoch 160/499, Training Loss: 0.002614\n",
      "Validation Loss: 0.000741\n",
      "Epoch 180/499, Training Loss: 0.002940\n",
      "Validation Loss: 0.000708\n",
      "Epoch 200/499, Training Loss: 0.002024\n",
      "Validation Loss: 0.000650\n",
      "Epoch 220/499, Training Loss: 0.002206\n",
      "Validation Loss: 0.000626\n",
      "Epoch 240/499, Training Loss: 0.002019\n",
      "Validation Loss: 0.000519\n",
      "Epoch 260/499, Training Loss: 0.002370\n",
      "Validation Loss: 0.001013\n",
      "Epoch 280/499, Training Loss: 0.001937\n",
      "Validation Loss: 0.000690\n",
      "Epoch 300/499, Training Loss: 0.001943\n",
      "Validation Loss: 0.000571\n",
      "Epoch 320/499, Training Loss: 0.001828\n",
      "Validation Loss: 0.000467\n",
      "Epoch 340/499, Training Loss: 0.001687\n",
      "Validation Loss: 0.000430\n",
      "Epoch 360/499, Training Loss: 0.001837\n",
      "Validation Loss: 0.000386\n",
      "Epoch 380/499, Training Loss: 0.001626\n",
      "Validation Loss: 0.000372\n",
      "Epoch 400/499, Training Loss: 0.001611\n",
      "Validation Loss: 0.000469\n",
      "Epoch 420/499, Training Loss: 0.001756\n",
      "Validation Loss: 0.000569\n",
      "Epoch 440/499, Training Loss: 0.001551\n",
      "Validation Loss: 0.000369\n",
      "Epoch 460/499, Training Loss: 0.001504\n",
      "Validation Loss: 0.000416\n",
      "Epoch 480/499, Training Loss: 0.001619\n",
      "Validation Loss: 0.000423\n",
      "Test Loss: 0.000729\n",
      "model LSTM: DLR\n",
      "Epoch 0/499, Training Loss: 0.294080\n",
      "Validation Loss: 0.152465\n",
      "Epoch 20/499, Training Loss: 0.039013\n",
      "Validation Loss: 0.024706\n",
      "Epoch 40/499, Training Loss: 0.009640\n",
      "Validation Loss: 0.002140\n",
      "Epoch 60/499, Training Loss: 0.006867\n",
      "Validation Loss: 0.001422\n",
      "Epoch 80/499, Training Loss: 0.005953\n",
      "Validation Loss: 0.001345\n",
      "Epoch 100/499, Training Loss: 0.005403\n",
      "Validation Loss: 0.001173\n",
      "Epoch 120/499, Training Loss: 0.004362\n",
      "Validation Loss: 0.001139\n",
      "Epoch 140/499, Training Loss: 0.004246\n",
      "Validation Loss: 0.001096\n",
      "Epoch 160/499, Training Loss: 0.004005\n",
      "Validation Loss: 0.001063\n",
      "Epoch 180/499, Training Loss: 0.003827\n",
      "Validation Loss: 0.001508\n",
      "Epoch 200/499, Training Loss: 0.003343\n",
      "Validation Loss: 0.001028\n",
      "Epoch 220/499, Training Loss: 0.003295\n",
      "Validation Loss: 0.000990\n",
      "Epoch 240/499, Training Loss: 0.003009\n",
      "Validation Loss: 0.001035\n",
      "Epoch 260/499, Training Loss: 0.002948\n",
      "Validation Loss: 0.000964\n",
      "Epoch 280/499, Training Loss: 0.002637\n",
      "Validation Loss: 0.000946\n",
      "Epoch 300/499, Training Loss: 0.002639\n",
      "Validation Loss: 0.000917\n",
      "Epoch 320/499, Training Loss: 0.002719\n",
      "Validation Loss: 0.001117\n",
      "Epoch 340/499, Training Loss: 0.002365\n",
      "Validation Loss: 0.000827\n",
      "Epoch 360/499, Training Loss: 0.002436\n",
      "Validation Loss: 0.000932\n",
      "Epoch 380/499, Training Loss: 0.002210\n",
      "Validation Loss: 0.000781\n",
      "Epoch 400/499, Training Loss: 0.002010\n",
      "Validation Loss: 0.000763\n",
      "Epoch 420/499, Training Loss: 0.002020\n",
      "Validation Loss: 0.000771\n",
      "Epoch 440/499, Training Loss: 0.001982\n",
      "Validation Loss: 0.000731\n",
      "Epoch 460/499, Training Loss: 0.001852\n",
      "Validation Loss: 0.001047\n",
      "Epoch 480/499, Training Loss: 0.001871\n",
      "Validation Loss: 0.000810\n",
      "Test Loss: 0.000622\n",
      "model LSTM: ULTA\n",
      "Epoch 0/499, Training Loss: 0.207019\n",
      "Validation Loss: 0.076671\n",
      "Epoch 20/499, Training Loss: 0.018577\n",
      "Validation Loss: 0.010991\n",
      "Epoch 40/499, Training Loss: 0.005448\n",
      "Validation Loss: 0.001017\n",
      "Epoch 60/499, Training Loss: 0.003976\n",
      "Validation Loss: 0.000747\n",
      "Epoch 80/499, Training Loss: 0.003793\n",
      "Validation Loss: 0.000817\n",
      "Epoch 100/499, Training Loss: 0.002893\n",
      "Validation Loss: 0.000754\n",
      "Epoch 120/499, Training Loss: 0.002791\n",
      "Validation Loss: 0.000684\n",
      "Epoch 140/499, Training Loss: 0.002257\n",
      "Validation Loss: 0.000691\n",
      "Epoch 160/499, Training Loss: 0.002536\n",
      "Validation Loss: 0.000692\n",
      "Epoch 180/499, Training Loss: 0.002336\n",
      "Validation Loss: 0.000682\n",
      "Epoch 200/499, Training Loss: 0.001990\n",
      "Validation Loss: 0.000619\n",
      "Epoch 220/499, Training Loss: 0.001887\n",
      "Validation Loss: 0.000629\n",
      "Epoch 240/499, Training Loss: 0.001847\n",
      "Validation Loss: 0.000584\n",
      "Epoch 260/499, Training Loss: 0.001775\n",
      "Validation Loss: 0.000567\n",
      "Epoch 280/499, Training Loss: 0.001870\n",
      "Validation Loss: 0.000536\n",
      "Epoch 300/499, Training Loss: 0.001750\n",
      "Validation Loss: 0.000557\n",
      "Epoch 320/499, Training Loss: 0.001608\n",
      "Validation Loss: 0.000624\n",
      "Epoch 340/499, Training Loss: 0.001611\n",
      "Validation Loss: 0.000569\n",
      "Epoch 360/499, Training Loss: 0.001519\n",
      "Validation Loss: 0.000507\n",
      "Epoch 380/499, Training Loss: 0.001646\n",
      "Validation Loss: 0.000808\n",
      "Epoch 400/499, Training Loss: 0.001670\n",
      "Validation Loss: 0.000462\n",
      "Epoch 420/499, Training Loss: 0.001519\n",
      "Validation Loss: 0.000542\n",
      "Epoch 440/499, Training Loss: 0.001347\n",
      "Validation Loss: 0.000515\n",
      "Epoch 460/499, Training Loss: 0.001339\n",
      "Validation Loss: 0.000461\n",
      "Epoch 480/499, Training Loss: 0.001437\n",
      "Validation Loss: 0.000411\n",
      "Test Loss: 0.000537\n",
      "model LSTM: V\n",
      "Epoch 0/499, Training Loss: 0.289147\n",
      "Validation Loss: 0.148949\n",
      "Epoch 20/499, Training Loss: 0.008426\n",
      "Validation Loss: 0.001888\n",
      "Epoch 40/499, Training Loss: 0.005351\n",
      "Validation Loss: 0.000927\n",
      "Epoch 60/499, Training Loss: 0.004413\n",
      "Validation Loss: 0.001049\n",
      "Epoch 80/499, Training Loss: 0.003933\n",
      "Validation Loss: 0.000427\n",
      "Epoch 100/499, Training Loss: 0.003809\n",
      "Validation Loss: 0.000393\n",
      "Epoch 120/499, Training Loss: 0.003204\n",
      "Validation Loss: 0.000725\n",
      "Epoch 140/499, Training Loss: 0.002917\n",
      "Validation Loss: 0.000426\n",
      "Epoch 160/499, Training Loss: 0.002772\n",
      "Validation Loss: 0.000403\n",
      "Epoch 180/499, Training Loss: 0.002615\n",
      "Validation Loss: 0.000423\n",
      "Epoch 200/499, Training Loss: 0.002286\n",
      "Validation Loss: 0.000361\n",
      "Epoch 220/499, Training Loss: 0.002395\n",
      "Validation Loss: 0.000493\n",
      "Epoch 240/499, Training Loss: 0.002233\n",
      "Validation Loss: 0.000406\n",
      "Epoch 260/499, Training Loss: 0.002006\n",
      "Validation Loss: 0.000322\n",
      "Epoch 280/499, Training Loss: 0.002015\n",
      "Validation Loss: 0.000382\n",
      "Epoch 300/499, Training Loss: 0.002136\n",
      "Validation Loss: 0.000312\n",
      "Epoch 320/499, Training Loss: 0.002091\n",
      "Validation Loss: 0.000531\n",
      "Epoch 340/499, Training Loss: 0.001977\n",
      "Validation Loss: 0.000337\n",
      "Epoch 360/499, Training Loss: 0.001872\n",
      "Validation Loss: 0.000393\n",
      "Epoch 380/499, Training Loss: 0.001958\n",
      "Validation Loss: 0.000300\n",
      "Epoch 400/499, Training Loss: 0.001875\n",
      "Validation Loss: 0.000535\n",
      "Epoch 420/499, Training Loss: 0.001857\n",
      "Validation Loss: 0.000317\n",
      "Epoch 440/499, Training Loss: 0.001841\n",
      "Validation Loss: 0.000278\n",
      "Epoch 460/499, Training Loss: 0.001635\n",
      "Validation Loss: 0.000339\n",
      "Epoch 480/499, Training Loss: 0.001571\n",
      "Validation Loss: 0.000568\n",
      "Test Loss: 0.000310\n",
      "model LSTM: DIS\n",
      "Epoch 0/499, Training Loss: 0.158416\n",
      "Validation Loss: 0.061470\n",
      "Epoch 20/499, Training Loss: 0.009156\n",
      "Validation Loss: 0.004583\n",
      "Epoch 40/499, Training Loss: 0.004218\n",
      "Validation Loss: 0.001643\n",
      "Epoch 60/499, Training Loss: 0.003055\n",
      "Validation Loss: 0.001003\n",
      "Epoch 80/499, Training Loss: 0.003478\n",
      "Validation Loss: 0.000956\n",
      "Epoch 100/499, Training Loss: 0.002796\n",
      "Validation Loss: 0.000892\n",
      "Epoch 120/499, Training Loss: 0.002242\n",
      "Validation Loss: 0.001027\n",
      "Epoch 140/499, Training Loss: 0.002299\n",
      "Validation Loss: 0.000844\n",
      "Epoch 160/499, Training Loss: 0.002364\n",
      "Validation Loss: 0.000725\n",
      "Epoch 180/499, Training Loss: 0.002111\n",
      "Validation Loss: 0.000693\n",
      "Epoch 200/499, Training Loss: 0.001776\n",
      "Validation Loss: 0.000628\n",
      "Epoch 220/499, Training Loss: 0.001950\n",
      "Validation Loss: 0.000723\n",
      "Epoch 240/499, Training Loss: 0.001931\n",
      "Validation Loss: 0.000567\n",
      "Epoch 260/499, Training Loss: 0.001721\n",
      "Validation Loss: 0.000735\n",
      "Epoch 280/499, Training Loss: 0.001585\n",
      "Validation Loss: 0.000618\n",
      "Epoch 300/499, Training Loss: 0.001599\n",
      "Validation Loss: 0.000516\n",
      "Epoch 320/499, Training Loss: 0.001420\n",
      "Validation Loss: 0.000686\n",
      "Epoch 340/499, Training Loss: 0.001527\n",
      "Validation Loss: 0.000435\n",
      "Epoch 360/499, Training Loss: 0.001665\n",
      "Validation Loss: 0.000629\n",
      "Epoch 380/499, Training Loss: 0.001527\n",
      "Validation Loss: 0.000417\n",
      "Epoch 400/499, Training Loss: 0.001336\n",
      "Validation Loss: 0.000405\n",
      "Epoch 420/499, Training Loss: 0.001686\n",
      "Validation Loss: 0.000419\n",
      "Epoch 440/499, Training Loss: 0.001483\n",
      "Validation Loss: 0.000412\n",
      "Epoch 460/499, Training Loss: 0.001330\n",
      "Validation Loss: 0.000432\n",
      "Epoch 480/499, Training Loss: 0.001441\n",
      "Validation Loss: 0.000596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000392\n",
      "model LSTM: CME\n",
      "Epoch 0/499, Training Loss: 0.295450\n",
      "Validation Loss: 0.089933\n",
      "Epoch 20/499, Training Loss: 0.072899\n",
      "Validation Loss: 0.055010\n",
      "Epoch 40/499, Training Loss: 0.071912\n",
      "Validation Loss: 0.062863\n",
      "Epoch 60/499, Training Loss: 0.071312\n",
      "Validation Loss: 0.063063\n",
      "Epoch 80/499, Training Loss: 0.028621\n",
      "Validation Loss: 0.014190\n",
      "Epoch 100/499, Training Loss: 0.012154\n",
      "Validation Loss: 0.004982\n",
      "Epoch 120/499, Training Loss: 0.006505\n",
      "Validation Loss: 0.001233\n",
      "Epoch 140/499, Training Loss: 0.005031\n",
      "Validation Loss: 0.001073\n",
      "Epoch 160/499, Training Loss: 0.003959\n",
      "Validation Loss: 0.000855\n",
      "Epoch 180/499, Training Loss: 0.004099\n",
      "Validation Loss: 0.000809\n",
      "Epoch 200/499, Training Loss: 0.003570\n",
      "Validation Loss: 0.000749\n",
      "Epoch 220/499, Training Loss: 0.003456\n",
      "Validation Loss: 0.000833\n",
      "Epoch 240/499, Training Loss: 0.003381\n",
      "Validation Loss: 0.000897\n",
      "Epoch 260/499, Training Loss: 0.003075\n",
      "Validation Loss: 0.000731\n",
      "Epoch 280/499, Training Loss: 0.002886\n",
      "Validation Loss: 0.000708\n",
      "Epoch 300/499, Training Loss: 0.002675\n",
      "Validation Loss: 0.000770\n",
      "Epoch 320/499, Training Loss: 0.002804\n",
      "Validation Loss: 0.000649\n",
      "Epoch 340/499, Training Loss: 0.002534\n",
      "Validation Loss: 0.000708\n",
      "Epoch 360/499, Training Loss: 0.002471\n",
      "Validation Loss: 0.001041\n",
      "Epoch 380/499, Training Loss: 0.002445\n",
      "Validation Loss: 0.000739\n",
      "Epoch 400/499, Training Loss: 0.002508\n",
      "Validation Loss: 0.000829\n",
      "Epoch 420/499, Training Loss: 0.002266\n",
      "Validation Loss: 0.001007\n",
      "Epoch 440/499, Training Loss: 0.002284\n",
      "Validation Loss: 0.000603\n",
      "Epoch 460/499, Training Loss: 0.002276\n",
      "Validation Loss: 0.000847\n",
      "Epoch 480/499, Training Loss: 0.002014\n",
      "Validation Loss: 0.000580\n",
      "Test Loss: 0.000721\n",
      "model LSTM: AAPL\n",
      "Epoch 0/499, Training Loss: 0.179811\n",
      "Validation Loss: 0.103892\n",
      "Epoch 20/499, Training Loss: 0.041078\n",
      "Validation Loss: 0.045955\n",
      "Epoch 40/499, Training Loss: 0.039504\n",
      "Validation Loss: 0.070077\n",
      "Epoch 60/499, Training Loss: 0.040969\n",
      "Validation Loss: 0.040242\n",
      "Epoch 80/499, Training Loss: 0.030384\n",
      "Validation Loss: 0.067866\n",
      "Epoch 100/499, Training Loss: 0.015658\n",
      "Validation Loss: 0.006263\n",
      "Epoch 120/499, Training Loss: 0.013558\n",
      "Validation Loss: 0.009422\n",
      "Epoch 140/499, Training Loss: 0.008306\n",
      "Validation Loss: 0.005162\n",
      "Epoch 160/499, Training Loss: 0.012292\n",
      "Validation Loss: 0.007915\n",
      "Epoch 180/499, Training Loss: 0.005546\n",
      "Validation Loss: 0.001137\n",
      "Epoch 200/499, Training Loss: 0.004766\n",
      "Validation Loss: 0.000736\n",
      "Epoch 220/499, Training Loss: 0.004372\n",
      "Validation Loss: 0.001003\n",
      "Epoch 240/499, Training Loss: 0.003929\n",
      "Validation Loss: 0.001034\n",
      "Epoch 260/499, Training Loss: 0.003665\n",
      "Validation Loss: 0.000573\n",
      "Epoch 280/499, Training Loss: 0.003376\n",
      "Validation Loss: 0.000626\n",
      "Epoch 300/499, Training Loss: 0.003353\n",
      "Validation Loss: 0.000563\n",
      "Epoch 320/499, Training Loss: 0.003402\n",
      "Validation Loss: 0.000488\n",
      "Epoch 340/499, Training Loss: 0.003458\n",
      "Validation Loss: 0.000831\n",
      "Epoch 360/499, Training Loss: 0.003103\n",
      "Validation Loss: 0.000460\n",
      "Epoch 380/499, Training Loss: 0.003105\n",
      "Validation Loss: 0.000478\n",
      "Epoch 400/499, Training Loss: 0.003431\n",
      "Validation Loss: 0.000603\n",
      "Epoch 420/499, Training Loss: 0.002854\n",
      "Validation Loss: 0.000871\n",
      "Epoch 440/499, Training Loss: 0.002757\n",
      "Validation Loss: 0.000400\n",
      "Epoch 460/499, Training Loss: 0.002892\n",
      "Validation Loss: 0.000451\n",
      "Epoch 480/499, Training Loss: 0.003019\n",
      "Validation Loss: 0.000447\n",
      "Test Loss: 0.000414\n",
      "model LSTM: GOOGL\n",
      "Epoch 0/499, Training Loss: 0.261198\n",
      "Validation Loss: 0.139389\n",
      "Epoch 20/499, Training Loss: 0.033463\n",
      "Validation Loss: 0.019674\n",
      "Epoch 40/499, Training Loss: 0.008581\n",
      "Validation Loss: 0.004274\n",
      "Epoch 60/499, Training Loss: 0.004502\n",
      "Validation Loss: 0.001126\n",
      "Epoch 80/499, Training Loss: 0.003492\n",
      "Validation Loss: 0.000565\n",
      "Epoch 100/499, Training Loss: 0.003315\n",
      "Validation Loss: 0.000439\n",
      "Epoch 120/499, Training Loss: 0.003136\n",
      "Validation Loss: 0.000666\n",
      "Epoch 140/499, Training Loss: 0.002671\n",
      "Validation Loss: 0.000582\n",
      "Epoch 160/499, Training Loss: 0.002554\n",
      "Validation Loss: 0.000375\n",
      "Epoch 180/499, Training Loss: 0.002563\n",
      "Validation Loss: 0.000353\n",
      "Epoch 200/499, Training Loss: 0.002320\n",
      "Validation Loss: 0.000441\n",
      "Epoch 220/499, Training Loss: 0.002259\n",
      "Validation Loss: 0.000436\n",
      "Epoch 240/499, Training Loss: 0.002335\n",
      "Validation Loss: 0.000479\n",
      "Epoch 260/499, Training Loss: 0.002140\n",
      "Validation Loss: 0.000350\n",
      "Epoch 280/499, Training Loss: 0.002305\n",
      "Validation Loss: 0.000329\n",
      "Epoch 300/499, Training Loss: 0.001894\n",
      "Validation Loss: 0.000484\n",
      "Epoch 320/499, Training Loss: 0.002000\n",
      "Validation Loss: 0.000377\n",
      "Epoch 340/499, Training Loss: 0.001912\n",
      "Validation Loss: 0.000301\n",
      "Epoch 360/499, Training Loss: 0.001848\n",
      "Validation Loss: 0.000336\n",
      "Epoch 380/499, Training Loss: 0.001586\n",
      "Validation Loss: 0.000330\n",
      "Epoch 400/499, Training Loss: 0.001845\n",
      "Validation Loss: 0.000296\n",
      "Epoch 420/499, Training Loss: 0.001646\n",
      "Validation Loss: 0.000666\n",
      "Epoch 440/499, Training Loss: 0.001479\n",
      "Validation Loss: 0.000438\n",
      "Epoch 460/499, Training Loss: 0.001590\n",
      "Validation Loss: 0.000545\n",
      "Epoch 480/499, Training Loss: 0.001669\n",
      "Validation Loss: 0.000541\n",
      "Test Loss: 0.000250\n",
      "model LSTM: UNH\n",
      "Epoch 0/499, Training Loss: 0.222480\n",
      "Validation Loss: 0.110013\n",
      "Epoch 20/499, Training Loss: 0.016372\n",
      "Validation Loss: 0.008260\n",
      "Epoch 40/499, Training Loss: 0.006327\n",
      "Validation Loss: 0.000903\n",
      "Epoch 60/499, Training Loss: 0.005331\n",
      "Validation Loss: 0.000575\n",
      "Epoch 80/499, Training Loss: 0.004239\n",
      "Validation Loss: 0.000554\n",
      "Epoch 100/499, Training Loss: 0.004433\n",
      "Validation Loss: 0.000543\n",
      "Epoch 120/499, Training Loss: 0.003939\n",
      "Validation Loss: 0.000451\n",
      "Epoch 140/499, Training Loss: 0.003984\n",
      "Validation Loss: 0.000523\n",
      "Epoch 160/499, Training Loss: 0.003455\n",
      "Validation Loss: 0.000454\n",
      "Epoch 180/499, Training Loss: 0.003114\n",
      "Validation Loss: 0.000386\n",
      "Epoch 200/499, Training Loss: 0.003262\n",
      "Validation Loss: 0.000442\n",
      "Epoch 220/499, Training Loss: 0.003183\n",
      "Validation Loss: 0.000408\n",
      "Epoch 240/499, Training Loss: 0.002813\n",
      "Validation Loss: 0.000361\n",
      "Epoch 260/499, Training Loss: 0.002754\n",
      "Validation Loss: 0.000444\n",
      "Epoch 280/499, Training Loss: 0.002573\n",
      "Validation Loss: 0.000384\n",
      "Epoch 300/499, Training Loss: 0.002448\n",
      "Validation Loss: 0.000531\n",
      "Epoch 320/499, Training Loss: 0.002339\n",
      "Validation Loss: 0.000408\n",
      "Epoch 340/499, Training Loss: 0.002279\n",
      "Validation Loss: 0.000335\n",
      "Epoch 360/499, Training Loss: 0.002379\n",
      "Validation Loss: 0.000336\n",
      "Epoch 380/499, Training Loss: 0.002109\n",
      "Validation Loss: 0.000350\n",
      "Epoch 400/499, Training Loss: 0.002400\n",
      "Validation Loss: 0.000363\n",
      "Epoch 420/499, Training Loss: 0.002108\n",
      "Validation Loss: 0.000506\n",
      "Epoch 440/499, Training Loss: 0.002293\n",
      "Validation Loss: 0.000691\n",
      "Epoch 460/499, Training Loss: 0.002214\n",
      "Validation Loss: 0.000277\n",
      "Epoch 480/499, Training Loss: 0.002473\n",
      "Validation Loss: 0.000308\n",
      "Test Loss: 0.000509\n",
      "model LSTM: GS\n",
      "Epoch 0/499, Training Loss: 0.201714\n",
      "Validation Loss: 0.098347\n",
      "Epoch 20/499, Training Loss: 0.017764\n",
      "Validation Loss: 0.013458\n",
      "Epoch 40/499, Training Loss: 0.007266\n",
      "Validation Loss: 0.002716\n",
      "Epoch 60/499, Training Loss: 0.005034\n",
      "Validation Loss: 0.001097\n",
      "Epoch 80/499, Training Loss: 0.004273\n",
      "Validation Loss: 0.000768\n",
      "Epoch 100/499, Training Loss: 0.003573\n",
      "Validation Loss: 0.000689\n",
      "Epoch 120/499, Training Loss: 0.003389\n",
      "Validation Loss: 0.000692\n",
      "Epoch 140/499, Training Loss: 0.002848\n",
      "Validation Loss: 0.000651\n",
      "Epoch 160/499, Training Loss: 0.002539\n",
      "Validation Loss: 0.000584\n",
      "Epoch 180/499, Training Loss: 0.003414\n",
      "Validation Loss: 0.000816\n",
      "Epoch 200/499, Training Loss: 0.002578\n",
      "Validation Loss: 0.000584\n",
      "Epoch 220/499, Training Loss: 0.002354\n",
      "Validation Loss: 0.000638\n",
      "Epoch 240/499, Training Loss: 0.002061\n",
      "Validation Loss: 0.000615\n",
      "Epoch 260/499, Training Loss: 0.002132\n",
      "Validation Loss: 0.000625\n",
      "Epoch 280/499, Training Loss: 0.002071\n",
      "Validation Loss: 0.000515\n",
      "Epoch 300/499, Training Loss: 0.001945\n",
      "Validation Loss: 0.000566\n",
      "Epoch 320/499, Training Loss: 0.001769\n",
      "Validation Loss: 0.000744\n",
      "Epoch 340/499, Training Loss: 0.001694\n",
      "Validation Loss: 0.000494\n",
      "Epoch 360/499, Training Loss: 0.001667\n",
      "Validation Loss: 0.000449\n",
      "Epoch 380/499, Training Loss: 0.001893\n",
      "Validation Loss: 0.000629\n",
      "Epoch 400/499, Training Loss: 0.001508\n",
      "Validation Loss: 0.000452\n",
      "Epoch 420/499, Training Loss: 0.001567\n",
      "Validation Loss: 0.000461\n",
      "Epoch 440/499, Training Loss: 0.001642\n",
      "Validation Loss: 0.000443\n",
      "Epoch 460/499, Training Loss: 0.001628\n",
      "Validation Loss: 0.000399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/499, Training Loss: 0.001448\n",
      "Validation Loss: 0.000481\n",
      "Test Loss: 0.000452\n",
      "model LSTM: QCOM\n",
      "Epoch 0/499, Training Loss: 0.151167\n",
      "Validation Loss: 0.073165\n",
      "Epoch 20/499, Training Loss: 0.015465\n",
      "Validation Loss: 0.011988\n",
      "Epoch 40/499, Training Loss: 0.004574\n",
      "Validation Loss: 0.001124\n",
      "Epoch 60/499, Training Loss: 0.003919\n",
      "Validation Loss: 0.000748\n",
      "Epoch 80/499, Training Loss: 0.003037\n",
      "Validation Loss: 0.000842\n",
      "Epoch 100/499, Training Loss: 0.002759\n",
      "Validation Loss: 0.001181\n",
      "Epoch 120/499, Training Loss: 0.002599\n",
      "Validation Loss: 0.000802\n",
      "Epoch 140/499, Training Loss: 0.002371\n",
      "Validation Loss: 0.000652\n",
      "Epoch 160/499, Training Loss: 0.002145\n",
      "Validation Loss: 0.000631\n",
      "Epoch 180/499, Training Loss: 0.001937\n",
      "Validation Loss: 0.000667\n",
      "Epoch 200/499, Training Loss: 0.002089\n",
      "Validation Loss: 0.000697\n",
      "Epoch 220/499, Training Loss: 0.002041\n",
      "Validation Loss: 0.001121\n",
      "Epoch 240/499, Training Loss: 0.001937\n",
      "Validation Loss: 0.000605\n",
      "Epoch 260/499, Training Loss: 0.001878\n",
      "Validation Loss: 0.000603\n",
      "Epoch 280/499, Training Loss: 0.001642\n",
      "Validation Loss: 0.000536\n",
      "Epoch 300/499, Training Loss: 0.001598\n",
      "Validation Loss: 0.000609\n",
      "Epoch 320/499, Training Loss: 0.001594\n",
      "Validation Loss: 0.000476\n",
      "Epoch 340/499, Training Loss: 0.001533\n",
      "Validation Loss: 0.000505\n",
      "Epoch 360/499, Training Loss: 0.001668\n",
      "Validation Loss: 0.000521\n",
      "Epoch 380/499, Training Loss: 0.001523\n",
      "Validation Loss: 0.000468\n",
      "Epoch 400/499, Training Loss: 0.001429\n",
      "Validation Loss: 0.000533\n",
      "Epoch 420/499, Training Loss: 0.001554\n",
      "Validation Loss: 0.000430\n",
      "Epoch 440/499, Training Loss: 0.001440\n",
      "Validation Loss: 0.000412\n",
      "Epoch 460/499, Training Loss: 0.001539\n",
      "Validation Loss: 0.000573\n",
      "Epoch 480/499, Training Loss: 0.001409\n",
      "Validation Loss: 0.000393\n",
      "Test Loss: 0.000330\n",
      "model LSTM: BA\n",
      "Epoch 0/499, Training Loss: 0.081326\n",
      "Validation Loss: 0.049684\n",
      "Epoch 20/499, Training Loss: 0.006339\n",
      "Validation Loss: 0.004051\n",
      "Epoch 40/499, Training Loss: 0.003471\n",
      "Validation Loss: 0.001346\n",
      "Epoch 60/499, Training Loss: 0.002693\n",
      "Validation Loss: 0.001230\n",
      "Epoch 80/499, Training Loss: 0.002431\n",
      "Validation Loss: 0.001014\n",
      "Epoch 100/499, Training Loss: 0.002309\n",
      "Validation Loss: 0.001059\n",
      "Epoch 120/499, Training Loss: 0.002271\n",
      "Validation Loss: 0.000894\n",
      "Epoch 140/499, Training Loss: 0.001935\n",
      "Validation Loss: 0.000910\n",
      "Epoch 160/499, Training Loss: 0.001904\n",
      "Validation Loss: 0.000806\n",
      "Epoch 180/499, Training Loss: 0.001751\n",
      "Validation Loss: 0.000919\n",
      "Epoch 200/499, Training Loss: 0.001755\n",
      "Validation Loss: 0.000706\n",
      "Epoch 220/499, Training Loss: 0.001756\n",
      "Validation Loss: 0.000699\n",
      "Epoch 240/499, Training Loss: 0.001807\n",
      "Validation Loss: 0.000636\n",
      "Epoch 260/499, Training Loss: 0.001725\n",
      "Validation Loss: 0.000511\n",
      "Epoch 280/499, Training Loss: 0.001473\n",
      "Validation Loss: 0.000499\n",
      "Epoch 300/499, Training Loss: 0.001544\n",
      "Validation Loss: 0.000540\n",
      "Epoch 320/499, Training Loss: 0.001602\n",
      "Validation Loss: 0.000419\n",
      "Epoch 340/499, Training Loss: 0.001694\n",
      "Validation Loss: 0.000667\n",
      "Epoch 360/499, Training Loss: 0.001593\n",
      "Validation Loss: 0.000551\n",
      "Epoch 380/499, Training Loss: 0.001475\n",
      "Validation Loss: 0.000416\n",
      "Epoch 400/499, Training Loss: 0.001433\n",
      "Validation Loss: 0.000377\n",
      "Epoch 420/499, Training Loss: 0.001484\n",
      "Validation Loss: 0.000406\n",
      "Epoch 440/499, Training Loss: 0.001907\n",
      "Validation Loss: 0.001060\n",
      "Epoch 460/499, Training Loss: 0.001760\n",
      "Validation Loss: 0.000520\n",
      "Epoch 480/499, Training Loss: 0.001506\n",
      "Validation Loss: 0.000391\n",
      "Test Loss: 0.000424\n",
      "model LSTM: STZ\n",
      "Epoch 0/499, Training Loss: 0.542222\n",
      "Validation Loss: 0.336058\n",
      "Epoch 20/499, Training Loss: 0.073634\n",
      "Validation Loss: 0.050637\n",
      "Epoch 40/499, Training Loss: 0.026632\n",
      "Validation Loss: 0.011156\n",
      "Epoch 60/499, Training Loss: 0.012593\n",
      "Validation Loss: 0.001242\n",
      "Epoch 80/499, Training Loss: 0.010274\n",
      "Validation Loss: 0.001956\n",
      "Epoch 100/499, Training Loss: 0.009002\n",
      "Validation Loss: 0.001196\n",
      "Epoch 120/499, Training Loss: 0.007974\n",
      "Validation Loss: 0.001083\n",
      "Epoch 140/499, Training Loss: 0.007310\n",
      "Validation Loss: 0.001146\n",
      "Epoch 160/499, Training Loss: 0.006777\n",
      "Validation Loss: 0.001269\n",
      "Epoch 180/499, Training Loss: 0.005856\n",
      "Validation Loss: 0.001421\n",
      "Epoch 200/499, Training Loss: 0.005772\n",
      "Validation Loss: 0.001233\n",
      "Epoch 220/499, Training Loss: 0.005294\n",
      "Validation Loss: 0.000972\n",
      "Epoch 240/499, Training Loss: 0.005392\n",
      "Validation Loss: 0.000934\n",
      "Epoch 260/499, Training Loss: 0.004932\n",
      "Validation Loss: 0.001001\n",
      "Epoch 280/499, Training Loss: 0.004160\n",
      "Validation Loss: 0.001044\n",
      "Epoch 300/499, Training Loss: 0.004094\n",
      "Validation Loss: 0.001271\n",
      "Epoch 320/499, Training Loss: 0.003782\n",
      "Validation Loss: 0.000971\n",
      "Epoch 340/499, Training Loss: 0.003715\n",
      "Validation Loss: 0.000699\n",
      "Epoch 360/499, Training Loss: 0.003260\n",
      "Validation Loss: 0.000752\n",
      "Epoch 380/499, Training Loss: 0.003469\n",
      "Validation Loss: 0.000732\n",
      "Epoch 400/499, Training Loss: 0.002988\n",
      "Validation Loss: 0.000873\n",
      "Epoch 420/499, Training Loss: 0.002931\n",
      "Validation Loss: 0.000647\n",
      "Epoch 440/499, Training Loss: 0.003002\n",
      "Validation Loss: 0.000614\n",
      "Epoch 460/499, Training Loss: 0.002874\n",
      "Validation Loss: 0.000552\n",
      "Epoch 480/499, Training Loss: 0.002963\n",
      "Validation Loss: 0.000598\n",
      "Test Loss: 0.000571\n",
      "model LSTM: NTDOY\n",
      "Epoch 0/499, Training Loss: 0.335365\n",
      "Validation Loss: 0.148120\n",
      "Epoch 20/499, Training Loss: 0.030033\n",
      "Validation Loss: 0.021748\n",
      "Epoch 40/499, Training Loss: 0.025939\n",
      "Validation Loss: 0.015753\n",
      "Epoch 60/499, Training Loss: 0.009115\n",
      "Validation Loss: 0.000908\n",
      "Epoch 80/499, Training Loss: 0.007430\n",
      "Validation Loss: 0.000881\n",
      "Epoch 100/499, Training Loss: 0.006533\n",
      "Validation Loss: 0.000753\n",
      "Epoch 120/499, Training Loss: 0.005691\n",
      "Validation Loss: 0.000732\n",
      "Epoch 140/499, Training Loss: 0.005639\n",
      "Validation Loss: 0.000611\n",
      "Epoch 160/499, Training Loss: 0.005023\n",
      "Validation Loss: 0.000848\n",
      "Epoch 180/499, Training Loss: 0.004602\n",
      "Validation Loss: 0.000553\n",
      "Epoch 200/499, Training Loss: 0.004712\n",
      "Validation Loss: 0.000772\n",
      "Epoch 220/499, Training Loss: 0.003703\n",
      "Validation Loss: 0.000666\n",
      "Epoch 240/499, Training Loss: 0.003974\n",
      "Validation Loss: 0.000566\n",
      "Epoch 260/499, Training Loss: 0.003549\n",
      "Validation Loss: 0.000515\n",
      "Epoch 280/499, Training Loss: 0.002978\n",
      "Validation Loss: 0.000497\n",
      "Epoch 300/499, Training Loss: 0.003043\n",
      "Validation Loss: 0.000608\n",
      "Epoch 320/499, Training Loss: 0.002900\n",
      "Validation Loss: 0.000490\n",
      "Epoch 340/499, Training Loss: 0.002731\n",
      "Validation Loss: 0.000560\n",
      "Epoch 360/499, Training Loss: 0.002398\n",
      "Validation Loss: 0.000620\n",
      "Epoch 380/499, Training Loss: 0.002385\n",
      "Validation Loss: 0.000634\n",
      "Epoch 400/499, Training Loss: 0.002450\n",
      "Validation Loss: 0.000611\n",
      "Epoch 420/499, Training Loss: 0.002305\n",
      "Validation Loss: 0.000636\n",
      "Epoch 440/499, Training Loss: 0.002187\n",
      "Validation Loss: 0.000467\n",
      "Epoch 460/499, Training Loss: 0.002045\n",
      "Validation Loss: 0.000557\n",
      "Epoch 480/499, Training Loss: 0.001925\n",
      "Validation Loss: 0.000463\n",
      "Test Loss: 0.000421\n",
      "model LSTM: MSFT\n",
      "Epoch 0/499, Training Loss: 0.305390\n",
      "Validation Loss: 0.202723\n",
      "Epoch 20/499, Training Loss: 0.020789\n",
      "Validation Loss: 0.023280\n",
      "Epoch 40/499, Training Loss: 0.008994\n",
      "Validation Loss: 0.002545\n",
      "Epoch 60/499, Training Loss: 0.005966\n",
      "Validation Loss: 0.000624\n",
      "Epoch 80/499, Training Loss: 0.004642\n",
      "Validation Loss: 0.000289\n",
      "Epoch 100/499, Training Loss: 0.004276\n",
      "Validation Loss: 0.000349\n",
      "Epoch 120/499, Training Loss: 0.003582\n",
      "Validation Loss: 0.000387\n",
      "Epoch 140/499, Training Loss: 0.003631\n",
      "Validation Loss: 0.000285\n",
      "Epoch 160/499, Training Loss: 0.003434\n",
      "Validation Loss: 0.000302\n",
      "Epoch 180/499, Training Loss: 0.003060\n",
      "Validation Loss: 0.000258\n",
      "Epoch 200/499, Training Loss: 0.002986\n",
      "Validation Loss: 0.000269\n",
      "Epoch 220/499, Training Loss: 0.003248\n",
      "Validation Loss: 0.000309\n",
      "Epoch 240/499, Training Loss: 0.002644\n",
      "Validation Loss: 0.000261\n",
      "Epoch 260/499, Training Loss: 0.002623\n",
      "Validation Loss: 0.000279\n",
      "Epoch 280/499, Training Loss: 0.002645\n",
      "Validation Loss: 0.000289\n",
      "Epoch 300/499, Training Loss: 0.002270\n",
      "Validation Loss: 0.000431\n",
      "Epoch 320/499, Training Loss: 0.002183\n",
      "Validation Loss: 0.000266\n",
      "Epoch 340/499, Training Loss: 0.002171\n",
      "Validation Loss: 0.000326\n",
      "Epoch 360/499, Training Loss: 0.002310\n",
      "Validation Loss: 0.000267\n",
      "Epoch 380/499, Training Loss: 0.002318\n",
      "Validation Loss: 0.000304\n",
      "Epoch 400/499, Training Loss: 0.001978\n",
      "Validation Loss: 0.000400\n",
      "Epoch 420/499, Training Loss: 0.002090\n",
      "Validation Loss: 0.000411\n",
      "Epoch 440/499, Training Loss: 0.001977\n",
      "Validation Loss: 0.000265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/499, Training Loss: 0.002027\n",
      "Validation Loss: 0.000336\n",
      "Epoch 480/499, Training Loss: 0.001886\n",
      "Validation Loss: 0.000282\n",
      "Test Loss: 0.000253\n",
      "model LSTM: DE\n",
      "Epoch 0/499, Training Loss: 0.373869\n",
      "Validation Loss: 0.210640\n",
      "Epoch 20/499, Training Loss: 0.022187\n",
      "Validation Loss: 0.009881\n",
      "Epoch 40/499, Training Loss: 0.010081\n",
      "Validation Loss: 0.002125\n",
      "Epoch 60/499, Training Loss: 0.006606\n",
      "Validation Loss: 0.000898\n",
      "Epoch 80/499, Training Loss: 0.005674\n",
      "Validation Loss: 0.000943\n",
      "Epoch 100/499, Training Loss: 0.005519\n",
      "Validation Loss: 0.000802\n",
      "Epoch 120/499, Training Loss: 0.004742\n",
      "Validation Loss: 0.000766\n",
      "Epoch 140/499, Training Loss: 0.004815\n",
      "Validation Loss: 0.000668\n",
      "Epoch 160/499, Training Loss: 0.004060\n",
      "Validation Loss: 0.000811\n",
      "Epoch 180/499, Training Loss: 0.003572\n",
      "Validation Loss: 0.000683\n",
      "Epoch 200/499, Training Loss: 0.004089\n",
      "Validation Loss: 0.000751\n",
      "Epoch 220/499, Training Loss: 0.003320\n",
      "Validation Loss: 0.000623\n",
      "Epoch 240/499, Training Loss: 0.003719\n",
      "Validation Loss: 0.000595\n",
      "Epoch 260/499, Training Loss: 0.003204\n",
      "Validation Loss: 0.000786\n",
      "Epoch 280/499, Training Loss: 0.003461\n",
      "Validation Loss: 0.000599\n",
      "Epoch 300/499, Training Loss: 0.003170\n",
      "Validation Loss: 0.000700\n",
      "Epoch 320/499, Training Loss: 0.002989\n",
      "Validation Loss: 0.000804\n",
      "Epoch 340/499, Training Loss: 0.003042\n",
      "Validation Loss: 0.000667\n",
      "Epoch 360/499, Training Loss: 0.002656\n",
      "Validation Loss: 0.000609\n",
      "Epoch 380/499, Training Loss: 0.002927\n",
      "Validation Loss: 0.000540\n",
      "Epoch 400/499, Training Loss: 0.002695\n",
      "Validation Loss: 0.000537\n",
      "Epoch 420/499, Training Loss: 0.002575\n",
      "Validation Loss: 0.000594\n",
      "Epoch 440/499, Training Loss: 0.002937\n",
      "Validation Loss: 0.000575\n",
      "Epoch 460/499, Training Loss: 0.002865\n",
      "Validation Loss: 0.000510\n",
      "Epoch 480/499, Training Loss: 0.002423\n",
      "Validation Loss: 0.000549\n",
      "Test Loss: 0.000598\n",
      "model LSTM: BAC\n",
      "Epoch 0/499, Training Loss: 0.188890\n",
      "Validation Loss: 0.063279\n",
      "Epoch 20/499, Training Loss: 0.037304\n",
      "Validation Loss: 0.023381\n",
      "Epoch 40/499, Training Loss: 0.009682\n",
      "Validation Loss: 0.002744\n",
      "Epoch 60/499, Training Loss: 0.005857\n",
      "Validation Loss: 0.001555\n",
      "Epoch 80/499, Training Loss: 0.004390\n",
      "Validation Loss: 0.000868\n",
      "Epoch 100/499, Training Loss: 0.004080\n",
      "Validation Loss: 0.000883\n",
      "Epoch 120/499, Training Loss: 0.004308\n",
      "Validation Loss: 0.000785\n",
      "Epoch 140/499, Training Loss: 0.003390\n",
      "Validation Loss: 0.000771\n",
      "Epoch 160/499, Training Loss: 0.003072\n",
      "Validation Loss: 0.000797\n",
      "Epoch 180/499, Training Loss: 0.002975\n",
      "Validation Loss: 0.000720\n",
      "Epoch 200/499, Training Loss: 0.002804\n",
      "Validation Loss: 0.000703\n",
      "Epoch 220/499, Training Loss: 0.002617\n",
      "Validation Loss: 0.000781\n",
      "Epoch 240/499, Training Loss: 0.002333\n",
      "Validation Loss: 0.000669\n",
      "Epoch 260/499, Training Loss: 0.002389\n",
      "Validation Loss: 0.000866\n",
      "Epoch 280/499, Training Loss: 0.002000\n",
      "Validation Loss: 0.000627\n",
      "Epoch 300/499, Training Loss: 0.001985\n",
      "Validation Loss: 0.000627\n",
      "Epoch 320/499, Training Loss: 0.002079\n",
      "Validation Loss: 0.000631\n",
      "Epoch 340/499, Training Loss: 0.001766\n",
      "Validation Loss: 0.000561\n",
      "Epoch 360/499, Training Loss: 0.001820\n",
      "Validation Loss: 0.000595\n",
      "Epoch 380/499, Training Loss: 0.001955\n",
      "Validation Loss: 0.000563\n",
      "Epoch 400/499, Training Loss: 0.001762\n",
      "Validation Loss: 0.000578\n",
      "Epoch 420/499, Training Loss: 0.001564\n",
      "Validation Loss: 0.000555\n",
      "Epoch 440/499, Training Loss: 0.001884\n",
      "Validation Loss: 0.000496\n",
      "Epoch 460/499, Training Loss: 0.001638\n",
      "Validation Loss: 0.000544\n",
      "Epoch 480/499, Training Loss: 0.001492\n",
      "Validation Loss: 0.000490\n",
      "Test Loss: 0.000523\n",
      "model LSTM: IRM\n",
      "Epoch 0/499, Training Loss: 0.168065\n",
      "Validation Loss: 0.069849\n",
      "Epoch 20/499, Training Loss: 0.016282\n",
      "Validation Loss: 0.015110\n",
      "Epoch 40/499, Training Loss: 0.013799\n",
      "Validation Loss: 0.012846\n",
      "Epoch 60/499, Training Loss: 0.012774\n",
      "Validation Loss: 0.013141\n",
      "Epoch 80/499, Training Loss: 0.018257\n",
      "Validation Loss: 0.019385\n",
      "Epoch 100/499, Training Loss: 0.016195\n",
      "Validation Loss: 0.006163\n",
      "Epoch 120/499, Training Loss: 0.004247\n",
      "Validation Loss: 0.002247\n",
      "Epoch 140/499, Training Loss: 0.002442\n",
      "Validation Loss: 0.000725\n",
      "Epoch 160/499, Training Loss: 0.001990\n",
      "Validation Loss: 0.000481\n",
      "Epoch 180/499, Training Loss: 0.001758\n",
      "Validation Loss: 0.000490\n",
      "Epoch 200/499, Training Loss: 0.001980\n",
      "Validation Loss: 0.000464\n",
      "Epoch 220/499, Training Loss: 0.001742\n",
      "Validation Loss: 0.000431\n",
      "Epoch 240/499, Training Loss: 0.001590\n",
      "Validation Loss: 0.000424\n",
      "Epoch 260/499, Training Loss: 0.001519\n",
      "Validation Loss: 0.000395\n",
      "Epoch 280/499, Training Loss: 0.001511\n",
      "Validation Loss: 0.000356\n",
      "Epoch 300/499, Training Loss: 0.001432\n",
      "Validation Loss: 0.000450\n",
      "Epoch 320/499, Training Loss: 0.001411\n",
      "Validation Loss: 0.000320\n",
      "Epoch 340/499, Training Loss: 0.001520\n",
      "Validation Loss: 0.000314\n",
      "Epoch 360/499, Training Loss: 0.001384\n",
      "Validation Loss: 0.000377\n",
      "Epoch 380/499, Training Loss: 0.001228\n",
      "Validation Loss: 0.000400\n",
      "Epoch 400/499, Training Loss: 0.001331\n",
      "Validation Loss: 0.000405\n",
      "Epoch 420/499, Training Loss: 0.001331\n",
      "Validation Loss: 0.000299\n",
      "Epoch 440/499, Training Loss: 0.001141\n",
      "Validation Loss: 0.000298\n",
      "Epoch 460/499, Training Loss: 0.001231\n",
      "Validation Loss: 0.000334\n",
      "Epoch 480/499, Training Loss: 0.001087\n",
      "Validation Loss: 0.000296\n",
      "Test Loss: 0.000367\n",
      "model LSTM: ADM\n",
      "Epoch 0/499, Training Loss: 0.192635\n",
      "Validation Loss: 0.082477\n",
      "Epoch 20/499, Training Loss: 0.017943\n",
      "Validation Loss: 0.013351\n",
      "Epoch 40/499, Training Loss: 0.007334\n",
      "Validation Loss: 0.003747\n",
      "Epoch 60/499, Training Loss: 0.005202\n",
      "Validation Loss: 0.001013\n",
      "Epoch 80/499, Training Loss: 0.004185\n",
      "Validation Loss: 0.000818\n",
      "Epoch 100/499, Training Loss: 0.003566\n",
      "Validation Loss: 0.000695\n",
      "Epoch 120/499, Training Loss: 0.003473\n",
      "Validation Loss: 0.000708\n",
      "Epoch 140/499, Training Loss: 0.003085\n",
      "Validation Loss: 0.000604\n",
      "Epoch 160/499, Training Loss: 0.002859\n",
      "Validation Loss: 0.000698\n",
      "Epoch 180/499, Training Loss: 0.002406\n",
      "Validation Loss: 0.000558\n",
      "Epoch 200/499, Training Loss: 0.002597\n",
      "Validation Loss: 0.000631\n",
      "Epoch 220/499, Training Loss: 0.002322\n",
      "Validation Loss: 0.000702\n",
      "Epoch 240/499, Training Loss: 0.002033\n",
      "Validation Loss: 0.000527\n",
      "Epoch 260/499, Training Loss: 0.002321\n",
      "Validation Loss: 0.000441\n",
      "Epoch 280/499, Training Loss: 0.002098\n",
      "Validation Loss: 0.000525\n",
      "Epoch 300/499, Training Loss: 0.001898\n",
      "Validation Loss: 0.000461\n",
      "Epoch 320/499, Training Loss: 0.001788\n",
      "Validation Loss: 0.000431\n",
      "Epoch 340/499, Training Loss: 0.001977\n",
      "Validation Loss: 0.000392\n",
      "Epoch 360/499, Training Loss: 0.001785\n",
      "Validation Loss: 0.000527\n",
      "Epoch 380/499, Training Loss: 0.001537\n",
      "Validation Loss: 0.000389\n",
      "Epoch 400/499, Training Loss: 0.001672\n",
      "Validation Loss: 0.000388\n",
      "Epoch 420/499, Training Loss: 0.001563\n",
      "Validation Loss: 0.000504\n",
      "Epoch 440/499, Training Loss: 0.001516\n",
      "Validation Loss: 0.000351\n",
      "Epoch 460/499, Training Loss: 0.001582\n",
      "Validation Loss: 0.000324\n",
      "Epoch 480/499, Training Loss: 0.001537\n",
      "Validation Loss: 0.000339\n",
      "Test Loss: 0.000392\n",
      "model LSTM: GOOG\n",
      "Epoch 0/499, Training Loss: 0.122301\n",
      "Validation Loss: 0.063699\n",
      "Epoch 20/499, Training Loss: 0.018380\n",
      "Validation Loss: 0.010311\n",
      "Epoch 40/499, Training Loss: 0.004687\n",
      "Validation Loss: 0.002467\n",
      "Epoch 60/499, Training Loss: 0.003514\n",
      "Validation Loss: 0.000959\n",
      "Epoch 80/499, Training Loss: 0.002949\n",
      "Validation Loss: 0.000532\n",
      "Epoch 100/499, Training Loss: 0.002584\n",
      "Validation Loss: 0.000563\n",
      "Epoch 120/499, Training Loss: 0.002307\n",
      "Validation Loss: 0.000492\n",
      "Epoch 140/499, Training Loss: 0.002497\n",
      "Validation Loss: 0.000430\n",
      "Epoch 160/499, Training Loss: 0.002533\n",
      "Validation Loss: 0.000513\n",
      "Epoch 180/499, Training Loss: 0.001915\n",
      "Validation Loss: 0.000509\n",
      "Epoch 200/499, Training Loss: 0.001879\n",
      "Validation Loss: 0.000428\n",
      "Epoch 220/499, Training Loss: 0.001862\n",
      "Validation Loss: 0.000497\n",
      "Epoch 240/499, Training Loss: 0.001790\n",
      "Validation Loss: 0.000416\n",
      "Epoch 260/499, Training Loss: 0.001763\n",
      "Validation Loss: 0.000347\n",
      "Epoch 280/499, Training Loss: 0.001745\n",
      "Validation Loss: 0.000426\n",
      "Epoch 300/499, Training Loss: 0.001634\n",
      "Validation Loss: 0.000374\n",
      "Epoch 320/499, Training Loss: 0.001735\n",
      "Validation Loss: 0.000407\n",
      "Epoch 340/499, Training Loss: 0.001571\n",
      "Validation Loss: 0.000365\n",
      "Epoch 360/499, Training Loss: 0.001597\n",
      "Validation Loss: 0.000302\n",
      "Epoch 380/499, Training Loss: 0.001860\n",
      "Validation Loss: 0.000751\n",
      "Epoch 400/499, Training Loss: 0.001650\n",
      "Validation Loss: 0.000379\n",
      "Epoch 420/499, Training Loss: 0.001534\n",
      "Validation Loss: 0.000325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440/499, Training Loss: 0.001491\n",
      "Validation Loss: 0.000352\n",
      "Epoch 460/499, Training Loss: 0.001557\n",
      "Validation Loss: 0.000370\n",
      "Epoch 480/499, Training Loss: 0.001446\n",
      "Validation Loss: 0.000317\n",
      "Test Loss: 0.000278\n",
      "model LSTM: META\n",
      "Epoch 0/499, Training Loss: 0.055064\n",
      "Validation Loss: 0.056756\n",
      "Epoch 20/499, Training Loss: 0.009368\n",
      "Validation Loss: 0.007461\n",
      "Epoch 40/499, Training Loss: 0.002790\n",
      "Validation Loss: 0.000662\n",
      "Epoch 60/499, Training Loss: 0.001953\n",
      "Validation Loss: 0.000510\n",
      "Epoch 80/499, Training Loss: 0.001729\n",
      "Validation Loss: 0.000507\n",
      "Epoch 100/499, Training Loss: 0.001599\n",
      "Validation Loss: 0.000534\n",
      "Epoch 120/499, Training Loss: 0.001484\n",
      "Validation Loss: 0.000533\n",
      "Epoch 140/499, Training Loss: 0.001289\n",
      "Validation Loss: 0.000587\n",
      "Epoch 160/499, Training Loss: 0.001349\n",
      "Validation Loss: 0.000500\n",
      "Epoch 180/499, Training Loss: 0.001479\n",
      "Validation Loss: 0.000521\n",
      "Epoch 200/499, Training Loss: 0.001382\n",
      "Validation Loss: 0.000481\n",
      "Epoch 220/499, Training Loss: 0.001348\n",
      "Validation Loss: 0.000459\n",
      "Epoch 240/499, Training Loss: 0.001176\n",
      "Validation Loss: 0.000469\n",
      "Epoch 260/499, Training Loss: 0.001269\n",
      "Validation Loss: 0.000494\n",
      "Epoch 280/499, Training Loss: 0.001397\n",
      "Validation Loss: 0.000447\n",
      "Epoch 300/499, Training Loss: 0.001159\n",
      "Validation Loss: 0.000416\n",
      "Epoch 320/499, Training Loss: 0.001323\n",
      "Validation Loss: 0.000385\n",
      "Epoch 340/499, Training Loss: 0.001175\n",
      "Validation Loss: 0.000474\n",
      "Epoch 360/499, Training Loss: 0.001256\n",
      "Validation Loss: 0.000450\n",
      "Epoch 380/499, Training Loss: 0.001286\n",
      "Validation Loss: 0.000358\n",
      "Epoch 400/499, Training Loss: 0.001228\n",
      "Validation Loss: 0.000423\n",
      "Epoch 420/499, Training Loss: 0.001046\n",
      "Validation Loss: 0.000327\n",
      "Epoch 440/499, Training Loss: 0.001005\n",
      "Validation Loss: 0.000326\n",
      "Epoch 460/499, Training Loss: 0.001147\n",
      "Validation Loss: 0.000308\n",
      "Epoch 480/499, Training Loss: 0.001063\n",
      "Validation Loss: 0.000247\n",
      "Test Loss: 0.000225\n",
      "model LSTM: PG\n",
      "Epoch 0/499, Training Loss: 0.130576\n",
      "Validation Loss: 0.079579\n",
      "Epoch 20/499, Training Loss: 0.008331\n",
      "Validation Loss: 0.004227\n",
      "Epoch 40/499, Training Loss: 0.004953\n",
      "Validation Loss: 0.001022\n",
      "Epoch 60/499, Training Loss: 0.003668\n",
      "Validation Loss: 0.000959\n",
      "Epoch 80/499, Training Loss: 0.003566\n",
      "Validation Loss: 0.000654\n",
      "Epoch 100/499, Training Loss: 0.003225\n",
      "Validation Loss: 0.000659\n",
      "Epoch 120/499, Training Loss: 0.003012\n",
      "Validation Loss: 0.000629\n",
      "Epoch 140/499, Training Loss: 0.002678\n",
      "Validation Loss: 0.000696\n",
      "Epoch 160/499, Training Loss: 0.002553\n",
      "Validation Loss: 0.000810\n",
      "Epoch 180/499, Training Loss: 0.002402\n",
      "Validation Loss: 0.000661\n",
      "Epoch 200/499, Training Loss: 0.002373\n",
      "Validation Loss: 0.000590\n",
      "Epoch 220/499, Training Loss: 0.002634\n",
      "Validation Loss: 0.001006\n",
      "Epoch 240/499, Training Loss: 0.002517\n",
      "Validation Loss: 0.000531\n",
      "Epoch 260/499, Training Loss: 0.002091\n",
      "Validation Loss: 0.000558\n",
      "Epoch 280/499, Training Loss: 0.002293\n",
      "Validation Loss: 0.000559\n",
      "Epoch 300/499, Training Loss: 0.002025\n",
      "Validation Loss: 0.000455\n",
      "Epoch 320/499, Training Loss: 0.002184\n",
      "Validation Loss: 0.000570\n",
      "Epoch 340/499, Training Loss: 0.002016\n",
      "Validation Loss: 0.000437\n",
      "Epoch 360/499, Training Loss: 0.002172\n",
      "Validation Loss: 0.000528\n",
      "Epoch 380/499, Training Loss: 0.002170\n",
      "Validation Loss: 0.000456\n",
      "Epoch 400/499, Training Loss: 0.001929\n",
      "Validation Loss: 0.000490\n",
      "Epoch 420/499, Training Loss: 0.002106\n",
      "Validation Loss: 0.000464\n",
      "Epoch 440/499, Training Loss: 0.002019\n",
      "Validation Loss: 0.000489\n",
      "Epoch 460/499, Training Loss: 0.002006\n",
      "Validation Loss: 0.000602\n",
      "Epoch 480/499, Training Loss: 0.002088\n",
      "Validation Loss: 0.000419\n",
      "Test Loss: 0.000560\n",
      "model LSTM: ARE\n",
      "Epoch 0/499, Training Loss: 0.319846\n",
      "Validation Loss: 0.174070\n",
      "Epoch 20/499, Training Loss: 0.048764\n",
      "Validation Loss: 0.037888\n",
      "Epoch 40/499, Training Loss: 0.012985\n",
      "Validation Loss: 0.004631\n",
      "Epoch 60/499, Training Loss: 0.006981\n",
      "Validation Loss: 0.001467\n",
      "Epoch 80/499, Training Loss: 0.005152\n",
      "Validation Loss: 0.001503\n",
      "Epoch 100/499, Training Loss: 0.004893\n",
      "Validation Loss: 0.000940\n",
      "Epoch 120/499, Training Loss: 0.004475\n",
      "Validation Loss: 0.000775\n",
      "Epoch 140/499, Training Loss: 0.003392\n",
      "Validation Loss: 0.000961\n",
      "Epoch 160/499, Training Loss: 0.003312\n",
      "Validation Loss: 0.000875\n",
      "Epoch 180/499, Training Loss: 0.003386\n",
      "Validation Loss: 0.001009\n",
      "Epoch 200/499, Training Loss: 0.003111\n",
      "Validation Loss: 0.000665\n",
      "Epoch 220/499, Training Loss: 0.003034\n",
      "Validation Loss: 0.000636\n",
      "Epoch 240/499, Training Loss: 0.002808\n",
      "Validation Loss: 0.000628\n",
      "Epoch 260/499, Training Loss: 0.002592\n",
      "Validation Loss: 0.000610\n",
      "Epoch 280/499, Training Loss: 0.002452\n",
      "Validation Loss: 0.000635\n",
      "Epoch 300/499, Training Loss: 0.002323\n",
      "Validation Loss: 0.000676\n",
      "Epoch 320/499, Training Loss: 0.002320\n",
      "Validation Loss: 0.000956\n",
      "Epoch 340/499, Training Loss: 0.002220\n",
      "Validation Loss: 0.000611\n",
      "Epoch 360/499, Training Loss: 0.002091\n",
      "Validation Loss: 0.000579\n",
      "Epoch 380/499, Training Loss: 0.001999\n",
      "Validation Loss: 0.000581\n",
      "Epoch 400/499, Training Loss: 0.002096\n",
      "Validation Loss: 0.000621\n",
      "Epoch 420/499, Training Loss: 0.001981\n",
      "Validation Loss: 0.000523\n",
      "Epoch 440/499, Training Loss: 0.001902\n",
      "Validation Loss: 0.000513\n",
      "Epoch 460/499, Training Loss: 0.001887\n",
      "Validation Loss: 0.000684\n",
      "Epoch 480/499, Training Loss: 0.001906\n",
      "Validation Loss: 0.000527\n",
      "Test Loss: 0.000407\n",
      "model LSTM: LH\n",
      "Epoch 0/499, Training Loss: 0.235041\n",
      "Validation Loss: 0.071374\n",
      "Epoch 20/499, Training Loss: 0.058674\n",
      "Validation Loss: 0.039696\n",
      "Epoch 40/499, Training Loss: 0.020486\n",
      "Validation Loss: 0.015466\n",
      "Epoch 60/499, Training Loss: 0.014808\n",
      "Validation Loss: 0.011035\n",
      "Epoch 80/499, Training Loss: 0.007046\n",
      "Validation Loss: 0.002132\n",
      "Epoch 100/499, Training Loss: 0.005082\n",
      "Validation Loss: 0.000893\n",
      "Epoch 120/499, Training Loss: 0.004609\n",
      "Validation Loss: 0.000788\n",
      "Epoch 140/499, Training Loss: 0.004178\n",
      "Validation Loss: 0.000855\n",
      "Epoch 160/499, Training Loss: 0.003812\n",
      "Validation Loss: 0.000712\n",
      "Epoch 180/499, Training Loss: 0.003864\n",
      "Validation Loss: 0.000660\n",
      "Epoch 200/499, Training Loss: 0.003537\n",
      "Validation Loss: 0.000679\n",
      "Epoch 220/499, Training Loss: 0.003291\n",
      "Validation Loss: 0.000655\n",
      "Epoch 240/499, Training Loss: 0.003130\n",
      "Validation Loss: 0.000583\n",
      "Epoch 260/499, Training Loss: 0.003033\n",
      "Validation Loss: 0.000549\n",
      "Epoch 280/499, Training Loss: 0.002700\n",
      "Validation Loss: 0.000563\n",
      "Epoch 300/499, Training Loss: 0.002678\n",
      "Validation Loss: 0.000551\n",
      "Epoch 320/499, Training Loss: 0.002547\n",
      "Validation Loss: 0.000683\n",
      "Epoch 340/499, Training Loss: 0.002396\n",
      "Validation Loss: 0.000541\n",
      "Epoch 360/499, Training Loss: 0.002374\n",
      "Validation Loss: 0.000702\n",
      "Epoch 380/499, Training Loss: 0.002635\n",
      "Validation Loss: 0.000519\n",
      "Epoch 400/499, Training Loss: 0.002137\n",
      "Validation Loss: 0.000518\n",
      "Epoch 420/499, Training Loss: 0.002199\n",
      "Validation Loss: 0.000510\n",
      "Epoch 440/499, Training Loss: 0.002258\n",
      "Validation Loss: 0.000505\n",
      "Epoch 460/499, Training Loss: 0.002184\n",
      "Validation Loss: 0.000547\n",
      "Epoch 480/499, Training Loss: 0.002026\n",
      "Validation Loss: 0.000552\n",
      "Test Loss: 0.000568\n",
      "model LSTM: MRK\n",
      "Epoch 0/499, Training Loss: 0.166146\n",
      "Validation Loss: 0.077717\n",
      "Epoch 20/499, Training Loss: 0.023381\n",
      "Validation Loss: 0.014541\n",
      "Epoch 40/499, Training Loss: 0.005215\n",
      "Validation Loss: 0.000969\n",
      "Epoch 60/499, Training Loss: 0.002951\n",
      "Validation Loss: 0.000562\n",
      "Epoch 80/499, Training Loss: 0.002890\n",
      "Validation Loss: 0.000583\n",
      "Epoch 100/499, Training Loss: 0.002462\n",
      "Validation Loss: 0.000559\n",
      "Epoch 120/499, Training Loss: 0.002416\n",
      "Validation Loss: 0.000575\n",
      "Epoch 140/499, Training Loss: 0.002196\n",
      "Validation Loss: 0.000551\n",
      "Epoch 160/499, Training Loss: 0.002071\n",
      "Validation Loss: 0.000742\n",
      "Epoch 180/499, Training Loss: 0.002033\n",
      "Validation Loss: 0.000512\n",
      "Epoch 200/499, Training Loss: 0.002030\n",
      "Validation Loss: 0.000550\n",
      "Epoch 220/499, Training Loss: 0.001968\n",
      "Validation Loss: 0.000515\n",
      "Epoch 240/499, Training Loss: 0.001924\n",
      "Validation Loss: 0.000614\n",
      "Epoch 260/499, Training Loss: 0.002005\n",
      "Validation Loss: 0.000533\n",
      "Epoch 280/499, Training Loss: 0.001626\n",
      "Validation Loss: 0.000481\n",
      "Epoch 300/499, Training Loss: 0.001624\n",
      "Validation Loss: 0.000480\n",
      "Epoch 320/499, Training Loss: 0.001588\n",
      "Validation Loss: 0.000463\n",
      "Epoch 340/499, Training Loss: 0.001530\n",
      "Validation Loss: 0.000517\n",
      "Epoch 360/499, Training Loss: 0.001495\n",
      "Validation Loss: 0.000474\n",
      "Epoch 380/499, Training Loss: 0.001524\n",
      "Validation Loss: 0.000444\n",
      "Epoch 400/499, Training Loss: 0.001534\n",
      "Validation Loss: 0.000509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/499, Training Loss: 0.001380\n",
      "Validation Loss: 0.000398\n",
      "Epoch 440/499, Training Loss: 0.001405\n",
      "Validation Loss: 0.000437\n",
      "Epoch 460/499, Training Loss: 0.001458\n",
      "Validation Loss: 0.000500\n",
      "Epoch 480/499, Training Loss: 0.001512\n",
      "Validation Loss: 0.000398\n",
      "Test Loss: 0.000307\n",
      "model LSTM: LVMUY\n",
      "Epoch 0/499, Training Loss: 0.236772\n",
      "Validation Loss: 0.120149\n",
      "Epoch 20/499, Training Loss: 0.019875\n",
      "Validation Loss: 0.012058\n",
      "Epoch 40/499, Training Loss: 0.007782\n",
      "Validation Loss: 0.000982\n",
      "Epoch 60/499, Training Loss: 0.005331\n",
      "Validation Loss: 0.000410\n",
      "Epoch 80/499, Training Loss: 0.005172\n",
      "Validation Loss: 0.000492\n",
      "Epoch 100/499, Training Loss: 0.004462\n",
      "Validation Loss: 0.000370\n",
      "Epoch 120/499, Training Loss: 0.003717\n",
      "Validation Loss: 0.000440\n",
      "Epoch 140/499, Training Loss: 0.003922\n",
      "Validation Loss: 0.000744\n",
      "Epoch 160/499, Training Loss: 0.003245\n",
      "Validation Loss: 0.000516\n",
      "Epoch 180/499, Training Loss: 0.003081\n",
      "Validation Loss: 0.000356\n",
      "Epoch 200/499, Training Loss: 0.003746\n",
      "Validation Loss: 0.000427\n",
      "Epoch 220/499, Training Loss: 0.002667\n",
      "Validation Loss: 0.000451\n",
      "Epoch 240/499, Training Loss: 0.002546\n",
      "Validation Loss: 0.000386\n",
      "Epoch 260/499, Training Loss: 0.002561\n",
      "Validation Loss: 0.000368\n",
      "Epoch 280/499, Training Loss: 0.002623\n",
      "Validation Loss: 0.000348\n",
      "Epoch 300/499, Training Loss: 0.002362\n",
      "Validation Loss: 0.000400\n",
      "Epoch 320/499, Training Loss: 0.002643\n",
      "Validation Loss: 0.000323\n",
      "Epoch 340/499, Training Loss: 0.002426\n",
      "Validation Loss: 0.000329\n",
      "Epoch 360/499, Training Loss: 0.002128\n",
      "Validation Loss: 0.000335\n",
      "Epoch 380/499, Training Loss: 0.002219\n",
      "Validation Loss: 0.000505\n",
      "Epoch 400/499, Training Loss: 0.001927\n",
      "Validation Loss: 0.000425\n",
      "Epoch 420/499, Training Loss: 0.002173\n",
      "Validation Loss: 0.000391\n",
      "Epoch 440/499, Training Loss: 0.002231\n",
      "Validation Loss: 0.000452\n",
      "Epoch 460/499, Training Loss: 0.002116\n",
      "Validation Loss: 0.000287\n",
      "Epoch 480/499, Training Loss: 0.002001\n",
      "Validation Loss: 0.000276\n",
      "Test Loss: 0.000432\n",
      "model LSTM: LMT\n",
      "Epoch 0/499, Training Loss: 0.188877\n",
      "Validation Loss: 0.087345\n",
      "Epoch 20/499, Training Loss: 0.021352\n",
      "Validation Loss: 0.013888\n",
      "Epoch 40/499, Training Loss: 0.006936\n",
      "Validation Loss: 0.000888\n",
      "Epoch 60/499, Training Loss: 0.004786\n",
      "Validation Loss: 0.000814\n",
      "Epoch 80/499, Training Loss: 0.003985\n",
      "Validation Loss: 0.000928\n",
      "Epoch 100/499, Training Loss: 0.003577\n",
      "Validation Loss: 0.000736\n",
      "Epoch 120/499, Training Loss: 0.003324\n",
      "Validation Loss: 0.000825\n",
      "Epoch 140/499, Training Loss: 0.003104\n",
      "Validation Loss: 0.000704\n",
      "Epoch 160/499, Training Loss: 0.003029\n",
      "Validation Loss: 0.000962\n",
      "Epoch 180/499, Training Loss: 0.002603\n",
      "Validation Loss: 0.000571\n",
      "Epoch 200/499, Training Loss: 0.002435\n",
      "Validation Loss: 0.000548\n",
      "Epoch 220/499, Training Loss: 0.002382\n",
      "Validation Loss: 0.000625\n",
      "Epoch 240/499, Training Loss: 0.002274\n",
      "Validation Loss: 0.000530\n",
      "Epoch 260/499, Training Loss: 0.002255\n",
      "Validation Loss: 0.000701\n",
      "Epoch 280/499, Training Loss: 0.002182\n",
      "Validation Loss: 0.000517\n",
      "Epoch 300/499, Training Loss: 0.002214\n",
      "Validation Loss: 0.000759\n",
      "Epoch 320/499, Training Loss: 0.002197\n",
      "Validation Loss: 0.000672\n",
      "Epoch 340/499, Training Loss: 0.001921\n",
      "Validation Loss: 0.000480\n",
      "Epoch 360/499, Training Loss: 0.001830\n",
      "Validation Loss: 0.000567\n",
      "Epoch 380/499, Training Loss: 0.001834\n",
      "Validation Loss: 0.000441\n",
      "Epoch 400/499, Training Loss: 0.001962\n",
      "Validation Loss: 0.000685\n",
      "Epoch 420/499, Training Loss: 0.001859\n",
      "Validation Loss: 0.000452\n",
      "Epoch 440/499, Training Loss: 0.001693\n",
      "Validation Loss: 0.000432\n",
      "Epoch 460/499, Training Loss: 0.001723\n",
      "Validation Loss: 0.000430\n",
      "Epoch 480/499, Training Loss: 0.001681\n",
      "Validation Loss: 0.000400\n",
      "Test Loss: 0.000404\n",
      "model LSTM: MCD\n",
      "Epoch 0/499, Training Loss: 0.333839\n",
      "Validation Loss: 0.184589\n",
      "Epoch 20/499, Training Loss: 0.010312\n",
      "Validation Loss: 0.001810\n",
      "Epoch 40/499, Training Loss: 0.007028\n",
      "Validation Loss: 0.000592\n",
      "Epoch 60/499, Training Loss: 0.005857\n",
      "Validation Loss: 0.000415\n",
      "Epoch 80/499, Training Loss: 0.004826\n",
      "Validation Loss: 0.000450\n",
      "Epoch 100/499, Training Loss: 0.004563\n",
      "Validation Loss: 0.000432\n",
      "Epoch 120/499, Training Loss: 0.004295\n",
      "Validation Loss: 0.000768\n",
      "Epoch 140/499, Training Loss: 0.003980\n",
      "Validation Loss: 0.000404\n",
      "Epoch 160/499, Training Loss: 0.003517\n",
      "Validation Loss: 0.000579\n",
      "Epoch 180/499, Training Loss: 0.003333\n",
      "Validation Loss: 0.000863\n",
      "Epoch 200/499, Training Loss: 0.003246\n",
      "Validation Loss: 0.000894\n",
      "Epoch 220/499, Training Loss: 0.003268\n",
      "Validation Loss: 0.000405\n",
      "Epoch 240/499, Training Loss: 0.002703\n",
      "Validation Loss: 0.000363\n",
      "Epoch 260/499, Training Loss: 0.003090\n",
      "Validation Loss: 0.000535\n",
      "Epoch 280/499, Training Loss: 0.002958\n",
      "Validation Loss: 0.000535\n",
      "Epoch 300/499, Training Loss: 0.002740\n",
      "Validation Loss: 0.000373\n",
      "Epoch 320/499, Training Loss: 0.002371\n",
      "Validation Loss: 0.000423\n",
      "Epoch 340/499, Training Loss: 0.002390\n",
      "Validation Loss: 0.000822\n",
      "Epoch 360/499, Training Loss: 0.002248\n",
      "Validation Loss: 0.000652\n",
      "Epoch 380/499, Training Loss: 0.002414\n",
      "Validation Loss: 0.000331\n",
      "Epoch 400/499, Training Loss: 0.002198\n",
      "Validation Loss: 0.000462\n",
      "Epoch 420/499, Training Loss: 0.002087\n",
      "Validation Loss: 0.000324\n",
      "Epoch 440/499, Training Loss: 0.002516\n",
      "Validation Loss: 0.000476\n",
      "Epoch 460/499, Training Loss: 0.002154\n",
      "Validation Loss: 0.000409\n",
      "Epoch 480/499, Training Loss: 0.001980\n",
      "Validation Loss: 0.000559\n",
      "Test Loss: 0.000406\n",
      "model LSTM: MSBHF\n",
      "Epoch 0/499, Training Loss: 0.048580\n",
      "Validation Loss: 0.042453\n",
      "Epoch 20/499, Training Loss: 0.004446\n",
      "Validation Loss: 0.002929\n",
      "Epoch 40/499, Training Loss: 0.001996\n",
      "Validation Loss: 0.000409\n",
      "Epoch 60/499, Training Loss: 0.001650\n",
      "Validation Loss: 0.000224\n",
      "Epoch 80/499, Training Loss: 0.001246\n",
      "Validation Loss: 0.000218\n",
      "Epoch 100/499, Training Loss: 0.001145\n",
      "Validation Loss: 0.000233\n",
      "Epoch 120/499, Training Loss: 0.001132\n",
      "Validation Loss: 0.000260\n",
      "Epoch 140/499, Training Loss: 0.001076\n",
      "Validation Loss: 0.000267\n",
      "Epoch 160/499, Training Loss: 0.001009\n",
      "Validation Loss: 0.000286\n",
      "Epoch 180/499, Training Loss: 0.001060\n",
      "Validation Loss: 0.000255\n",
      "Epoch 200/499, Training Loss: 0.001017\n",
      "Validation Loss: 0.000250\n",
      "Epoch 220/499, Training Loss: 0.001029\n",
      "Validation Loss: 0.000208\n",
      "Epoch 240/499, Training Loss: 0.000837\n",
      "Validation Loss: 0.000201\n",
      "Epoch 260/499, Training Loss: 0.000951\n",
      "Validation Loss: 0.000302\n",
      "Epoch 280/499, Training Loss: 0.000860\n",
      "Validation Loss: 0.000277\n",
      "Epoch 300/499, Training Loss: 0.000834\n",
      "Validation Loss: 0.000225\n",
      "Epoch 320/499, Training Loss: 0.000851\n",
      "Validation Loss: 0.000249\n",
      "Epoch 340/499, Training Loss: 0.000847\n",
      "Validation Loss: 0.000268\n",
      "Epoch 360/499, Training Loss: 0.000943\n",
      "Validation Loss: 0.000198\n",
      "Epoch 380/499, Training Loss: 0.000835\n",
      "Validation Loss: 0.000258\n",
      "Epoch 400/499, Training Loss: 0.000926\n",
      "Validation Loss: 0.000264\n",
      "Epoch 420/499, Training Loss: 0.000887\n",
      "Validation Loss: 0.000202\n",
      "Epoch 440/499, Training Loss: 0.000782\n",
      "Validation Loss: 0.000227\n",
      "Epoch 460/499, Training Loss: 0.000918\n",
      "Validation Loss: 0.000214\n",
      "Epoch 480/499, Training Loss: 0.000806\n",
      "Validation Loss: 0.000235\n",
      "Test Loss: 0.000236\n",
      "model LSTM: JCI\n",
      "Epoch 0/499, Training Loss: 0.255906\n",
      "Validation Loss: 0.113818\n",
      "Epoch 20/499, Training Loss: 0.011591\n",
      "Validation Loss: 0.003717\n",
      "Epoch 40/499, Training Loss: 0.006385\n",
      "Validation Loss: 0.001100\n",
      "Epoch 60/499, Training Loss: 0.004598\n",
      "Validation Loss: 0.000924\n",
      "Epoch 80/499, Training Loss: 0.004154\n",
      "Validation Loss: 0.000990\n",
      "Epoch 100/499, Training Loss: 0.003888\n",
      "Validation Loss: 0.001196\n",
      "Epoch 120/499, Training Loss: 0.003460\n",
      "Validation Loss: 0.000749\n",
      "Epoch 140/499, Training Loss: 0.003441\n",
      "Validation Loss: 0.000800\n",
      "Epoch 160/499, Training Loss: 0.002813\n",
      "Validation Loss: 0.000856\n",
      "Epoch 180/499, Training Loss: 0.002963\n",
      "Validation Loss: 0.000739\n",
      "Epoch 200/499, Training Loss: 0.002740\n",
      "Validation Loss: 0.000669\n",
      "Epoch 220/499, Training Loss: 0.002304\n",
      "Validation Loss: 0.000755\n",
      "Epoch 240/499, Training Loss: 0.002364\n",
      "Validation Loss: 0.000875\n",
      "Epoch 260/499, Training Loss: 0.001935\n",
      "Validation Loss: 0.000918\n",
      "Epoch 280/499, Training Loss: 0.001765\n",
      "Validation Loss: 0.000530\n",
      "Epoch 300/499, Training Loss: 0.001772\n",
      "Validation Loss: 0.000567\n",
      "Epoch 320/499, Training Loss: 0.001880\n",
      "Validation Loss: 0.000859\n",
      "Epoch 340/499, Training Loss: 0.001743\n",
      "Validation Loss: 0.000613\n",
      "Epoch 360/499, Training Loss: 0.001493\n",
      "Validation Loss: 0.000537\n",
      "Epoch 380/499, Training Loss: 0.001474\n",
      "Validation Loss: 0.000543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/499, Training Loss: 0.001560\n",
      "Validation Loss: 0.000489\n",
      "Epoch 420/499, Training Loss: 0.001520\n",
      "Validation Loss: 0.000546\n",
      "Epoch 440/499, Training Loss: 0.001462\n",
      "Validation Loss: 0.000412\n",
      "Epoch 460/499, Training Loss: 0.001289\n",
      "Validation Loss: 0.000427\n",
      "Epoch 480/499, Training Loss: 0.001293\n",
      "Validation Loss: 0.000394\n",
      "Test Loss: 0.000456\n",
      "model LSTM: MDLZ\n",
      "Epoch 0/499, Training Loss: 0.351904\n",
      "Validation Loss: 0.209859\n",
      "Epoch 20/499, Training Loss: 0.060730\n",
      "Validation Loss: 0.054253\n",
      "Epoch 40/499, Training Loss: 0.016309\n",
      "Validation Loss: 0.005731\n",
      "Epoch 60/499, Training Loss: 0.009899\n",
      "Validation Loss: 0.001421\n",
      "Epoch 80/499, Training Loss: 0.006748\n",
      "Validation Loss: 0.000767\n",
      "Epoch 100/499, Training Loss: 0.006481\n",
      "Validation Loss: 0.000622\n",
      "Epoch 120/499, Training Loss: 0.006219\n",
      "Validation Loss: 0.000591\n",
      "Epoch 140/499, Training Loss: 0.005708\n",
      "Validation Loss: 0.000595\n",
      "Epoch 160/499, Training Loss: 0.004871\n",
      "Validation Loss: 0.000638\n",
      "Epoch 180/499, Training Loss: 0.004678\n",
      "Validation Loss: 0.000532\n",
      "Epoch 200/499, Training Loss: 0.004441\n",
      "Validation Loss: 0.000514\n",
      "Epoch 220/499, Training Loss: 0.004230\n",
      "Validation Loss: 0.001045\n",
      "Epoch 240/499, Training Loss: 0.003882\n",
      "Validation Loss: 0.000483\n",
      "Epoch 260/499, Training Loss: 0.003738\n",
      "Validation Loss: 0.000484\n",
      "Epoch 280/499, Training Loss: 0.003637\n",
      "Validation Loss: 0.000479\n",
      "Epoch 300/499, Training Loss: 0.003062\n",
      "Validation Loss: 0.000451\n",
      "Epoch 320/499, Training Loss: 0.003080\n",
      "Validation Loss: 0.000470\n",
      "Epoch 340/499, Training Loss: 0.002973\n",
      "Validation Loss: 0.000428\n",
      "Epoch 360/499, Training Loss: 0.002703\n",
      "Validation Loss: 0.000473\n",
      "Epoch 380/499, Training Loss: 0.002590\n",
      "Validation Loss: 0.000472\n",
      "Epoch 400/499, Training Loss: 0.002735\n",
      "Validation Loss: 0.000475\n",
      "Epoch 420/499, Training Loss: 0.002293\n",
      "Validation Loss: 0.000416\n",
      "Epoch 440/499, Training Loss: 0.002501\n",
      "Validation Loss: 0.000444\n",
      "Epoch 460/499, Training Loss: 0.002295\n",
      "Validation Loss: 0.000396\n",
      "Epoch 480/499, Training Loss: 0.002109\n",
      "Validation Loss: 0.000401\n",
      "Test Loss: 0.000537\n",
      "model LSTM: ORCL\n",
      "Epoch 0/499, Training Loss: 0.316926\n",
      "Validation Loss: 0.165905\n",
      "Epoch 20/499, Training Loss: 0.035889\n",
      "Validation Loss: 0.018908\n",
      "Epoch 40/499, Training Loss: 0.009519\n",
      "Validation Loss: 0.002988\n",
      "Epoch 60/499, Training Loss: 0.006152\n",
      "Validation Loss: 0.001010\n",
      "Epoch 80/499, Training Loss: 0.004631\n",
      "Validation Loss: 0.000832\n",
      "Epoch 100/499, Training Loss: 0.004397\n",
      "Validation Loss: 0.000574\n",
      "Epoch 120/499, Training Loss: 0.003722\n",
      "Validation Loss: 0.000579\n",
      "Epoch 140/499, Training Loss: 0.003998\n",
      "Validation Loss: 0.000540\n",
      "Epoch 160/499, Training Loss: 0.003460\n",
      "Validation Loss: 0.000544\n",
      "Epoch 180/499, Training Loss: 0.003152\n",
      "Validation Loss: 0.000527\n",
      "Epoch 200/499, Training Loss: 0.002985\n",
      "Validation Loss: 0.000528\n",
      "Epoch 220/499, Training Loss: 0.002558\n",
      "Validation Loss: 0.000513\n",
      "Epoch 240/499, Training Loss: 0.002665\n",
      "Validation Loss: 0.000457\n",
      "Epoch 260/499, Training Loss: 0.002358\n",
      "Validation Loss: 0.000472\n",
      "Epoch 280/499, Training Loss: 0.002156\n",
      "Validation Loss: 0.000423\n",
      "Epoch 300/499, Training Loss: 0.002388\n",
      "Validation Loss: 0.000420\n",
      "Epoch 320/499, Training Loss: 0.002144\n",
      "Validation Loss: 0.000437\n",
      "Epoch 340/499, Training Loss: 0.002365\n",
      "Validation Loss: 0.000398\n",
      "Epoch 360/499, Training Loss: 0.002230\n",
      "Validation Loss: 0.000377\n",
      "Epoch 380/499, Training Loss: 0.001921\n",
      "Validation Loss: 0.000494\n",
      "Epoch 400/499, Training Loss: 0.002138\n",
      "Validation Loss: 0.000420\n",
      "Epoch 420/499, Training Loss: 0.001726\n",
      "Validation Loss: 0.000359\n",
      "Epoch 440/499, Training Loss: 0.001831\n",
      "Validation Loss: 0.000399\n",
      "Epoch 460/499, Training Loss: 0.001628\n",
      "Validation Loss: 0.000330\n",
      "Epoch 480/499, Training Loss: 0.002139\n",
      "Validation Loss: 0.000501\n",
      "Test Loss: 0.000373\n",
      "model LSTM: HCA\n",
      "Epoch 0/499, Training Loss: 0.180564\n",
      "Validation Loss: 0.095304\n",
      "Epoch 20/499, Training Loss: 0.012127\n",
      "Validation Loss: 0.007311\n",
      "Epoch 40/499, Training Loss: 0.005036\n",
      "Validation Loss: 0.000777\n",
      "Epoch 60/499, Training Loss: 0.003926\n",
      "Validation Loss: 0.000598\n",
      "Epoch 80/499, Training Loss: 0.003176\n",
      "Validation Loss: 0.000437\n",
      "Epoch 100/499, Training Loss: 0.003179\n",
      "Validation Loss: 0.000401\n",
      "Epoch 120/499, Training Loss: 0.003122\n",
      "Validation Loss: 0.000408\n",
      "Epoch 140/499, Training Loss: 0.002538\n",
      "Validation Loss: 0.000448\n",
      "Epoch 160/499, Training Loss: 0.002351\n",
      "Validation Loss: 0.000486\n",
      "Epoch 180/499, Training Loss: 0.002293\n",
      "Validation Loss: 0.000368\n",
      "Epoch 200/499, Training Loss: 0.002239\n",
      "Validation Loss: 0.000361\n",
      "Epoch 220/499, Training Loss: 0.002555\n",
      "Validation Loss: 0.000415\n",
      "Epoch 240/499, Training Loss: 0.002264\n",
      "Validation Loss: 0.000819\n",
      "Epoch 260/499, Training Loss: 0.002008\n",
      "Validation Loss: 0.000433\n",
      "Epoch 280/499, Training Loss: 0.001950\n",
      "Validation Loss: 0.000329\n",
      "Epoch 300/499, Training Loss: 0.001841\n",
      "Validation Loss: 0.000317\n",
      "Epoch 320/499, Training Loss: 0.001938\n",
      "Validation Loss: 0.000352\n",
      "Epoch 340/499, Training Loss: 0.001805\n",
      "Validation Loss: 0.000453\n",
      "Epoch 360/499, Training Loss: 0.001801\n",
      "Validation Loss: 0.000309\n",
      "Epoch 380/499, Training Loss: 0.001823\n",
      "Validation Loss: 0.000313\n",
      "Epoch 400/499, Training Loss: 0.001723\n",
      "Validation Loss: 0.000274\n",
      "Epoch 420/499, Training Loss: 0.001862\n",
      "Validation Loss: 0.000327\n",
      "Epoch 440/499, Training Loss: 0.001737\n",
      "Validation Loss: 0.000324\n",
      "Epoch 460/499, Training Loss: 0.001709\n",
      "Validation Loss: 0.000285\n",
      "Epoch 480/499, Training Loss: 0.001777\n",
      "Validation Loss: 0.000403\n",
      "Test Loss: 0.000383\n",
      "model LSTM: NVDA\n",
      "Epoch 0/499, Training Loss: 0.100697\n",
      "Validation Loss: 0.042020\n",
      "Epoch 20/499, Training Loss: 0.010851\n",
      "Validation Loss: 0.007626\n",
      "Epoch 40/499, Training Loss: 0.002702\n",
      "Validation Loss: 0.000488\n",
      "Epoch 60/499, Training Loss: 0.001718\n",
      "Validation Loss: 0.000159\n",
      "Epoch 80/499, Training Loss: 0.001737\n",
      "Validation Loss: 0.000175\n",
      "Epoch 100/499, Training Loss: 0.001175\n",
      "Validation Loss: 0.000265\n",
      "Epoch 120/499, Training Loss: 0.001258\n",
      "Validation Loss: 0.000171\n",
      "Epoch 140/499, Training Loss: 0.001010\n",
      "Validation Loss: 0.000327\n",
      "Epoch 160/499, Training Loss: 0.001149\n",
      "Validation Loss: 0.000395\n",
      "Epoch 180/499, Training Loss: 0.000855\n",
      "Validation Loss: 0.000230\n",
      "Epoch 200/499, Training Loss: 0.000803\n",
      "Validation Loss: 0.000173\n",
      "Epoch 220/499, Training Loss: 0.000973\n",
      "Validation Loss: 0.000245\n",
      "Epoch 240/499, Training Loss: 0.000697\n",
      "Validation Loss: 0.000352\n",
      "Epoch 260/499, Training Loss: 0.000813\n",
      "Validation Loss: 0.000161\n",
      "Epoch 280/499, Training Loss: 0.000809\n",
      "Validation Loss: 0.000214\n",
      "Epoch 300/499, Training Loss: 0.000819\n",
      "Validation Loss: 0.000240\n",
      "Epoch 320/499, Training Loss: 0.000713\n",
      "Validation Loss: 0.000179\n",
      "Epoch 340/499, Training Loss: 0.000667\n",
      "Validation Loss: 0.000231\n",
      "Epoch 360/499, Training Loss: 0.000686\n",
      "Validation Loss: 0.000251\n",
      "Epoch 380/499, Training Loss: 0.000726\n",
      "Validation Loss: 0.000175\n",
      "Epoch 400/499, Training Loss: 0.000685\n",
      "Validation Loss: 0.000218\n",
      "Epoch 420/499, Training Loss: 0.000561\n",
      "Validation Loss: 0.000225\n",
      "Epoch 440/499, Training Loss: 0.000668\n",
      "Validation Loss: 0.000198\n",
      "Epoch 460/499, Training Loss: 0.000732\n",
      "Validation Loss: 0.000176\n",
      "Epoch 480/499, Training Loss: 0.000652\n",
      "Validation Loss: 0.000179\n",
      "Test Loss: 0.000258\n",
      "model LSTM: WFC\n",
      "Epoch 0/499, Training Loss: 0.293326\n",
      "Validation Loss: 0.128903\n",
      "Epoch 20/499, Training Loss: 0.012578\n",
      "Validation Loss: 0.009993\n",
      "Epoch 40/499, Training Loss: 0.006825\n",
      "Validation Loss: 0.002341\n",
      "Epoch 60/499, Training Loss: 0.005630\n",
      "Validation Loss: 0.001416\n",
      "Epoch 80/499, Training Loss: 0.004882\n",
      "Validation Loss: 0.001504\n",
      "Epoch 100/499, Training Loss: 0.004993\n",
      "Validation Loss: 0.001203\n",
      "Epoch 120/499, Training Loss: 0.003995\n",
      "Validation Loss: 0.001335\n",
      "Epoch 140/499, Training Loss: 0.003526\n",
      "Validation Loss: 0.001034\n",
      "Epoch 160/499, Training Loss: 0.003285\n",
      "Validation Loss: 0.001038\n",
      "Epoch 180/499, Training Loss: 0.002966\n",
      "Validation Loss: 0.000949\n",
      "Epoch 200/499, Training Loss: 0.002767\n",
      "Validation Loss: 0.001006\n",
      "Epoch 220/499, Training Loss: 0.002486\n",
      "Validation Loss: 0.000915\n",
      "Epoch 240/499, Training Loss: 0.002309\n",
      "Validation Loss: 0.000867\n",
      "Epoch 260/499, Training Loss: 0.002219\n",
      "Validation Loss: 0.000766\n",
      "Epoch 280/499, Training Loss: 0.002020\n",
      "Validation Loss: 0.000656\n",
      "Epoch 300/499, Training Loss: 0.001938\n",
      "Validation Loss: 0.000713\n",
      "Epoch 320/499, Training Loss: 0.001998\n",
      "Validation Loss: 0.001042\n",
      "Epoch 340/499, Training Loss: 0.001872\n",
      "Validation Loss: 0.000746\n",
      "Epoch 360/499, Training Loss: 0.001616\n",
      "Validation Loss: 0.000537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/499, Training Loss: 0.001564\n",
      "Validation Loss: 0.000508\n",
      "Epoch 400/499, Training Loss: 0.001357\n",
      "Validation Loss: 0.000480\n",
      "Epoch 420/499, Training Loss: 0.001375\n",
      "Validation Loss: 0.000540\n",
      "Epoch 440/499, Training Loss: 0.001315\n",
      "Validation Loss: 0.000552\n",
      "Epoch 460/499, Training Loss: 0.001332\n",
      "Validation Loss: 0.000459\n",
      "Epoch 480/499, Training Loss: 0.001250\n",
      "Validation Loss: 0.000509\n",
      "Test Loss: 0.000422\n",
      "model LSTM: PEP\n",
      "Epoch 0/499, Training Loss: 0.171937\n",
      "Validation Loss: 0.078156\n",
      "Epoch 20/499, Training Loss: 0.028932\n",
      "Validation Loss: 0.012069\n",
      "Epoch 40/499, Training Loss: 0.006627\n",
      "Validation Loss: 0.001453\n",
      "Epoch 60/499, Training Loss: 0.004468\n",
      "Validation Loss: 0.000719\n",
      "Epoch 80/499, Training Loss: 0.004130\n",
      "Validation Loss: 0.000531\n",
      "Epoch 100/499, Training Loss: 0.003877\n",
      "Validation Loss: 0.000637\n",
      "Epoch 120/499, Training Loss: 0.003461\n",
      "Validation Loss: 0.000511\n",
      "Epoch 140/499, Training Loss: 0.003498\n",
      "Validation Loss: 0.000495\n",
      "Epoch 160/499, Training Loss: 0.003512\n",
      "Validation Loss: 0.000479\n",
      "Epoch 180/499, Training Loss: 0.002964\n",
      "Validation Loss: 0.000675\n",
      "Epoch 200/499, Training Loss: 0.002776\n",
      "Validation Loss: 0.000442\n",
      "Epoch 220/499, Training Loss: 0.002844\n",
      "Validation Loss: 0.000586\n",
      "Epoch 240/499, Training Loss: 0.002476\n",
      "Validation Loss: 0.000441\n",
      "Epoch 260/499, Training Loss: 0.002460\n",
      "Validation Loss: 0.000483\n",
      "Epoch 280/499, Training Loss: 0.002301\n",
      "Validation Loss: 0.000498\n",
      "Epoch 300/499, Training Loss: 0.002206\n",
      "Validation Loss: 0.000419\n",
      "Epoch 320/499, Training Loss: 0.002169\n",
      "Validation Loss: 0.000401\n",
      "Epoch 340/499, Training Loss: 0.002409\n",
      "Validation Loss: 0.000646\n",
      "Epoch 360/499, Training Loss: 0.002150\n",
      "Validation Loss: 0.000766\n",
      "Epoch 380/499, Training Loss: 0.002098\n",
      "Validation Loss: 0.000379\n",
      "Epoch 400/499, Training Loss: 0.002171\n",
      "Validation Loss: 0.000359\n",
      "Epoch 420/499, Training Loss: 0.001969\n",
      "Validation Loss: 0.000369\n",
      "Epoch 440/499, Training Loss: 0.002039\n",
      "Validation Loss: 0.000415\n",
      "Epoch 460/499, Training Loss: 0.001987\n",
      "Validation Loss: 0.000377\n",
      "Epoch 480/499, Training Loss: 0.001792\n",
      "Validation Loss: 0.000436\n",
      "Test Loss: 0.000688\n",
      "model LSTM: IPGP\n",
      "Epoch 0/499, Training Loss: 0.195579\n",
      "Validation Loss: 0.087642\n",
      "Epoch 20/499, Training Loss: 0.012039\n",
      "Validation Loss: 0.005809\n",
      "Epoch 40/499, Training Loss: 0.006168\n",
      "Validation Loss: 0.001579\n",
      "Epoch 60/499, Training Loss: 0.004218\n",
      "Validation Loss: 0.001139\n",
      "Epoch 80/499, Training Loss: 0.003854\n",
      "Validation Loss: 0.001235\n",
      "Epoch 100/499, Training Loss: 0.003440\n",
      "Validation Loss: 0.001053\n",
      "Epoch 120/499, Training Loss: 0.003230\n",
      "Validation Loss: 0.001070\n",
      "Epoch 140/499, Training Loss: 0.002832\n",
      "Validation Loss: 0.001303\n",
      "Epoch 160/499, Training Loss: 0.003220\n",
      "Validation Loss: 0.000998\n",
      "Epoch 180/499, Training Loss: 0.002436\n",
      "Validation Loss: 0.001142\n",
      "Epoch 200/499, Training Loss: 0.002291\n",
      "Validation Loss: 0.000982\n",
      "Epoch 220/499, Training Loss: 0.002303\n",
      "Validation Loss: 0.000997\n",
      "Epoch 240/499, Training Loss: 0.002306\n",
      "Validation Loss: 0.000984\n",
      "Epoch 260/499, Training Loss: 0.002265\n",
      "Validation Loss: 0.001069\n",
      "Epoch 280/499, Training Loss: 0.002153\n",
      "Validation Loss: 0.000964\n",
      "Epoch 300/499, Training Loss: 0.002027\n",
      "Validation Loss: 0.000946\n",
      "Epoch 320/499, Training Loss: 0.002151\n",
      "Validation Loss: 0.001375\n",
      "Epoch 340/499, Training Loss: 0.002011\n",
      "Validation Loss: 0.000892\n",
      "Epoch 360/499, Training Loss: 0.001951\n",
      "Validation Loss: 0.000996\n",
      "Epoch 380/499, Training Loss: 0.002072\n",
      "Validation Loss: 0.000842\n",
      "Epoch 400/499, Training Loss: 0.001930\n",
      "Validation Loss: 0.001270\n",
      "Epoch 420/499, Training Loss: 0.002047\n",
      "Validation Loss: 0.000834\n",
      "Epoch 440/499, Training Loss: 0.001719\n",
      "Validation Loss: 0.000824\n",
      "Epoch 460/499, Training Loss: 0.001568\n",
      "Validation Loss: 0.000787\n",
      "Epoch 480/499, Training Loss: 0.001943\n",
      "Validation Loss: 0.000813\n",
      "Test Loss: 0.000603\n",
      "model LSTM: BRK-B\n",
      "Epoch 0/499, Training Loss: 0.079248\n",
      "Validation Loss: 0.078399\n",
      "Epoch 20/499, Training Loss: 0.005402\n",
      "Validation Loss: 0.004628\n",
      "Epoch 40/499, Training Loss: 0.003480\n",
      "Validation Loss: 0.000976\n",
      "Epoch 60/499, Training Loss: 0.002605\n",
      "Validation Loss: 0.000366\n",
      "Epoch 80/499, Training Loss: 0.002295\n",
      "Validation Loss: 0.000558\n",
      "Epoch 100/499, Training Loss: 0.002085\n",
      "Validation Loss: 0.000323\n",
      "Epoch 120/499, Training Loss: 0.002053\n",
      "Validation Loss: 0.000321\n",
      "Epoch 140/499, Training Loss: 0.001965\n",
      "Validation Loss: 0.000329\n",
      "Epoch 160/499, Training Loss: 0.001745\n",
      "Validation Loss: 0.000346\n",
      "Epoch 180/499, Training Loss: 0.001483\n",
      "Validation Loss: 0.000309\n",
      "Epoch 200/499, Training Loss: 0.001744\n",
      "Validation Loss: 0.000304\n",
      "Epoch 220/499, Training Loss: 0.001551\n",
      "Validation Loss: 0.000361\n",
      "Epoch 240/499, Training Loss: 0.001513\n",
      "Validation Loss: 0.000288\n",
      "Epoch 260/499, Training Loss: 0.001692\n",
      "Validation Loss: 0.000376\n",
      "Epoch 280/499, Training Loss: 0.001883\n",
      "Validation Loss: 0.000278\n",
      "Epoch 300/499, Training Loss: 0.001477\n",
      "Validation Loss: 0.000279\n",
      "Epoch 320/499, Training Loss: 0.001634\n",
      "Validation Loss: 0.000266\n",
      "Epoch 340/499, Training Loss: 0.001599\n",
      "Validation Loss: 0.000532\n",
      "Epoch 360/499, Training Loss: 0.001456\n",
      "Validation Loss: 0.000288\n",
      "Epoch 380/499, Training Loss: 0.001391\n",
      "Validation Loss: 0.000312\n",
      "Epoch 400/499, Training Loss: 0.001343\n",
      "Validation Loss: 0.000259\n",
      "Epoch 420/499, Training Loss: 0.001492\n",
      "Validation Loss: 0.000351\n",
      "Epoch 440/499, Training Loss: 0.001490\n",
      "Validation Loss: 0.000329\n",
      "Epoch 460/499, Training Loss: 0.001608\n",
      "Validation Loss: 0.000231\n",
      "Epoch 480/499, Training Loss: 0.001233\n",
      "Validation Loss: 0.000227\n",
      "Test Loss: 0.000296\n",
      "model LSTM: SBUX\n",
      "Epoch 0/499, Training Loss: 0.258392\n",
      "Validation Loss: 0.129861\n",
      "Epoch 20/499, Training Loss: 0.012545\n",
      "Validation Loss: 0.005301\n",
      "Epoch 40/499, Training Loss: 0.005916\n",
      "Validation Loss: 0.000945\n",
      "Epoch 60/499, Training Loss: 0.004778\n",
      "Validation Loss: 0.000788\n",
      "Epoch 80/499, Training Loss: 0.004459\n",
      "Validation Loss: 0.000752\n",
      "Epoch 100/499, Training Loss: 0.003827\n",
      "Validation Loss: 0.000828\n",
      "Epoch 120/499, Training Loss: 0.004143\n",
      "Validation Loss: 0.000791\n",
      "Epoch 140/499, Training Loss: 0.003367\n",
      "Validation Loss: 0.000652\n",
      "Epoch 160/499, Training Loss: 0.002828\n",
      "Validation Loss: 0.000627\n",
      "Epoch 180/499, Training Loss: 0.002890\n",
      "Validation Loss: 0.000975\n",
      "Epoch 200/499, Training Loss: 0.003514\n",
      "Validation Loss: 0.000622\n",
      "Epoch 220/499, Training Loss: 0.002214\n",
      "Validation Loss: 0.000549\n",
      "Epoch 240/499, Training Loss: 0.002463\n",
      "Validation Loss: 0.000562\n",
      "Epoch 260/499, Training Loss: 0.002137\n",
      "Validation Loss: 0.000583\n",
      "Epoch 280/499, Training Loss: 0.002220\n",
      "Validation Loss: 0.000593\n",
      "Epoch 300/499, Training Loss: 0.001898\n",
      "Validation Loss: 0.000453\n",
      "Epoch 320/499, Training Loss: 0.001671\n",
      "Validation Loss: 0.000695\n",
      "Epoch 340/499, Training Loss: 0.002303\n",
      "Validation Loss: 0.000564\n",
      "Epoch 360/499, Training Loss: 0.001823\n",
      "Validation Loss: 0.000808\n",
      "Epoch 380/499, Training Loss: 0.001784\n",
      "Validation Loss: 0.000465\n",
      "Epoch 400/499, Training Loss: 0.001624\n",
      "Validation Loss: 0.000376\n",
      "Epoch 420/499, Training Loss: 0.001548\n",
      "Validation Loss: 0.000367\n",
      "Epoch 440/499, Training Loss: 0.001651\n",
      "Validation Loss: 0.000436\n",
      "Epoch 460/499, Training Loss: 0.001650\n",
      "Validation Loss: 0.000435\n",
      "Epoch 480/499, Training Loss: 0.001468\n",
      "Validation Loss: 0.000423\n",
      "Test Loss: 0.000459\n",
      "model LSTM: DJP\n",
      "Epoch 0/499, Training Loss: 0.230221\n",
      "Validation Loss: 0.102289\n",
      "Epoch 20/499, Training Loss: 0.013986\n",
      "Validation Loss: 0.008112\n",
      "Epoch 40/499, Training Loss: 0.006216\n",
      "Validation Loss: 0.002238\n",
      "Epoch 60/499, Training Loss: 0.003775\n",
      "Validation Loss: 0.000607\n",
      "Epoch 80/499, Training Loss: 0.003815\n",
      "Validation Loss: 0.000500\n",
      "Epoch 100/499, Training Loss: 0.003169\n",
      "Validation Loss: 0.000495\n",
      "Epoch 120/499, Training Loss: 0.002675\n",
      "Validation Loss: 0.000476\n",
      "Epoch 140/499, Training Loss: 0.002540\n",
      "Validation Loss: 0.000513\n",
      "Epoch 160/499, Training Loss: 0.002401\n",
      "Validation Loss: 0.000545\n",
      "Epoch 180/499, Training Loss: 0.002223\n",
      "Validation Loss: 0.000465\n",
      "Epoch 200/499, Training Loss: 0.002198\n",
      "Validation Loss: 0.000508\n",
      "Epoch 220/499, Training Loss: 0.001733\n",
      "Validation Loss: 0.000463\n",
      "Epoch 240/499, Training Loss: 0.001934\n",
      "Validation Loss: 0.000463\n",
      "Epoch 260/499, Training Loss: 0.001658\n",
      "Validation Loss: 0.000360\n",
      "Epoch 280/499, Training Loss: 0.001569\n",
      "Validation Loss: 0.000380\n",
      "Epoch 300/499, Training Loss: 0.001654\n",
      "Validation Loss: 0.000343\n",
      "Epoch 320/499, Training Loss: 0.001480\n",
      "Validation Loss: 0.000344\n",
      "Epoch 340/499, Training Loss: 0.001398\n",
      "Validation Loss: 0.000427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/499, Training Loss: 0.001313\n",
      "Validation Loss: 0.000395\n",
      "Epoch 380/499, Training Loss: 0.001316\n",
      "Validation Loss: 0.000397\n",
      "Epoch 400/499, Training Loss: 0.001414\n",
      "Validation Loss: 0.000298\n",
      "Epoch 420/499, Training Loss: 0.001343\n",
      "Validation Loss: 0.000520\n",
      "Epoch 440/499, Training Loss: 0.001266\n",
      "Validation Loss: 0.000429\n",
      "Epoch 460/499, Training Loss: 0.001190\n",
      "Validation Loss: 0.000287\n",
      "Epoch 480/499, Training Loss: 0.001068\n",
      "Validation Loss: 0.000263\n",
      "Test Loss: 0.000302\n",
      "model LSTM: XLV\n",
      "Epoch 0/499, Training Loss: 0.183542\n",
      "Validation Loss: 0.092639\n",
      "Epoch 20/499, Training Loss: 0.020208\n",
      "Validation Loss: 0.016817\n",
      "Epoch 40/499, Training Loss: 0.007143\n",
      "Validation Loss: 0.001941\n",
      "Epoch 60/499, Training Loss: 0.004458\n",
      "Validation Loss: 0.000718\n",
      "Epoch 80/499, Training Loss: 0.004462\n",
      "Validation Loss: 0.000667\n",
      "Epoch 100/499, Training Loss: 0.003451\n",
      "Validation Loss: 0.000656\n",
      "Epoch 120/499, Training Loss: 0.003343\n",
      "Validation Loss: 0.000579\n",
      "Epoch 140/499, Training Loss: 0.003211\n",
      "Validation Loss: 0.000835\n",
      "Epoch 160/499, Training Loss: 0.002750\n",
      "Validation Loss: 0.000663\n",
      "Epoch 180/499, Training Loss: 0.002851\n",
      "Validation Loss: 0.000462\n",
      "Epoch 200/499, Training Loss: 0.002819\n",
      "Validation Loss: 0.000582\n",
      "Epoch 220/499, Training Loss: 0.002871\n",
      "Validation Loss: 0.000535\n",
      "Epoch 240/499, Training Loss: 0.002535\n",
      "Validation Loss: 0.000470\n",
      "Epoch 260/499, Training Loss: 0.002597\n",
      "Validation Loss: 0.000657\n",
      "Epoch 280/499, Training Loss: 0.002584\n",
      "Validation Loss: 0.000413\n",
      "Epoch 300/499, Training Loss: 0.002464\n",
      "Validation Loss: 0.000457\n",
      "Epoch 320/499, Training Loss: 0.002279\n",
      "Validation Loss: 0.000350\n",
      "Epoch 340/499, Training Loss: 0.002255\n",
      "Validation Loss: 0.000450\n",
      "Epoch 360/499, Training Loss: 0.002164\n",
      "Validation Loss: 0.000418\n",
      "Epoch 380/499, Training Loss: 0.002136\n",
      "Validation Loss: 0.000360\n",
      "Epoch 400/499, Training Loss: 0.002050\n",
      "Validation Loss: 0.000370\n",
      "Epoch 420/499, Training Loss: 0.002037\n",
      "Validation Loss: 0.000423\n",
      "Epoch 440/499, Training Loss: 0.002065\n",
      "Validation Loss: 0.000344\n",
      "Epoch 460/499, Training Loss: 0.002194\n",
      "Validation Loss: 0.000550\n",
      "Epoch 480/499, Training Loss: 0.001837\n",
      "Validation Loss: 0.000363\n",
      "Test Loss: 0.000364\n",
      "model LSTM: VFSTX\n",
      "Epoch 0/499, Training Loss: 0.422507\n",
      "Validation Loss: 0.215080\n",
      "Epoch 20/499, Training Loss: 0.023988\n",
      "Validation Loss: 0.013552\n",
      "Epoch 40/499, Training Loss: 0.015992\n",
      "Validation Loss: 0.010600\n",
      "Epoch 60/499, Training Loss: 0.008158\n",
      "Validation Loss: 0.002796\n",
      "Epoch 80/499, Training Loss: 0.005703\n",
      "Validation Loss: 0.000792\n",
      "Epoch 100/499, Training Loss: 0.005316\n",
      "Validation Loss: 0.000683\n",
      "Epoch 120/499, Training Loss: 0.004826\n",
      "Validation Loss: 0.000636\n",
      "Epoch 140/499, Training Loss: 0.004505\n",
      "Validation Loss: 0.000587\n",
      "Epoch 160/499, Training Loss: 0.004169\n",
      "Validation Loss: 0.000752\n",
      "Epoch 180/499, Training Loss: 0.003869\n",
      "Validation Loss: 0.000577\n",
      "Epoch 200/499, Training Loss: 0.003405\n",
      "Validation Loss: 0.000577\n",
      "Epoch 220/499, Training Loss: 0.003304\n",
      "Validation Loss: 0.000601\n",
      "Epoch 240/499, Training Loss: 0.003302\n",
      "Validation Loss: 0.000567\n",
      "Epoch 260/499, Training Loss: 0.002920\n",
      "Validation Loss: 0.000780\n",
      "Epoch 280/499, Training Loss: 0.002952\n",
      "Validation Loss: 0.000556\n",
      "Epoch 300/499, Training Loss: 0.002588\n",
      "Validation Loss: 0.000522\n",
      "Epoch 320/499, Training Loss: 0.002503\n",
      "Validation Loss: 0.000743\n",
      "Epoch 340/499, Training Loss: 0.002345\n",
      "Validation Loss: 0.000614\n",
      "Epoch 360/499, Training Loss: 0.002331\n",
      "Validation Loss: 0.000485\n",
      "Epoch 380/499, Training Loss: 0.002044\n",
      "Validation Loss: 0.000500\n",
      "Epoch 400/499, Training Loss: 0.002129\n",
      "Validation Loss: 0.000486\n",
      "Epoch 420/499, Training Loss: 0.002006\n",
      "Validation Loss: 0.000480\n",
      "Epoch 440/499, Training Loss: 0.001804\n",
      "Validation Loss: 0.000421\n",
      "Epoch 460/499, Training Loss: 0.001702\n",
      "Validation Loss: 0.000406\n",
      "Epoch 480/499, Training Loss: 0.001792\n",
      "Validation Loss: 0.000377\n",
      "Test Loss: 0.000468\n",
      "model LSTM: XLP\n",
      "Epoch 0/499, Training Loss: 0.268679\n",
      "Validation Loss: 0.142148\n",
      "Epoch 20/499, Training Loss: 0.015638\n",
      "Validation Loss: 0.008662\n",
      "Epoch 40/499, Training Loss: 0.006344\n",
      "Validation Loss: 0.001098\n",
      "Epoch 60/499, Training Loss: 0.005045\n",
      "Validation Loss: 0.000841\n",
      "Epoch 80/499, Training Loss: 0.004848\n",
      "Validation Loss: 0.000859\n",
      "Epoch 100/499, Training Loss: 0.004337\n",
      "Validation Loss: 0.000759\n",
      "Epoch 120/499, Training Loss: 0.003807\n",
      "Validation Loss: 0.000912\n",
      "Epoch 140/499, Training Loss: 0.003636\n",
      "Validation Loss: 0.000756\n",
      "Epoch 160/499, Training Loss: 0.003393\n",
      "Validation Loss: 0.000666\n",
      "Epoch 180/499, Training Loss: 0.003400\n",
      "Validation Loss: 0.000706\n",
      "Epoch 200/499, Training Loss: 0.003062\n",
      "Validation Loss: 0.000653\n",
      "Epoch 220/499, Training Loss: 0.002940\n",
      "Validation Loss: 0.000788\n",
      "Epoch 240/499, Training Loss: 0.002580\n",
      "Validation Loss: 0.000562\n",
      "Epoch 260/499, Training Loss: 0.002804\n",
      "Validation Loss: 0.000605\n",
      "Epoch 280/499, Training Loss: 0.002474\n",
      "Validation Loss: 0.000525\n",
      "Epoch 300/499, Training Loss: 0.002702\n",
      "Validation Loss: 0.000682\n",
      "Epoch 320/499, Training Loss: 0.002403\n",
      "Validation Loss: 0.000503\n",
      "Epoch 340/499, Training Loss: 0.002068\n",
      "Validation Loss: 0.000600\n",
      "Epoch 360/499, Training Loss: 0.002077\n",
      "Validation Loss: 0.000644\n",
      "Epoch 380/499, Training Loss: 0.001991\n",
      "Validation Loss: 0.000583\n",
      "Epoch 400/499, Training Loss: 0.001909\n",
      "Validation Loss: 0.000445\n",
      "Epoch 420/499, Training Loss: 0.001868\n",
      "Validation Loss: 0.000741\n",
      "Epoch 440/499, Training Loss: 0.002065\n",
      "Validation Loss: 0.000454\n",
      "Epoch 460/499, Training Loss: 0.002023\n",
      "Validation Loss: 0.000485\n",
      "Epoch 480/499, Training Loss: 0.001753\n",
      "Validation Loss: 0.000416\n",
      "Test Loss: 0.000516\n",
      "model LSTM: XLB\n",
      "Epoch 0/499, Training Loss: 0.469458\n",
      "Validation Loss: 0.262100\n",
      "Epoch 20/499, Training Loss: 0.084791\n",
      "Validation Loss: 0.063876\n",
      "Epoch 40/499, Training Loss: 0.047128\n",
      "Validation Loss: 0.033862\n",
      "Epoch 60/499, Training Loss: 0.048967\n",
      "Validation Loss: 0.041998\n",
      "Epoch 80/499, Training Loss: 0.048322\n",
      "Validation Loss: 0.043373\n",
      "Epoch 100/499, Training Loss: 0.045198\n",
      "Validation Loss: 0.040435\n",
      "Epoch 120/499, Training Loss: 0.050485\n",
      "Validation Loss: 0.048995\n",
      "Epoch 140/499, Training Loss: 0.059423\n",
      "Validation Loss: 0.054772\n",
      "Epoch 160/499, Training Loss: 0.058197\n",
      "Validation Loss: 0.055128\n",
      "Epoch 180/499, Training Loss: 0.058104\n",
      "Validation Loss: 0.055328\n",
      "Epoch 200/499, Training Loss: 0.058499\n",
      "Validation Loss: 0.054355\n",
      "Epoch 220/499, Training Loss: 0.058499\n",
      "Validation Loss: 0.054404\n",
      "Epoch 240/499, Training Loss: 0.058279\n",
      "Validation Loss: 0.054453\n",
      "Epoch 260/499, Training Loss: 0.058306\n",
      "Validation Loss: 0.054149\n",
      "Epoch 280/499, Training Loss: 0.056388\n",
      "Validation Loss: 0.053156\n",
      "Epoch 300/499, Training Loss: 0.057652\n",
      "Validation Loss: 0.054296\n",
      "Epoch 320/499, Training Loss: 0.056810\n",
      "Validation Loss: 0.054312\n",
      "Epoch 340/499, Training Loss: 0.056373\n",
      "Validation Loss: 0.054253\n",
      "Epoch 360/499, Training Loss: 0.055572\n",
      "Validation Loss: 0.052273\n",
      "Epoch 380/499, Training Loss: 0.012215\n",
      "Validation Loss: 0.007988\n",
      "Epoch 400/499, Training Loss: 0.006751\n",
      "Validation Loss: 0.003150\n",
      "Epoch 420/499, Training Loss: 0.004913\n",
      "Validation Loss: 0.001455\n",
      "Epoch 440/499, Training Loss: 0.004007\n",
      "Validation Loss: 0.001133\n",
      "Epoch 460/499, Training Loss: 0.003680\n",
      "Validation Loss: 0.000881\n",
      "Epoch 480/499, Training Loss: 0.003480\n",
      "Validation Loss: 0.000871\n",
      "Test Loss: 0.000921\n",
      "model LSTM: XLE\n",
      "Epoch 0/499, Training Loss: 0.323242\n",
      "Validation Loss: 0.143582\n",
      "Epoch 20/499, Training Loss: 0.022659\n",
      "Validation Loss: 0.015651\n",
      "Epoch 40/499, Training Loss: 0.009055\n",
      "Validation Loss: 0.002794\n",
      "Epoch 60/499, Training Loss: 0.006543\n",
      "Validation Loss: 0.001354\n",
      "Epoch 80/499, Training Loss: 0.006044\n",
      "Validation Loss: 0.001014\n",
      "Epoch 100/499, Training Loss: 0.005287\n",
      "Validation Loss: 0.001140\n",
      "Epoch 120/499, Training Loss: 0.004718\n",
      "Validation Loss: 0.000843\n",
      "Epoch 140/499, Training Loss: 0.004583\n",
      "Validation Loss: 0.000795\n",
      "Epoch 160/499, Training Loss: 0.004315\n",
      "Validation Loss: 0.000753\n",
      "Epoch 180/499, Training Loss: 0.003582\n",
      "Validation Loss: 0.000724\n",
      "Epoch 200/499, Training Loss: 0.003308\n",
      "Validation Loss: 0.000762\n",
      "Epoch 220/499, Training Loss: 0.003100\n",
      "Validation Loss: 0.000727\n",
      "Epoch 240/499, Training Loss: 0.002693\n",
      "Validation Loss: 0.000809\n",
      "Epoch 260/499, Training Loss: 0.002428\n",
      "Validation Loss: 0.000758\n",
      "Epoch 280/499, Training Loss: 0.002282\n",
      "Validation Loss: 0.000614\n",
      "Epoch 300/499, Training Loss: 0.002379\n",
      "Validation Loss: 0.000509\n",
      "Epoch 320/499, Training Loss: 0.001909\n",
      "Validation Loss: 0.000489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/499, Training Loss: 0.001751\n",
      "Validation Loss: 0.000468\n",
      "Epoch 360/499, Training Loss: 0.001795\n",
      "Validation Loss: 0.000448\n",
      "Epoch 380/499, Training Loss: 0.001782\n",
      "Validation Loss: 0.000405\n",
      "Epoch 400/499, Training Loss: 0.001531\n",
      "Validation Loss: 0.000397\n",
      "Epoch 420/499, Training Loss: 0.001607\n",
      "Validation Loss: 0.000496\n",
      "Epoch 440/499, Training Loss: 0.001371\n",
      "Validation Loss: 0.000425\n",
      "Epoch 460/499, Training Loss: 0.001428\n",
      "Validation Loss: 0.000435\n",
      "Epoch 480/499, Training Loss: 0.001421\n",
      "Validation Loss: 0.000339\n",
      "Test Loss: 0.000604\n",
      "model LSTM: INDA\n",
      "Epoch 0/499, Training Loss: 0.219872\n",
      "Validation Loss: 0.103722\n",
      "Epoch 20/499, Training Loss: 0.017360\n",
      "Validation Loss: 0.011642\n",
      "Epoch 40/499, Training Loss: 0.007349\n",
      "Validation Loss: 0.002624\n",
      "Epoch 60/499, Training Loss: 0.004624\n",
      "Validation Loss: 0.000921\n",
      "Epoch 80/499, Training Loss: 0.003729\n",
      "Validation Loss: 0.000533\n",
      "Epoch 100/499, Training Loss: 0.003766\n",
      "Validation Loss: 0.000575\n",
      "Epoch 120/499, Training Loss: 0.002728\n",
      "Validation Loss: 0.000498\n",
      "Epoch 140/499, Training Loss: 0.002873\n",
      "Validation Loss: 0.000481\n",
      "Epoch 160/499, Training Loss: 0.002414\n",
      "Validation Loss: 0.000468\n",
      "Epoch 180/499, Training Loss: 0.002169\n",
      "Validation Loss: 0.000461\n",
      "Epoch 200/499, Training Loss: 0.001958\n",
      "Validation Loss: 0.000423\n",
      "Epoch 220/499, Training Loss: 0.001953\n",
      "Validation Loss: 0.000389\n",
      "Epoch 240/499, Training Loss: 0.001779\n",
      "Validation Loss: 0.000372\n",
      "Epoch 260/499, Training Loss: 0.001657\n",
      "Validation Loss: 0.000342\n",
      "Epoch 280/499, Training Loss: 0.001606\n",
      "Validation Loss: 0.000468\n",
      "Epoch 300/499, Training Loss: 0.001744\n",
      "Validation Loss: 0.000318\n",
      "Epoch 320/499, Training Loss: 0.001509\n",
      "Validation Loss: 0.000690\n",
      "Epoch 340/499, Training Loss: 0.001481\n",
      "Validation Loss: 0.000353\n",
      "Epoch 360/499, Training Loss: 0.001401\n",
      "Validation Loss: 0.000293\n",
      "Epoch 380/499, Training Loss: 0.001306\n",
      "Validation Loss: 0.000305\n",
      "Epoch 400/499, Training Loss: 0.001277\n",
      "Validation Loss: 0.000265\n",
      "Epoch 420/499, Training Loss: 0.001345\n",
      "Validation Loss: 0.000308\n",
      "Epoch 440/499, Training Loss: 0.001252\n",
      "Validation Loss: 0.000290\n",
      "Epoch 460/499, Training Loss: 0.001177\n",
      "Validation Loss: 0.000289\n",
      "Epoch 480/499, Training Loss: 0.001381\n",
      "Validation Loss: 0.000358\n",
      "Test Loss: 0.000388\n",
      "model LSTM: VCIT\n",
      "Epoch 0/499, Training Loss: 0.315276\n",
      "Validation Loss: 0.168937\n",
      "Epoch 20/499, Training Loss: 0.047049\n",
      "Validation Loss: 0.046028\n",
      "Epoch 40/499, Training Loss: 0.007835\n",
      "Validation Loss: 0.003909\n",
      "Epoch 60/499, Training Loss: 0.005570\n",
      "Validation Loss: 0.001874\n",
      "Epoch 80/499, Training Loss: 0.004437\n",
      "Validation Loss: 0.000968\n",
      "Epoch 100/499, Training Loss: 0.003870\n",
      "Validation Loss: 0.000938\n",
      "Epoch 120/499, Training Loss: 0.003543\n",
      "Validation Loss: 0.000943\n",
      "Epoch 140/499, Training Loss: 0.003298\n",
      "Validation Loss: 0.000891\n",
      "Epoch 160/499, Training Loss: 0.003317\n",
      "Validation Loss: 0.000870\n",
      "Epoch 180/499, Training Loss: 0.003059\n",
      "Validation Loss: 0.000901\n",
      "Epoch 200/499, Training Loss: 0.002832\n",
      "Validation Loss: 0.001050\n",
      "Epoch 220/499, Training Loss: 0.002554\n",
      "Validation Loss: 0.000846\n",
      "Epoch 240/499, Training Loss: 0.002475\n",
      "Validation Loss: 0.000984\n",
      "Epoch 260/499, Training Loss: 0.002391\n",
      "Validation Loss: 0.000887\n",
      "Epoch 280/499, Training Loss: 0.002229\n",
      "Validation Loss: 0.001003\n",
      "Epoch 300/499, Training Loss: 0.002172\n",
      "Validation Loss: 0.000815\n",
      "Epoch 320/499, Training Loss: 0.001942\n",
      "Validation Loss: 0.000766\n",
      "Epoch 340/499, Training Loss: 0.001905\n",
      "Validation Loss: 0.000743\n",
      "Epoch 360/499, Training Loss: 0.001888\n",
      "Validation Loss: 0.000712\n",
      "Epoch 380/499, Training Loss: 0.001742\n",
      "Validation Loss: 0.000669\n",
      "Epoch 400/499, Training Loss: 0.001734\n",
      "Validation Loss: 0.000703\n",
      "Epoch 420/499, Training Loss: 0.001831\n",
      "Validation Loss: 0.000603\n",
      "Epoch 440/499, Training Loss: 0.001648\n",
      "Validation Loss: 0.000548\n",
      "Epoch 460/499, Training Loss: 0.001592\n",
      "Validation Loss: 0.000608\n",
      "Epoch 480/499, Training Loss: 0.001741\n",
      "Validation Loss: 0.000632\n",
      "Test Loss: 0.000489\n",
      "model LSTM: BND\n",
      "Epoch 0/499, Training Loss: 0.434894\n",
      "Validation Loss: 0.230983\n",
      "Epoch 20/499, Training Loss: 0.067457\n",
      "Validation Loss: 0.063616\n",
      "Epoch 40/499, Training Loss: 0.026198\n",
      "Validation Loss: 0.016019\n",
      "Epoch 60/499, Training Loss: 0.018478\n",
      "Validation Loss: 0.010330\n",
      "Epoch 80/499, Training Loss: 0.010798\n",
      "Validation Loss: 0.004193\n",
      "Epoch 100/499, Training Loss: 0.008530\n",
      "Validation Loss: 0.002126\n",
      "Epoch 120/499, Training Loss: 0.007396\n",
      "Validation Loss: 0.002480\n",
      "Epoch 140/499, Training Loss: 0.006309\n",
      "Validation Loss: 0.001657\n",
      "Epoch 160/499, Training Loss: 0.005399\n",
      "Validation Loss: 0.001235\n",
      "Epoch 180/499, Training Loss: 0.005209\n",
      "Validation Loss: 0.001258\n",
      "Epoch 200/499, Training Loss: 0.004224\n",
      "Validation Loss: 0.000860\n",
      "Epoch 220/499, Training Loss: 0.004561\n",
      "Validation Loss: 0.000928\n",
      "Epoch 240/499, Training Loss: 0.004126\n",
      "Validation Loss: 0.000931\n",
      "Epoch 260/499, Training Loss: 0.003821\n",
      "Validation Loss: 0.000709\n",
      "Epoch 280/499, Training Loss: 0.003296\n",
      "Validation Loss: 0.000701\n",
      "Epoch 300/499, Training Loss: 0.002899\n",
      "Validation Loss: 0.000718\n",
      "Epoch 320/499, Training Loss: 0.002814\n",
      "Validation Loss: 0.000733\n",
      "Epoch 340/499, Training Loss: 0.002604\n",
      "Validation Loss: 0.000632\n",
      "Epoch 360/499, Training Loss: 0.002759\n",
      "Validation Loss: 0.000635\n",
      "Epoch 380/499, Training Loss: 0.002352\n",
      "Validation Loss: 0.000638\n",
      "Epoch 400/499, Training Loss: 0.002212\n",
      "Validation Loss: 0.000622\n",
      "Epoch 420/499, Training Loss: 0.002302\n",
      "Validation Loss: 0.000636\n",
      "Epoch 440/499, Training Loss: 0.002308\n",
      "Validation Loss: 0.000541\n",
      "Epoch 460/499, Training Loss: 0.002200\n",
      "Validation Loss: 0.000819\n",
      "Epoch 480/499, Training Loss: 0.002242\n",
      "Validation Loss: 0.000953\n",
      "Test Loss: 0.000811\n",
      "model LSTM: XLF\n",
      "Epoch 0/499, Training Loss: 0.226311\n",
      "Validation Loss: 0.105101\n",
      "Epoch 20/499, Training Loss: 0.009611\n",
      "Validation Loss: 0.005146\n",
      "Epoch 40/499, Training Loss: 0.006136\n",
      "Validation Loss: 0.001048\n",
      "Epoch 60/499, Training Loss: 0.003980\n",
      "Validation Loss: 0.000813\n",
      "Epoch 80/499, Training Loss: 0.003333\n",
      "Validation Loss: 0.000749\n",
      "Epoch 100/499, Training Loss: 0.003167\n",
      "Validation Loss: 0.000751\n",
      "Epoch 120/499, Training Loss: 0.003033\n",
      "Validation Loss: 0.000708\n",
      "Epoch 140/499, Training Loss: 0.002729\n",
      "Validation Loss: 0.000701\n",
      "Epoch 160/499, Training Loss: 0.002574\n",
      "Validation Loss: 0.001484\n",
      "Epoch 180/499, Training Loss: 0.002519\n",
      "Validation Loss: 0.000610\n",
      "Epoch 200/499, Training Loss: 0.002170\n",
      "Validation Loss: 0.000752\n",
      "Epoch 220/499, Training Loss: 0.002645\n",
      "Validation Loss: 0.000616\n",
      "Epoch 240/499, Training Loss: 0.002258\n",
      "Validation Loss: 0.000533\n",
      "Epoch 260/499, Training Loss: 0.001960\n",
      "Validation Loss: 0.000525\n",
      "Epoch 280/499, Training Loss: 0.001819\n",
      "Validation Loss: 0.000502\n",
      "Epoch 300/499, Training Loss: 0.001901\n",
      "Validation Loss: 0.000836\n",
      "Epoch 320/499, Training Loss: 0.001939\n",
      "Validation Loss: 0.000462\n",
      "Epoch 340/499, Training Loss: 0.001879\n",
      "Validation Loss: 0.000498\n",
      "Epoch 360/499, Training Loss: 0.001852\n",
      "Validation Loss: 0.000468\n",
      "Epoch 380/499, Training Loss: 0.001680\n",
      "Validation Loss: 0.000417\n",
      "Epoch 400/499, Training Loss: 0.001740\n",
      "Validation Loss: 0.000445\n",
      "Epoch 420/499, Training Loss: 0.001614\n",
      "Validation Loss: 0.000464\n",
      "Epoch 440/499, Training Loss: 0.001617\n",
      "Validation Loss: 0.000405\n",
      "Epoch 460/499, Training Loss: 0.002806\n",
      "Validation Loss: 0.001100\n",
      "Epoch 480/499, Training Loss: 0.001688\n",
      "Validation Loss: 0.000474\n",
      "Test Loss: 0.000383\n",
      "model LSTM: IXP\n",
      "Epoch 0/499, Training Loss: 0.373510\n",
      "Validation Loss: 0.192147\n",
      "Epoch 20/499, Training Loss: 0.046736\n",
      "Validation Loss: 0.032581\n",
      "Epoch 40/499, Training Loss: 0.012218\n",
      "Validation Loss: 0.006418\n",
      "Epoch 60/499, Training Loss: 0.007808\n",
      "Validation Loss: 0.001950\n",
      "Epoch 80/499, Training Loss: 0.005985\n",
      "Validation Loss: 0.001367\n",
      "Epoch 100/499, Training Loss: 0.005751\n",
      "Validation Loss: 0.000994\n",
      "Epoch 120/499, Training Loss: 0.004934\n",
      "Validation Loss: 0.000944\n",
      "Epoch 140/499, Training Loss: 0.004458\n",
      "Validation Loss: 0.001014\n",
      "Epoch 160/499, Training Loss: 0.003814\n",
      "Validation Loss: 0.000810\n",
      "Epoch 180/499, Training Loss: 0.003624\n",
      "Validation Loss: 0.000744\n",
      "Epoch 200/499, Training Loss: 0.003654\n",
      "Validation Loss: 0.000846\n",
      "Epoch 220/499, Training Loss: 0.003498\n",
      "Validation Loss: 0.000738\n",
      "Epoch 240/499, Training Loss: 0.003271\n",
      "Validation Loss: 0.000730\n",
      "Epoch 260/499, Training Loss: 0.003416\n",
      "Validation Loss: 0.000781\n",
      "Epoch 280/499, Training Loss: 0.003063\n",
      "Validation Loss: 0.000599\n",
      "Epoch 300/499, Training Loss: 0.002818\n",
      "Validation Loss: 0.000569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320/499, Training Loss: 0.002589\n",
      "Validation Loss: 0.000659\n",
      "Epoch 340/499, Training Loss: 0.002649\n",
      "Validation Loss: 0.000658\n",
      "Epoch 360/499, Training Loss: 0.002338\n",
      "Validation Loss: 0.000597\n",
      "Epoch 380/499, Training Loss: 0.002486\n",
      "Validation Loss: 0.000805\n",
      "Epoch 400/499, Training Loss: 0.002362\n",
      "Validation Loss: 0.000567\n",
      "Epoch 420/499, Training Loss: 0.002218\n",
      "Validation Loss: 0.000489\n",
      "Epoch 440/499, Training Loss: 0.001944\n",
      "Validation Loss: 0.000538\n",
      "Epoch 460/499, Training Loss: 0.001812\n",
      "Validation Loss: 0.000471\n",
      "Epoch 480/499, Training Loss: 0.001745\n",
      "Validation Loss: 0.000541\n",
      "Test Loss: 0.000485\n",
      "model LSTM: XLI\n",
      "Epoch 0/499, Training Loss: 0.163624\n",
      "Validation Loss: 0.081267\n",
      "Epoch 20/499, Training Loss: 0.015332\n",
      "Validation Loss: 0.013159\n",
      "Epoch 40/499, Training Loss: 0.004906\n",
      "Validation Loss: 0.001018\n",
      "Epoch 60/499, Training Loss: 0.003792\n",
      "Validation Loss: 0.000822\n",
      "Epoch 80/499, Training Loss: 0.003223\n",
      "Validation Loss: 0.000688\n",
      "Epoch 100/499, Training Loss: 0.003079\n",
      "Validation Loss: 0.000620\n",
      "Epoch 120/499, Training Loss: 0.002788\n",
      "Validation Loss: 0.000621\n",
      "Epoch 140/499, Training Loss: 0.002599\n",
      "Validation Loss: 0.000757\n",
      "Epoch 160/499, Training Loss: 0.002509\n",
      "Validation Loss: 0.000524\n",
      "Epoch 180/499, Training Loss: 0.002116\n",
      "Validation Loss: 0.000597\n",
      "Epoch 200/499, Training Loss: 0.002149\n",
      "Validation Loss: 0.000499\n",
      "Epoch 220/499, Training Loss: 0.002048\n",
      "Validation Loss: 0.000837\n",
      "Epoch 240/499, Training Loss: 0.001918\n",
      "Validation Loss: 0.000488\n",
      "Epoch 260/499, Training Loss: 0.001979\n",
      "Validation Loss: 0.000556\n",
      "Epoch 280/499, Training Loss: 0.001858\n",
      "Validation Loss: 0.000459\n",
      "Epoch 300/499, Training Loss: 0.001758\n",
      "Validation Loss: 0.000671\n",
      "Epoch 320/499, Training Loss: 0.001811\n",
      "Validation Loss: 0.000571\n",
      "Epoch 340/499, Training Loss: 0.001706\n",
      "Validation Loss: 0.000487\n",
      "Epoch 360/499, Training Loss: 0.001679\n",
      "Validation Loss: 0.000407\n",
      "Epoch 380/499, Training Loss: 0.001633\n",
      "Validation Loss: 0.000538\n",
      "Epoch 400/499, Training Loss: 0.001656\n",
      "Validation Loss: 0.000412\n",
      "Epoch 420/499, Training Loss: 0.001530\n",
      "Validation Loss: 0.000485\n",
      "Epoch 440/499, Training Loss: 0.001441\n",
      "Validation Loss: 0.000494\n",
      "Epoch 460/499, Training Loss: 0.001514\n",
      "Validation Loss: 0.000469\n",
      "Epoch 480/499, Training Loss: 0.001621\n",
      "Validation Loss: 0.000407\n",
      "Test Loss: 0.000412\n",
      "model LSTM: EFA\n",
      "Epoch 0/499, Training Loss: 0.413707\n",
      "Validation Loss: 0.222183\n",
      "Epoch 20/499, Training Loss: 0.056756\n",
      "Validation Loss: 0.038223\n",
      "Epoch 40/499, Training Loss: 0.048475\n",
      "Validation Loss: 0.037676\n",
      "Epoch 60/499, Training Loss: 0.016023\n",
      "Validation Loss: 0.006103\n",
      "Epoch 80/499, Training Loss: 0.009656\n",
      "Validation Loss: 0.001779\n",
      "Epoch 100/499, Training Loss: 0.007474\n",
      "Validation Loss: 0.001490\n",
      "Epoch 120/499, Training Loss: 0.006522\n",
      "Validation Loss: 0.001997\n",
      "Epoch 140/499, Training Loss: 0.005193\n",
      "Validation Loss: 0.001071\n",
      "Epoch 160/499, Training Loss: 0.004918\n",
      "Validation Loss: 0.001181\n",
      "Epoch 180/499, Training Loss: 0.004665\n",
      "Validation Loss: 0.001015\n",
      "Epoch 200/499, Training Loss: 0.004303\n",
      "Validation Loss: 0.000885\n",
      "Epoch 220/499, Training Loss: 0.003947\n",
      "Validation Loss: 0.001118\n",
      "Epoch 240/499, Training Loss: 0.003712\n",
      "Validation Loss: 0.000924\n",
      "Epoch 260/499, Training Loss: 0.003820\n",
      "Validation Loss: 0.000730\n",
      "Epoch 280/499, Training Loss: 0.003313\n",
      "Validation Loss: 0.000706\n",
      "Epoch 300/499, Training Loss: 0.003543\n",
      "Validation Loss: 0.000700\n",
      "Epoch 320/499, Training Loss: 0.002889\n",
      "Validation Loss: 0.000728\n",
      "Epoch 340/499, Training Loss: 0.002729\n",
      "Validation Loss: 0.000766\n",
      "Epoch 360/499, Training Loss: 0.002734\n",
      "Validation Loss: 0.000681\n",
      "Epoch 380/499, Training Loss: 0.002512\n",
      "Validation Loss: 0.000580\n",
      "Epoch 400/499, Training Loss: 0.002319\n",
      "Validation Loss: 0.000595\n",
      "Epoch 420/499, Training Loss: 0.002187\n",
      "Validation Loss: 0.000546\n",
      "Epoch 440/499, Training Loss: 0.002222\n",
      "Validation Loss: 0.000736\n",
      "Epoch 460/499, Training Loss: 0.002045\n",
      "Validation Loss: 0.000555\n",
      "Epoch 480/499, Training Loss: 0.001948\n",
      "Validation Loss: 0.000558\n",
      "Test Loss: 0.000641\n",
      "model LSTM: XLU\n",
      "Epoch 0/499, Training Loss: 0.404728\n",
      "Validation Loss: 0.206247\n",
      "Epoch 20/499, Training Loss: 0.013011\n",
      "Validation Loss: 0.004271\n",
      "Epoch 40/499, Training Loss: 0.007574\n",
      "Validation Loss: 0.001704\n",
      "Epoch 60/499, Training Loss: 0.006928\n",
      "Validation Loss: 0.001172\n",
      "Epoch 80/499, Training Loss: 0.006094\n",
      "Validation Loss: 0.000985\n",
      "Epoch 100/499, Training Loss: 0.006279\n",
      "Validation Loss: 0.001362\n",
      "Epoch 120/499, Training Loss: 0.005007\n",
      "Validation Loss: 0.000959\n",
      "Epoch 140/499, Training Loss: 0.004888\n",
      "Validation Loss: 0.000890\n",
      "Epoch 160/499, Training Loss: 0.004320\n",
      "Validation Loss: 0.000851\n",
      "Epoch 180/499, Training Loss: 0.004228\n",
      "Validation Loss: 0.001051\n",
      "Epoch 200/499, Training Loss: 0.003870\n",
      "Validation Loss: 0.000809\n",
      "Epoch 220/499, Training Loss: 0.003700\n",
      "Validation Loss: 0.000733\n",
      "Epoch 240/499, Training Loss: 0.003437\n",
      "Validation Loss: 0.000711\n",
      "Epoch 260/499, Training Loss: 0.002846\n",
      "Validation Loss: 0.000666\n",
      "Epoch 280/499, Training Loss: 0.003114\n",
      "Validation Loss: 0.000631\n",
      "Epoch 300/499, Training Loss: 0.002804\n",
      "Validation Loss: 0.000735\n",
      "Epoch 320/499, Training Loss: 0.002640\n",
      "Validation Loss: 0.001099\n",
      "Epoch 340/499, Training Loss: 0.002449\n",
      "Validation Loss: 0.000684\n",
      "Epoch 360/499, Training Loss: 0.002290\n",
      "Validation Loss: 0.000553\n",
      "Epoch 380/499, Training Loss: 0.002266\n",
      "Validation Loss: 0.000606\n",
      "Epoch 400/499, Training Loss: 0.002022\n",
      "Validation Loss: 0.000592\n",
      "Epoch 420/499, Training Loss: 0.002000\n",
      "Validation Loss: 0.000505\n",
      "Epoch 440/499, Training Loss: 0.002029\n",
      "Validation Loss: 0.000467\n",
      "Epoch 460/499, Training Loss: 0.001946\n",
      "Validation Loss: 0.000673\n",
      "Epoch 480/499, Training Loss: 0.001748\n",
      "Validation Loss: 0.000488\n",
      "Test Loss: 0.000553\n",
      "model LSTM: EEM\n",
      "Epoch 0/499, Training Loss: 0.226202\n",
      "Validation Loss: 0.114412\n",
      "Epoch 20/499, Training Loss: 0.040571\n",
      "Validation Loss: 0.034233\n",
      "Epoch 40/499, Training Loss: 0.014413\n",
      "Validation Loss: 0.006999\n",
      "Epoch 60/499, Training Loss: 0.006261\n",
      "Validation Loss: 0.001849\n",
      "Epoch 80/499, Training Loss: 0.004837\n",
      "Validation Loss: 0.001037\n",
      "Epoch 100/499, Training Loss: 0.003715\n",
      "Validation Loss: 0.001035\n",
      "Epoch 120/499, Training Loss: 0.003494\n",
      "Validation Loss: 0.000916\n",
      "Epoch 140/499, Training Loss: 0.003075\n",
      "Validation Loss: 0.000766\n",
      "Epoch 160/499, Training Loss: 0.003033\n",
      "Validation Loss: 0.000875\n",
      "Epoch 180/499, Training Loss: 0.002557\n",
      "Validation Loss: 0.000787\n",
      "Epoch 200/499, Training Loss: 0.002257\n",
      "Validation Loss: 0.000667\n",
      "Epoch 220/499, Training Loss: 0.002741\n",
      "Validation Loss: 0.000629\n",
      "Epoch 240/499, Training Loss: 0.002303\n",
      "Validation Loss: 0.000601\n",
      "Epoch 260/499, Training Loss: 0.002248\n",
      "Validation Loss: 0.000896\n",
      "Epoch 280/499, Training Loss: 0.001866\n",
      "Validation Loss: 0.000820\n",
      "Epoch 300/499, Training Loss: 0.001910\n",
      "Validation Loss: 0.000580\n",
      "Epoch 320/499, Training Loss: 0.001735\n",
      "Validation Loss: 0.000727\n",
      "Epoch 340/499, Training Loss: 0.001755\n",
      "Validation Loss: 0.000634\n",
      "Epoch 360/499, Training Loss: 0.001693\n",
      "Validation Loss: 0.000482\n",
      "Epoch 380/499, Training Loss: 0.001613\n",
      "Validation Loss: 0.000466\n",
      "Epoch 400/499, Training Loss: 0.001717\n",
      "Validation Loss: 0.000519\n",
      "Epoch 420/499, Training Loss: 0.001623\n",
      "Validation Loss: 0.000692\n",
      "Epoch 440/499, Training Loss: 0.001571\n",
      "Validation Loss: 0.000579\n",
      "Epoch 460/499, Training Loss: 0.001431\n",
      "Validation Loss: 0.000459\n",
      "Epoch 480/499, Training Loss: 0.001558\n",
      "Validation Loss: 0.000406\n",
      "Test Loss: 0.000455\n",
      "avg val loss 0.0004560928255496374\n",
      "avg test loss 0.00046118488670024953\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "lstm_models = {}\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "input_size = 6\n",
    "\n",
    "test_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for stock in X_train_tensors:\n",
    "    print(\"model LSTM: \" + stock)\n",
    "    model = LSTMModel(input_dim=input_size, hidden_dim=40)\n",
    "#     model = DMLPModel(input_dim=60 * input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trained_model, val_loss = train_model(model, X_train_tensors[stock], y_train_tensors[stock], X_val_tensors[stock], y_val_tensors[stock], criterion, optimizer, num_epochs, patience=0, device=device, print_every=20)\n",
    "    val_losses.append(val_loss.item())\n",
    "    #Test\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(X_test_tensors[stock].to(device))\n",
    "        loss = criterion(outputs, y_test_tensors[stock].to(device).view(-1,1))\n",
    "\n",
    "        print(f\"Test Loss: {loss:.6f}\")\n",
    "        test_losses.append(loss.item())\n",
    "    lstm_models[stock] = trained_model\n",
    "\n",
    "print(\"avg val loss\", np.mean(np.array(val_losses)))\n",
    "print(\"avg test loss\", np.mean(np.array(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edc603b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model RF: DEO\n",
      "DEO - Train Error: 0.000198, Val Error: 0.000344, Test Error: 0.000357\n",
      "model RF: DLR\n",
      "DLR - Train Error: 0.000224, Val Error: 0.000518, Test Error: 0.000369\n",
      "model RF: ULTA\n",
      "ULTA - Train Error: 0.000155, Val Error: 0.000314, Test Error: 0.000324\n",
      "model RF: V\n",
      "V - Train Error: 0.000086, Val Error: 0.000145, Test Error: 0.000143\n",
      "model RF: DIS\n",
      "DIS - Train Error: 0.000216, Val Error: 0.000414, Test Error: 0.000337\n",
      "model RF: CME\n",
      "CME - Train Error: 0.000153, Val Error: 0.000229, Test Error: 0.000371\n",
      "model RF: AAPL\n",
      "AAPL - Train Error: 0.000084, Val Error: 0.000149, Test Error: 0.000136\n",
      "model RF: GOOGL\n",
      "GOOGL - Train Error: 0.000093, Val Error: 0.000163, Test Error: 0.000147\n",
      "model RF: UNH\n",
      "UNH - Train Error: 0.000083, Val Error: 0.000100, Test Error: 0.000150\n",
      "model RF: GS\n",
      "GS - Train Error: 0.000152, Val Error: 0.000296, Test Error: 0.000269\n",
      "model RF: QCOM\n",
      "QCOM - Train Error: 0.000167, Val Error: 0.000342, Test Error: 0.000338\n",
      "model RF: BA\n",
      "BA - Train Error: 0.000187, Val Error: 0.000397, Test Error: 0.000477\n",
      "model RF: STZ\n",
      "STZ - Train Error: 0.000181, Val Error: 0.000349, Test Error: 0.000371\n",
      "model RF: NTDOY\n",
      "NTDOY - Train Error: 0.000139, Val Error: 0.000273, Test Error: 0.000219\n",
      "model RF: MSFT\n",
      "MSFT - Train Error: 0.000055, Val Error: 0.000096, Test Error: 0.000107\n",
      "model RF: DE\n",
      "DE - Train Error: 0.000116, Val Error: 0.000192, Test Error: 0.000212\n",
      "model RF: BAC\n",
      "BAC - Train Error: 0.000148, Val Error: 0.000299, Test Error: 0.000299\n",
      "model RF: IRM\n",
      "IRM - Train Error: 0.000110, Val Error: 0.000162, Test Error: 0.000188\n",
      "model RF: ADM\n",
      "ADM - Train Error: 0.000169, Val Error: 0.000207, Test Error: 0.000246\n",
      "model RF: GOOG\n",
      "GOOG - Train Error: 0.000089, Val Error: 0.000160, Test Error: 0.000143\n",
      "model RF: META\n",
      "META - Train Error: 0.000116, Val Error: 0.000180, Test Error: 0.000115\n",
      "model RF: PG\n",
      "PG - Train Error: 0.000137, Val Error: 0.000242, Test Error: 0.000276\n",
      "model RF: ARE\n",
      "ARE - Train Error: 0.000165, Val Error: 0.000334, Test Error: 0.000280\n",
      "model RF: LH\n",
      "LH - Train Error: 0.000143, Val Error: 0.000266, Test Error: 0.000294\n",
      "model RF: MRK\n",
      "MRK - Train Error: 0.000107, Val Error: 0.000238, Test Error: 0.000180\n",
      "model RF: LVMUY\n",
      "LVMUY - Train Error: 0.000094, Val Error: 0.000184, Test Error: 0.000184\n",
      "model RF: LMT\n",
      "LMT - Train Error: 0.000170, Val Error: 0.000263, Test Error: 0.000294\n",
      "model RF: MCD\n",
      "MCD - Train Error: 0.000092, Val Error: 0.000157, Test Error: 0.000233\n",
      "model RF: MSBHF\n",
      "MSBHF - Train Error: 0.000077, Val Error: 0.000100, Test Error: 0.000143\n",
      "model RF: JCI\n",
      "JCI - Train Error: 0.000154, Val Error: 0.000289, Test Error: 0.000307\n",
      "model RF: MDLZ\n",
      "MDLZ - Train Error: 0.000164, Val Error: 0.000209, Test Error: 0.000323\n",
      "model RF: ORCL\n",
      "ORCL - Train Error: 0.000125, Val Error: 0.000205, Test Error: 0.000235\n",
      "model RF: HCA\n",
      "HCA - Train Error: 0.000112, Val Error: 0.000146, Test Error: 0.000213\n",
      "model RF: NVDA\n",
      "NVDA - Train Error: 0.000046, Val Error: 0.000104, Test Error: 0.000147\n",
      "model RF: WFC\n",
      "WFC - Train Error: 0.000248, Val Error: 0.000431, Test Error: 0.000465\n",
      "model RF: PEP\n",
      "PEP - Train Error: 0.000143, Val Error: 0.000199, Test Error: 0.000260\n",
      "model RF: IPGP\n",
      "IPGP - Train Error: 0.000242, Val Error: 0.000736, Test Error: 0.000486\n",
      "model RF: BRK-B\n",
      "BRK-B - Train Error: 0.000066, Val Error: 0.000105, Test Error: 0.000126\n",
      "model RF: SBUX\n",
      "SBUX - Train Error: 0.000149, Val Error: 0.000260, Test Error: 0.000358\n",
      "model RF: DJP\n",
      "DJP - Train Error: 0.000116, Val Error: 0.000186, Test Error: 0.000222\n",
      "model RF: XLV\n",
      "XLV - Train Error: 0.000108, Val Error: 0.000133, Test Error: 0.000177\n",
      "model RF: VFSTX\n",
      "VFSTX - Train Error: 0.000121, Val Error: 0.000224, Test Error: 0.000217\n",
      "model RF: XLP\n",
      "XLP - Train Error: 0.000171, Val Error: 0.000239, Test Error: 0.000373\n",
      "model RF: XLB\n",
      "XLB - Train Error: 0.000166, Val Error: 0.000267, Test Error: 0.000326\n",
      "model RF: XLE\n",
      "XLE - Train Error: 0.000166, Val Error: 0.000295, Test Error: 0.000350\n",
      "model RF: INDA\n",
      "INDA - Train Error: 0.000132, Val Error: 0.000176, Test Error: 0.000238\n",
      "model RF: VCIT\n",
      "VCIT - Train Error: 0.000184, Val Error: 0.000340, Test Error: 0.000449\n",
      "model RF: BND\n",
      "BND - Train Error: 0.000107, Val Error: 0.000252, Test Error: 0.000378\n",
      "model RF: XLF\n",
      "XLF - Train Error: 0.000151, Val Error: 0.000290, Test Error: 0.000312\n",
      "model RF: IXP\n",
      "IXP - Train Error: 0.000225, Val Error: 0.000348, Test Error: 0.000356\n",
      "model RF: XLI\n",
      "XLI - Train Error: 0.000112, Val Error: 0.000188, Test Error: 0.000234\n",
      "model RF: EFA\n",
      "EFA - Train Error: 0.000325, Val Error: 0.000452, Test Error: 0.000593\n",
      "model RF: XLU\n",
      "XLU - Train Error: 0.000273, Val Error: 0.000385, Test Error: 0.000487\n",
      "model RF: EEM\n",
      "EEM - Train Error: 0.000228, Val Error: 0.000395, Test Error: 0.000401\n",
      "avg val loss 0.00025864156618776366\n",
      "avg test loss 0.0002821050732916975\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "rf_models = {}\n",
    "test_losses = []\n",
    "val_losses = []\n",
    "input_size = 6\n",
    "\n",
    "for stock in stock_60d:\n",
    "    print(\"model RF: \" + stock)\n",
    "    model_rf = RandomForestModel()\n",
    "    model_rf.fit(X_train[stock].reshape(-1,60 * input_size), y_train[stock])\n",
    "\n",
    "    y_train_pred_rf = model_rf.predict(X_train[stock].reshape(-1,60 * input_size))\n",
    "    y_val_pred_rf = model_rf.predict(X_val[stock].reshape(-1,60 * input_size))\n",
    "    y_test_pred_rf = model_rf.predict(X_test[stock].reshape(-1,60 * input_size))\n",
    "\n",
    "    train_error_rf = mean_squared_error(y_train[stock], y_train_pred_rf)\n",
    "    val_error_rf = mean_squared_error(y_val[stock], y_val_pred_rf)\n",
    "    test_error_rf = mean_squared_error(y_test[stock], y_test_pred_rf)\n",
    "    \n",
    "    rf_models[stock] = model_rf\n",
    "    \n",
    "    val_losses.append(val_error_rf)\n",
    "    test_losses.append(test_error_rf)\n",
    "        \n",
    "    print(f\"{stock} - Train Error: {train_error_rf:.6f}, Val Error: {val_error_rf:.6f}, Test Error: {test_error_rf:.6f}\")\n",
    "\n",
    "print(\"avg val loss\", np.mean(np.array(val_losses)))\n",
    "print(\"avg test loss\", np.mean(np.array(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea02aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model RF: DEO\n",
      "DEO - Train Error: 0.002056, Val Error: 0.001983, Test Error: 0.002124\n",
      "model RF: DLR\n",
      "DLR - Train Error: 0.002124, Val Error: 0.002192, Test Error: 0.002033\n",
      "model RF: ULTA\n",
      "ULTA - Train Error: 0.002418, Val Error: 0.002274, Test Error: 0.002499\n",
      "model RF: V\n",
      "V - Train Error: 0.003521, Val Error: 0.003650, Test Error: 0.003401\n",
      "model RF: DIS\n",
      "DIS - Train Error: 0.002010, Val Error: 0.002147, Test Error: 0.002026\n",
      "model RF: CME\n",
      "CME - Train Error: 0.002437, Val Error: 0.002121, Test Error: 0.002284\n",
      "model RF: AAPL\n",
      "AAPL - Train Error: 0.003174, Val Error: 0.002930, Test Error: 0.002985\n",
      "model RF: GOOGL\n",
      "GOOGL - Train Error: 0.002535, Val Error: 0.002636, Test Error: 0.002404\n",
      "model RF: UNH\n",
      "UNH - Train Error: 0.003128, Val Error: 0.003182, Test Error: 0.003123\n",
      "model RF: GS\n",
      "GS - Train Error: 0.001692, Val Error: 0.001735, Test Error: 0.001752\n",
      "model RF: QCOM\n",
      "QCOM - Train Error: 0.001606, Val Error: 0.001698, Test Error: 0.001651\n",
      "model RF: BA\n",
      "BA - Train Error: 0.002332, Val Error: 0.002253, Test Error: 0.002186\n",
      "model RF: STZ\n",
      "STZ - Train Error: 0.002156, Val Error: 0.002093, Test Error: 0.002116\n",
      "model RF: NTDOY\n",
      "NTDOY - Train Error: 0.002168, Val Error: 0.002058, Test Error: 0.002087\n",
      "model RF: MSFT\n",
      "MSFT - Train Error: 0.002993, Val Error: 0.002932, Test Error: 0.002965\n",
      "model RF: DE\n",
      "DE - Train Error: 0.002282, Val Error: 0.002146, Test Error: 0.002368\n",
      "model RF: BAC\n",
      "BAC - Train Error: 0.002107, Val Error: 0.002272, Test Error: 0.001986\n",
      "model RF: IRM\n",
      "IRM - Train Error: 0.001685, Val Error: 0.001680, Test Error: 0.001824\n",
      "model RF: ADM\n",
      "ADM - Train Error: 0.002252, Val Error: 0.002525, Test Error: 0.002492\n",
      "model RF: GOOG\n",
      "GOOG - Train Error: 0.002543, Val Error: 0.002687, Test Error: 0.002433\n",
      "model RF: META\n",
      "META - Train Error: 0.001889, Val Error: 0.002325, Test Error: 0.001821\n",
      "model RF: PG\n",
      "PG - Train Error: 0.001898, Val Error: 0.002002, Test Error: 0.002100\n",
      "model RF: ARE\n",
      "ARE - Train Error: 0.001777, Val Error: 0.001829, Test Error: 0.001599\n",
      "model RF: LH\n",
      "LH - Train Error: 0.002376, Val Error: 0.002416, Test Error: 0.002349\n",
      "model RF: MRK\n",
      "MRK - Train Error: 0.001719, Val Error: 0.001741, Test Error: 0.001760\n",
      "model RF: LVMUY\n",
      "LVMUY - Train Error: 0.002310, Val Error: 0.002198, Test Error: 0.002146\n",
      "model RF: LMT\n",
      "LMT - Train Error: 0.002128, Val Error: 0.002042, Test Error: 0.002080\n",
      "model RF: MCD\n",
      "MCD - Train Error: 0.003146, Val Error: 0.003409, Test Error: 0.003184\n",
      "model RF: MSBHF\n",
      "MSBHF - Train Error: 0.001087, Val Error: 0.001112, Test Error: 0.001153\n",
      "model RF: JCI\n",
      "JCI - Train Error: 0.002074, Val Error: 0.002251, Test Error: 0.002440\n",
      "model RF: MDLZ\n",
      "MDLZ - Train Error: 0.001904, Val Error: 0.001778, Test Error: 0.001818\n",
      "model RF: ORCL\n",
      "ORCL - Train Error: 0.002102, Val Error: 0.001975, Test Error: 0.002075\n",
      "model RF: HCA\n",
      "HCA - Train Error: 0.002565, Val Error: 0.002467, Test Error: 0.002477\n",
      "model RF: NVDA\n",
      "NVDA - Train Error: 0.003550, Val Error: 0.003350, Test Error: 0.003419\n",
      "model RF: WFC\n",
      "WFC - Train Error: 0.002230, Val Error: 0.002229, Test Error: 0.002480\n",
      "model RF: PEP\n",
      "PEP - Train Error: 0.002280, Val Error: 0.002169, Test Error: 0.002126\n",
      "model RF: IPGP\n",
      "IPGP - Train Error: 0.002299, Val Error: 0.002598, Test Error: 0.002492\n",
      "model RF: BRK-B\n",
      "BRK-B - Train Error: 0.002080, Val Error: 0.001866, Test Error: 0.001944\n",
      "model RF: SBUX\n",
      "SBUX - Train Error: 0.001508, Val Error: 0.001624, Test Error: 0.001514\n",
      "model RF: DJP\n",
      "DJP - Train Error: 0.001503, Val Error: 0.001465, Test Error: 0.001712\n",
      "model RF: XLV\n",
      "XLV - Train Error: 0.002093, Val Error: 0.002225, Test Error: 0.002003\n",
      "model RF: VFSTX\n",
      "VFSTX - Train Error: 0.002579, Val Error: 0.002764, Test Error: 0.002657\n",
      "model RF: XLP\n",
      "XLP - Train Error: 0.002169, Val Error: 0.002275, Test Error: 0.002469\n",
      "model RF: XLB\n",
      "XLB - Train Error: 0.002409, Val Error: 0.002634, Test Error: 0.002504\n",
      "model RF: XLE\n",
      "XLE - Train Error: 0.002233, Val Error: 0.002133, Test Error: 0.002406\n",
      "model RF: INDA\n",
      "INDA - Train Error: 0.002142, Val Error: 0.002106, Test Error: 0.002275\n",
      "model RF: VCIT\n",
      "VCIT - Train Error: 0.002762, Val Error: 0.003008, Test Error: 0.002857\n",
      "model RF: BND\n",
      "BND - Train Error: 0.002155, Val Error: 0.002313, Test Error: 0.002368\n",
      "model RF: XLF\n",
      "XLF - Train Error: 0.002460, Val Error: 0.002429, Test Error: 0.002379\n",
      "model RF: IXP\n",
      "IXP - Train Error: 0.002266, Val Error: 0.002629, Test Error: 0.002492\n",
      "model RF: XLI\n",
      "XLI - Train Error: 0.001910, Val Error: 0.001914, Test Error: 0.001887\n",
      "model RF: EFA\n",
      "EFA - Train Error: 0.002700, Val Error: 0.002713, Test Error: 0.002947\n",
      "model RF: XLU\n",
      "XLU - Train Error: 0.002283, Val Error: 0.002274, Test Error: 0.002287\n",
      "model RF: EEM\n",
      "EEM - Train Error: 0.002316, Val Error: 0.002225, Test Error: 0.002473\n",
      "avg val loss 0.0022904309208924187\n",
      "avg test loss 0.002285827234191355\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "SVR_models = {}\n",
    "test_losses = []\n",
    "val_losses = []\n",
    "input_size = 6\n",
    "\n",
    "for stock in stock_60d:\n",
    "    print(\"model RF: \" + stock)\n",
    "    model_svr = SVRModel()\n",
    "    model_svr.fit(X_train[stock].reshape(-1,60 * input_size), y_train[stock])\n",
    "\n",
    "    y_train_pred_svr = model_svr.predict(X_train[stock].reshape(-1,60 * input_size))\n",
    "    y_val_pred_svr = model_svr.predict(X_val[stock].reshape(-1,60 * input_size))\n",
    "    y_test_pred_svr = model_svr.predict(X_test[stock].reshape(-1,60 * input_size))\n",
    "\n",
    "    train_error_svr = mean_squared_error(y_train[stock], y_train_pred_svr)\n",
    "    val_error_svr = mean_squared_error(y_val[stock], y_val_pred_svr)\n",
    "    test_error_svr = mean_squared_error(y_test[stock], y_test_pred_svr)\n",
    "    \n",
    "    SVR_models[stock] = model_svr\n",
    "    \n",
    "    val_losses.append(val_error_svr)\n",
    "    test_losses.append(test_error_svr)\n",
    "        \n",
    "    print(f\"{stock} - Train Error: {train_error_svr:.6f}, Val Error: {val_error_svr:.6f}, Test Error: {test_error_svr:.6f}\")\n",
    "\n",
    "print(\"avg val loss\", np.mean(np.array(val_losses)))\n",
    "print(\"avg test loss\", np.mean(np.array(test_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5aa777",
   "metadata": {},
   "source": [
    "### Mean test error (Next-day Closing Price Prediction)\n",
    "Note: Closing Price is standardized\n",
    "* SVR (2.2 * 10-3)\n",
    "* **RF (2.8 * 10-4)**\n",
    "* LSTM (4.6 * 10-4)\n",
    "* DMLP (5.3 * 10-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e08de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
